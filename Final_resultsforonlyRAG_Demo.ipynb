{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "acc164f856c14a3fb2de6c024d79c718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_240c56296c91453cb47559990719a50a",
              "IPY_MODEL_411f6aa03f0f4964a7b70a8394b4ce76",
              "IPY_MODEL_d454e2b981ae4ef38a93d6c118245631"
            ],
            "layout": "IPY_MODEL_1ea6754d8b14433e9200f3bf0ed0a4ca"
          }
        },
        "240c56296c91453cb47559990719a50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39eb357def142408441faf35aa31321",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c80790ab2543d7a99d95c6b8ffad3d",
            "value": ".gitattributes: 100%"
          }
        },
        "411f6aa03f0f4964a7b70a8394b4ce76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd9dece30b94dce9dfe57239a6b046f",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23514a968836402abad23d63d889ad0c",
            "value": 1175
          }
        },
        "d454e2b981ae4ef38a93d6c118245631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d77f4b2d3f44160b402c50beb88672b",
            "placeholder": "​",
            "style": "IPY_MODEL_3d910f6db09d4374bed44b2906684f85",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 97.1kB/s]"
          }
        },
        "1ea6754d8b14433e9200f3bf0ed0a4ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b39eb357def142408441faf35aa31321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c80790ab2543d7a99d95c6b8ffad3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbd9dece30b94dce9dfe57239a6b046f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23514a968836402abad23d63d889ad0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d77f4b2d3f44160b402c50beb88672b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d910f6db09d4374bed44b2906684f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab31bac58ac441a09e09c1c5bfffb69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98ffda006b5d48199c7e79d4aedf3ee6",
              "IPY_MODEL_4728b1dad370472796c890c1b7055281",
              "IPY_MODEL_02549f35f74a454a9bc58092da667cc9"
            ],
            "layout": "IPY_MODEL_e52402be8ff641d098f990fcb0019d65"
          }
        },
        "98ffda006b5d48199c7e79d4aedf3ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df2807bc936140d8ad162c3de8ce53f7",
            "placeholder": "​",
            "style": "IPY_MODEL_59c78bb6b95f4a55af87195bf6c2a5fa",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "4728b1dad370472796c890c1b7055281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_045189bedbe84110a19f48d9eef4d60d",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcf0852b53524a25825c6f2f360efc72",
            "value": 190
          }
        },
        "02549f35f74a454a9bc58092da667cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4017f9a1d5744ba3ad3da37136fcfe4c",
            "placeholder": "​",
            "style": "IPY_MODEL_8b5b134086eb40efbf46d4400d7635d7",
            "value": " 190/190 [00:00&lt;00:00, 17.3kB/s]"
          }
        },
        "e52402be8ff641d098f990fcb0019d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df2807bc936140d8ad162c3de8ce53f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59c78bb6b95f4a55af87195bf6c2a5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "045189bedbe84110a19f48d9eef4d60d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf0852b53524a25825c6f2f360efc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4017f9a1d5744ba3ad3da37136fcfe4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5b134086eb40efbf46d4400d7635d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1ff83ffe0294fc4bc399efdcf6bbe4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ec2a11b048e439ebb5ff76b0077c56d",
              "IPY_MODEL_45e9f5c7bb0c4d05a7b9ec76c356d192",
              "IPY_MODEL_6f12b7b66f304de7a05da6c720d8b81f"
            ],
            "layout": "IPY_MODEL_5238401aed724cc398de72163daf361f"
          }
        },
        "0ec2a11b048e439ebb5ff76b0077c56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2cb1fffe06c42df82024efd37075a2a",
            "placeholder": "​",
            "style": "IPY_MODEL_93db9e8c227b41ceacb4dc46a49750c4",
            "value": "README.md: 100%"
          }
        },
        "45e9f5c7bb0c4d05a7b9ec76c356d192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a92ed17fdd8a4cedb4b1d93d16675553",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a1fe82a0cb748cb99e39d02f73cc17d",
            "value": 10610
          }
        },
        "6f12b7b66f304de7a05da6c720d8b81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251b27fa83444977aadd836f50807d83",
            "placeholder": "​",
            "style": "IPY_MODEL_36b223cbaa9d460dba44c2195b755789",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 920kB/s]"
          }
        },
        "5238401aed724cc398de72163daf361f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cb1fffe06c42df82024efd37075a2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93db9e8c227b41ceacb4dc46a49750c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a92ed17fdd8a4cedb4b1d93d16675553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1fe82a0cb748cb99e39d02f73cc17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "251b27fa83444977aadd836f50807d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36b223cbaa9d460dba44c2195b755789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b4b8c723b24d64a9a986413c344c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3f7ba5282db441fbb8761771f19fa64",
              "IPY_MODEL_66d67c9abd404f74a6c36417b6eca7d0",
              "IPY_MODEL_33ebed83905942c58bf470e5bddf3efd"
            ],
            "layout": "IPY_MODEL_6ed93fad5515466bb1008329a2d0e817"
          }
        },
        "b3f7ba5282db441fbb8761771f19fa64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10140c51849e4862bb7922dbe67dd43f",
            "placeholder": "​",
            "style": "IPY_MODEL_d2cd503578e748fb8531a0b2bae42982",
            "value": "config.json: 100%"
          }
        },
        "66d67c9abd404f74a6c36417b6eca7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89b37a17a1c4412fb932e0df1a4db965",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b3deb1a7b074205933f8f38997471fb",
            "value": 612
          }
        },
        "33ebed83905942c58bf470e5bddf3efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65cdaebc4b054b77bc63e9755c194e8e",
            "placeholder": "​",
            "style": "IPY_MODEL_1d321a89ab314c31977ab3adf09c3107",
            "value": " 612/612 [00:00&lt;00:00, 55.1kB/s]"
          }
        },
        "6ed93fad5515466bb1008329a2d0e817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10140c51849e4862bb7922dbe67dd43f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2cd503578e748fb8531a0b2bae42982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b37a17a1c4412fb932e0df1a4db965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b3deb1a7b074205933f8f38997471fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65cdaebc4b054b77bc63e9755c194e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d321a89ab314c31977ab3adf09c3107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "491b62f0bfce42c78e5c3ed8c791f140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d0799fd4e16458bb22ecc838631c90f",
              "IPY_MODEL_9c6a198836054def8e34b280d843087d",
              "IPY_MODEL_bea933973ce7470e9333cfe45ce64a9e"
            ],
            "layout": "IPY_MODEL_d16c5708d6744ca19d0d9cf677383a7f"
          }
        },
        "2d0799fd4e16458bb22ecc838631c90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8736bc930975483f8d82f0ddf9caebdd",
            "placeholder": "​",
            "style": "IPY_MODEL_8998617c493048d29c185f9aa5c026a7",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "9c6a198836054def8e34b280d843087d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f14c8a01204ef2bec9aafbc9df27d9",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d6002004ae04e4b8b10916de1593c23",
            "value": 116
          }
        },
        "bea933973ce7470e9333cfe45ce64a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77bf9094af84c8f9b1702c3402e9237",
            "placeholder": "​",
            "style": "IPY_MODEL_706e876a3cac430b9f77df0d53b7e496",
            "value": " 116/116 [00:00&lt;00:00, 10.1kB/s]"
          }
        },
        "d16c5708d6744ca19d0d9cf677383a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8736bc930975483f8d82f0ddf9caebdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8998617c493048d29c185f9aa5c026a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49f14c8a01204ef2bec9aafbc9df27d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6002004ae04e4b8b10916de1593c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c77bf9094af84c8f9b1702c3402e9237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706e876a3cac430b9f77df0d53b7e496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64426035659b436780080e780a646ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93737b5623dd445e982d951874fd89dd",
              "IPY_MODEL_34a9e3fcd9764a6ca11485b45c010979",
              "IPY_MODEL_77ebe2d1b4304c26a150174ab101389e"
            ],
            "layout": "IPY_MODEL_1ce6770615d3462fa1fe2849d03af33c"
          }
        },
        "93737b5623dd445e982d951874fd89dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50a3b6f6712442619f1e575ecbbd641c",
            "placeholder": "​",
            "style": "IPY_MODEL_91f8eb16cf7347d6b0658b66d39a50e2",
            "value": "data_config.json: 100%"
          }
        },
        "34a9e3fcd9764a6ca11485b45c010979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3e65b4baaf405a93543c0a91beb1f7",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b0d6d31b8a1418f9f7bb3703385705c",
            "value": 39265
          }
        },
        "77ebe2d1b4304c26a150174ab101389e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a2f314a25c43338aaf3120d3a8d6f4",
            "placeholder": "​",
            "style": "IPY_MODEL_f58b82bf17984241a9de92bbf54268ad",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.52MB/s]"
          }
        },
        "1ce6770615d3462fa1fe2849d03af33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a3b6f6712442619f1e575ecbbd641c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f8eb16cf7347d6b0658b66d39a50e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec3e65b4baaf405a93543c0a91beb1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0d6d31b8a1418f9f7bb3703385705c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4a2f314a25c43338aaf3120d3a8d6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58b82bf17984241a9de92bbf54268ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0056f070dfd34827a5435ddb2d9e6320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f7daac1a00e4a7f919f824c6388cbcf",
              "IPY_MODEL_c70b1d407c774aed838fe23ee23098c3",
              "IPY_MODEL_f0a3e88fff2d4843be2966a1528dde0e"
            ],
            "layout": "IPY_MODEL_b036ef8590924547b0f02064d22c1bda"
          }
        },
        "8f7daac1a00e4a7f919f824c6388cbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc38988ab72c49e68d30ca9903550326",
            "placeholder": "​",
            "style": "IPY_MODEL_216cec2210e34883a1d98d2716e8c802",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "c70b1d407c774aed838fe23ee23098c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b913ff733e043d8957eeb3775a4873c",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a0006874bd74c5fac7f143316d39fe4",
            "value": 90888945
          }
        },
        "f0a3e88fff2d4843be2966a1528dde0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af55f91a8624fe8afd6d4f1679f1d73",
            "placeholder": "​",
            "style": "IPY_MODEL_d0d2b7cf639c4430bd02a860ca7b13a9",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 275MB/s]"
          }
        },
        "b036ef8590924547b0f02064d22c1bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc38988ab72c49e68d30ca9903550326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216cec2210e34883a1d98d2716e8c802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b913ff733e043d8957eeb3775a4873c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0006874bd74c5fac7f143316d39fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8af55f91a8624fe8afd6d4f1679f1d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d2b7cf639c4430bd02a860ca7b13a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c919f4499f43bc8acbfbf45b44fa9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cef3371198454c0dada18c0ac0243a83",
              "IPY_MODEL_9a4069bedf844305832c68a5c332542d",
              "IPY_MODEL_90851ac8c3b446049857d648e464d08d"
            ],
            "layout": "IPY_MODEL_f64b6819e1294df1962a9c1a9d39a4b7"
          }
        },
        "cef3371198454c0dada18c0ac0243a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_921b80a213b148cca704dee0665782a8",
            "placeholder": "​",
            "style": "IPY_MODEL_585f811ad55e48b5a6bb384954bd48c5",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "9a4069bedf844305832c68a5c332542d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12aa43e940741bc9e7dd4cc8cfe5e09",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23a6ed5be3a0428f9b7961df3720563b",
            "value": 53
          }
        },
        "90851ac8c3b446049857d648e464d08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9268221d6aa34c808504fcbf118d64d6",
            "placeholder": "​",
            "style": "IPY_MODEL_16fb94f40d4c47dca26fc21c0d8590b6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.62kB/s]"
          }
        },
        "f64b6819e1294df1962a9c1a9d39a4b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "921b80a213b148cca704dee0665782a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585f811ad55e48b5a6bb384954bd48c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f12aa43e940741bc9e7dd4cc8cfe5e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a6ed5be3a0428f9b7961df3720563b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9268221d6aa34c808504fcbf118d64d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16fb94f40d4c47dca26fc21c0d8590b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7965b3093236408f9bcb59056d877883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c91d09ed59db4f298adf9b97d6b29baf",
              "IPY_MODEL_6d65cdedef8940628bdb296392fb9c1b",
              "IPY_MODEL_c4ba6ecd50e04f9ba2c8a1e366a76b51"
            ],
            "layout": "IPY_MODEL_cb673b6db5ea4638a8cb11bc93a99aab"
          }
        },
        "c91d09ed59db4f298adf9b97d6b29baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_799ef4fee61f4c2ba58288b8943eeee7",
            "placeholder": "​",
            "style": "IPY_MODEL_87b5567ebaac429586367a3007374db1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6d65cdedef8940628bdb296392fb9c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3564ea7ba9ab446d891bb6edf087be3f",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8628f7947140467ea65da89a69723946",
            "value": 112
          }
        },
        "c4ba6ecd50e04f9ba2c8a1e366a76b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3689042fdc3499da5877d181c518b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_2f419b4852f44627b4a33cb31f202a53",
            "value": " 112/112 [00:00&lt;00:00, 10.4kB/s]"
          }
        },
        "cb673b6db5ea4638a8cb11bc93a99aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799ef4fee61f4c2ba58288b8943eeee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b5567ebaac429586367a3007374db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3564ea7ba9ab446d891bb6edf087be3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8628f7947140467ea65da89a69723946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3689042fdc3499da5877d181c518b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f419b4852f44627b4a33cb31f202a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6445a00ed9434fb6b06625977f823775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e22cf1785c74cecabdcc534256b5fab",
              "IPY_MODEL_b47879b7a2a74f918c186af044d6af99",
              "IPY_MODEL_bb85f6601a4548c3a637998f3cb67188"
            ],
            "layout": "IPY_MODEL_d28eddf96b524ba29fc769f50502748d"
          }
        },
        "2e22cf1785c74cecabdcc534256b5fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c1b3d9db5734f95bbf0145ac33cba33",
            "placeholder": "​",
            "style": "IPY_MODEL_24af5a50035241bca67ded3c10c5bca0",
            "value": "tokenizer.json: 100%"
          }
        },
        "b47879b7a2a74f918c186af044d6af99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81c4083ae5d94404a441bb211b8d8b4a",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9890ddfc5c0d4a51bce343f9ebb3e214",
            "value": 466247
          }
        },
        "bb85f6601a4548c3a637998f3cb67188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47f7d42eb93d4fb6bccc59bc518dc7d9",
            "placeholder": "​",
            "style": "IPY_MODEL_d95a5f5642134f3bb68ca098715a587d",
            "value": " 466k/466k [00:00&lt;00:00, 28.3MB/s]"
          }
        },
        "d28eddf96b524ba29fc769f50502748d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1b3d9db5734f95bbf0145ac33cba33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24af5a50035241bca67ded3c10c5bca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81c4083ae5d94404a441bb211b8d8b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9890ddfc5c0d4a51bce343f9ebb3e214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47f7d42eb93d4fb6bccc59bc518dc7d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95a5f5642134f3bb68ca098715a587d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af9c6bbe18934eff86680cdade04c38a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a7f01fb4ec947e8897100e84d7d995b",
              "IPY_MODEL_d26ebe5166d7442d80896a82841efb70",
              "IPY_MODEL_2df1b7ae27e044d4b4b744e3cae13a8e"
            ],
            "layout": "IPY_MODEL_3579b68d704b4c0b84b2744335845de3"
          }
        },
        "2a7f01fb4ec947e8897100e84d7d995b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20f94e26dacd4638a2a42e32d80953b1",
            "placeholder": "​",
            "style": "IPY_MODEL_73fa1b681312421cbe4ab7ea92e372de",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d26ebe5166d7442d80896a82841efb70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_629ee9c9e89b40eeaad25384c9964bf8",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f11fd149c44a47ba8267098532238ac0",
            "value": 350
          }
        },
        "2df1b7ae27e044d4b4b744e3cae13a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270519d0da064fa7810d69e18ab99771",
            "placeholder": "​",
            "style": "IPY_MODEL_3a3591c554a74502aebc36ec1aad6bdd",
            "value": " 350/350 [00:00&lt;00:00, 31.7kB/s]"
          }
        },
        "3579b68d704b4c0b84b2744335845de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f94e26dacd4638a2a42e32d80953b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fa1b681312421cbe4ab7ea92e372de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "629ee9c9e89b40eeaad25384c9964bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f11fd149c44a47ba8267098532238ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "270519d0da064fa7810d69e18ab99771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3591c554a74502aebc36ec1aad6bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd906bd9b9094cefb39b81a0cc16fda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_369285630d6a485f95a458bd6b0c4b47",
              "IPY_MODEL_dc66ac4b4b0c4590bfdb2f6388e739f7",
              "IPY_MODEL_aea3bf9ea73a424398307cf9334688ab"
            ],
            "layout": "IPY_MODEL_6de6aa9aed66461fbeb088327c6ade44"
          }
        },
        "369285630d6a485f95a458bd6b0c4b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_612896638e024c7d8e9310aa3c8879c0",
            "placeholder": "​",
            "style": "IPY_MODEL_2e5a77bb548e4808aa1b195c72941f89",
            "value": "train_script.py: 100%"
          }
        },
        "dc66ac4b4b0c4590bfdb2f6388e739f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6918ff3a306a4149a834c60930ad75be",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f9e316b1bc44b3a944954c1f09a322e",
            "value": 13156
          }
        },
        "aea3bf9ea73a424398307cf9334688ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15e2f0fd376640ef83ba7368ddeea3d2",
            "placeholder": "​",
            "style": "IPY_MODEL_335debbaad864d1bbb9d6acce60079ca",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 1.08MB/s]"
          }
        },
        "6de6aa9aed66461fbeb088327c6ade44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "612896638e024c7d8e9310aa3c8879c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5a77bb548e4808aa1b195c72941f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6918ff3a306a4149a834c60930ad75be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f9e316b1bc44b3a944954c1f09a322e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15e2f0fd376640ef83ba7368ddeea3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "335debbaad864d1bbb9d6acce60079ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fa04ee6946941778c2906aacfefcc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_615d728354a8428da3f852d9d7bdbd52",
              "IPY_MODEL_53513ca471ae4f0285248f3e7ce5bc75",
              "IPY_MODEL_29c80bb779524aeea836cdd3c1c23995"
            ],
            "layout": "IPY_MODEL_cc3cc69d6902400b85d6923d0b8df88a"
          }
        },
        "615d728354a8428da3f852d9d7bdbd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f3218b517443729fef733ad5c53ce3",
            "placeholder": "​",
            "style": "IPY_MODEL_b41f725971b7435e838a46b05353de3d",
            "value": "vocab.txt: 100%"
          }
        },
        "53513ca471ae4f0285248f3e7ce5bc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab2470691d84e16a8a477c5330b3c16",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3ac5c4943ce440ab9636b04e88682cd",
            "value": 231508
          }
        },
        "29c80bb779524aeea836cdd3c1c23995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d52fb65898141b2a29f6a66147d9aff",
            "placeholder": "​",
            "style": "IPY_MODEL_396aa44de2f34ae2ba47d299b60c4a26",
            "value": " 232k/232k [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "cc3cc69d6902400b85d6923d0b8df88a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f3218b517443729fef733ad5c53ce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b41f725971b7435e838a46b05353de3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab2470691d84e16a8a477c5330b3c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ac5c4943ce440ab9636b04e88682cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d52fb65898141b2a29f6a66147d9aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396aa44de2f34ae2ba47d299b60c4a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09f42c04be204babbe49c3911ff57f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61e46d06943744069483b399ff1f355a",
              "IPY_MODEL_172193310beb48878e1dbc48dedb613b",
              "IPY_MODEL_b495ef48425a4149bd735f78ab5fd938"
            ],
            "layout": "IPY_MODEL_f66ef04eb564421eb29a05222272ab62"
          }
        },
        "61e46d06943744069483b399ff1f355a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6dedcb833ad4240becd9a6a59d4c99d",
            "placeholder": "​",
            "style": "IPY_MODEL_d3470a00170d4fb197e3464339b2b49a",
            "value": "modules.json: 100%"
          }
        },
        "172193310beb48878e1dbc48dedb613b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d94ec03e837348038948405b66fb10f4",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c112568cf6343b2b1b59a1f8d2130e1",
            "value": 349
          }
        },
        "b495ef48425a4149bd735f78ab5fd938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646f4e4096b24acbb9075906274fd01b",
            "placeholder": "​",
            "style": "IPY_MODEL_3c63e1d7af97417ab450e11fcefb765a",
            "value": " 349/349 [00:00&lt;00:00, 32.8kB/s]"
          }
        },
        "f66ef04eb564421eb29a05222272ab62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6dedcb833ad4240becd9a6a59d4c99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3470a00170d4fb197e3464339b2b49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d94ec03e837348038948405b66fb10f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c112568cf6343b2b1b59a1f8d2130e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "646f4e4096b24acbb9075906274fd01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c63e1d7af97417ab450e11fcefb765a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a5f25e1a81146f8a9bdb8e3bf62a527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc90add27c2d4de4ac3b89e10b344897",
              "IPY_MODEL_eceed43a804c4467a6cc5258d98e8482",
              "IPY_MODEL_3dca81550b57417aa9fbaf9e371b61f2",
              "IPY_MODEL_82a937baf104441fbc04f2b986281f89"
            ],
            "layout": "IPY_MODEL_d821e5f448c44f4d8c7d164ee1142c35"
          }
        },
        "c90aa9bdd27d41d2b06c8e1213177317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8065f590ff5b471393c419b442daab19",
            "placeholder": "​",
            "style": "IPY_MODEL_184f012beb704cc3a1bbc113c2e9032a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "214a640f9a7c4c358527d9dde2329551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6747d1ba5f7e4638bdfc19244a5af9a3",
            "placeholder": "​",
            "style": "IPY_MODEL_c70afc62257744948a9abeef8e0ed95a",
            "value": ""
          }
        },
        "95e22dda833b4f939fe02be52d6d1080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1400d37ae3c041a2a3f56fdb461f1f96",
            "style": "IPY_MODEL_1a0ff08431144766b92e715d2989c79e",
            "value": true
          }
        },
        "fe9d58d4cc1945679c423ff15c86f87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e8ecd30f355d4b32ac313155374c37e6",
            "style": "IPY_MODEL_049d43d58b134ee885c21212e4ce85c2",
            "tooltip": ""
          }
        },
        "a90b1fa40ceb4ab1940e192d6c1c3d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6411d343be149c1b240aeee351fce0f",
            "placeholder": "​",
            "style": "IPY_MODEL_6a195ef1fd8f4627aae70b5ef7137fee",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d821e5f448c44f4d8c7d164ee1142c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8065f590ff5b471393c419b442daab19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184f012beb704cc3a1bbc113c2e9032a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6747d1ba5f7e4638bdfc19244a5af9a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70afc62257744948a9abeef8e0ed95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1400d37ae3c041a2a3f56fdb461f1f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0ff08431144766b92e715d2989c79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ecd30f355d4b32ac313155374c37e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049d43d58b134ee885c21212e4ce85c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f6411d343be149c1b240aeee351fce0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a195ef1fd8f4627aae70b5ef7137fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f18b08e3ed04fdf9b25bec96f53973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_946d7ccf4ae7405ba294a1b007c4884d",
            "placeholder": "​",
            "style": "IPY_MODEL_fcdcc1714c5c4d4ca0a0b673132cccea",
            "value": "Connecting..."
          }
        },
        "946d7ccf4ae7405ba294a1b007c4884d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcdcc1714c5c4d4ca0a0b673132cccea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc90add27c2d4de4ac3b89e10b344897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfdf40549f04413b833b670066baff25",
            "placeholder": "​",
            "style": "IPY_MODEL_1743bb1c1fdd46c99ce0afd525bf71a3",
            "value": "Token is valid (permission: read)."
          }
        },
        "eceed43a804c4467a6cc5258d98e8482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48392ceba8af4ab4a8c7ee2943add5b3",
            "placeholder": "​",
            "style": "IPY_MODEL_c6bb0fc7670845ceac7c3e8275569d6b",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "3dca81550b57417aa9fbaf9e371b61f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdd7cb5bbfec46a0a725d415054e0914",
            "placeholder": "​",
            "style": "IPY_MODEL_df248a71770044efb8a24f8a5e9d8f9d",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "82a937baf104441fbc04f2b986281f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221f7823354c4058bea465510b2d6fdf",
            "placeholder": "​",
            "style": "IPY_MODEL_99f47dc35c784f369808d56a566c6cd1",
            "value": "Login successful"
          }
        },
        "cfdf40549f04413b833b670066baff25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1743bb1c1fdd46c99ce0afd525bf71a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48392ceba8af4ab4a8c7ee2943add5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6bb0fc7670845ceac7c3e8275569d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdd7cb5bbfec46a0a725d415054e0914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df248a71770044efb8a24f8a5e9d8f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "221f7823354c4058bea465510b2d6fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f47dc35c784f369808d56a566c6cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd936267170e4e949972461da449ba98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52dd459e01454a3eb8a0acd097de7325",
              "IPY_MODEL_b038c3bb4e10488a9269c58680e4f121",
              "IPY_MODEL_8a1fcb08c5db40178ad31de55dd2d400"
            ],
            "layout": "IPY_MODEL_d6d88c72539b47438aa91ca03b801322"
          }
        },
        "52dd459e01454a3eb8a0acd097de7325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52709ac8b59b4c4ea2f4686ddc6cac18",
            "placeholder": "​",
            "style": "IPY_MODEL_f34fc7249cf845788e3a83ebf943f916",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b038c3bb4e10488a9269c58680e4f121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63b1c3956fa746dd8af79feaf0f91f83",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_164f3730277447b4af92c267bdb4a87f",
            "value": 1618
          }
        },
        "8a1fcb08c5db40178ad31de55dd2d400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5eb6449daf34bfe9848c3345bb962f6",
            "placeholder": "​",
            "style": "IPY_MODEL_4c1eb3395ae34d27b1a4e5754e8472df",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 143kB/s]"
          }
        },
        "d6d88c72539b47438aa91ca03b801322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52709ac8b59b4c4ea2f4686ddc6cac18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f34fc7249cf845788e3a83ebf943f916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63b1c3956fa746dd8af79feaf0f91f83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "164f3730277447b4af92c267bdb4a87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5eb6449daf34bfe9848c3345bb962f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1eb3395ae34d27b1a4e5754e8472df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a84a0e977246f0a0059601f4efa88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cd6411f0baa47a3bed321f4c70cea60",
              "IPY_MODEL_58ae471918534655bda14a3f6bdfdb8c",
              "IPY_MODEL_46a8a74a51a144f392159d0cb08f7ab4"
            ],
            "layout": "IPY_MODEL_de4cac5a69d644148669b468fb80bb61"
          }
        },
        "4cd6411f0baa47a3bed321f4c70cea60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ecb9cb514248ae92736fbc44d2ff0f",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc701af4a5842aba8eb370d8ae5882d",
            "value": "tokenizer.model: 100%"
          }
        },
        "58ae471918534655bda14a3f6bdfdb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f05b4653474883a6aec9d5efdeecca",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3f080da9869400fa6d972f69ed875f6",
            "value": 499723
          }
        },
        "46a8a74a51a144f392159d0cb08f7ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6597bd7fb5a43df83e7cbb5d2e59450",
            "placeholder": "​",
            "style": "IPY_MODEL_589ee30147c2424e92f457b9746899b3",
            "value": " 500k/500k [00:00&lt;00:00, 35.6MB/s]"
          }
        },
        "de4cac5a69d644148669b468fb80bb61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ecb9cb514248ae92736fbc44d2ff0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc701af4a5842aba8eb370d8ae5882d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f05b4653474883a6aec9d5efdeecca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f080da9869400fa6d972f69ed875f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6597bd7fb5a43df83e7cbb5d2e59450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589ee30147c2424e92f457b9746899b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "804bb6009e1546dca36f66794d27f1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cbc9b2932634a9ab240dc6b644af903",
              "IPY_MODEL_e9dfc86c9b3e48e2ae27fec50d9c1b99",
              "IPY_MODEL_0470f3d00a8f42ec80fa705fdba5ed80"
            ],
            "layout": "IPY_MODEL_321b320d455e491a993c6911b1890b06"
          }
        },
        "8cbc9b2932634a9ab240dc6b644af903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eeb1f9eded6453a938a078a838383b3",
            "placeholder": "​",
            "style": "IPY_MODEL_14dcef79caa54b96a8998f92c8ef6610",
            "value": "tokenizer.json: 100%"
          }
        },
        "e9dfc86c9b3e48e2ae27fec50d9c1b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ef463439e8a45419f86be22068424a3",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c94bbb6cf9049fca2c8553f650160e1",
            "value": 1842767
          }
        },
        "0470f3d00a8f42ec80fa705fdba5ed80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6ea9a78fb64dd0a8fc92686528ce99",
            "placeholder": "​",
            "style": "IPY_MODEL_4a6b70d82d3d446e8ba6abd95c88bbc6",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 29.4MB/s]"
          }
        },
        "321b320d455e491a993c6911b1890b06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eeb1f9eded6453a938a078a838383b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14dcef79caa54b96a8998f92c8ef6610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef463439e8a45419f86be22068424a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c94bbb6cf9049fca2c8553f650160e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d6ea9a78fb64dd0a8fc92686528ce99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6b70d82d3d446e8ba6abd95c88bbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb284a3f27244d1bb41e20e5e74f6d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9629b394cdfa4f93990ca45482a749a4",
              "IPY_MODEL_07a7b25c310d4a2086b0fd804118b782",
              "IPY_MODEL_de622d525eca4779a2e06437254c45ca"
            ],
            "layout": "IPY_MODEL_a8ccd5de72674da4a3c35f350b2bd1df"
          }
        },
        "9629b394cdfa4f93990ca45482a749a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc51593450a4ca0952dbc573e1ada77",
            "placeholder": "​",
            "style": "IPY_MODEL_a603972ada6246ce8c1ff21ed729279e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "07a7b25c310d4a2086b0fd804118b782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cab29ce43b244aba97355d331d9f4a9",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9494e650e424f58a0082de782cc962b",
            "value": 414
          }
        },
        "de622d525eca4779a2e06437254c45ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941dcb28ff9e475c85e426fc4323b841",
            "placeholder": "​",
            "style": "IPY_MODEL_5b55dcb7841e4489ad9b059308c71a48",
            "value": " 414/414 [00:00&lt;00:00, 37.2kB/s]"
          }
        },
        "a8ccd5de72674da4a3c35f350b2bd1df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc51593450a4ca0952dbc573e1ada77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a603972ada6246ce8c1ff21ed729279e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cab29ce43b244aba97355d331d9f4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9494e650e424f58a0082de782cc962b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "941dcb28ff9e475c85e426fc4323b841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b55dcb7841e4489ad9b059308c71a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54825b751974458b8fb12bf071cace8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbfb40ffef92488ba3c6075e646c272c",
              "IPY_MODEL_ef92216f6f2a408dada9a6dab9ff644b",
              "IPY_MODEL_ed89e182b5f64cd191190fadde63261f"
            ],
            "layout": "IPY_MODEL_328c658f1af24008b952a0f72aba0cef"
          }
        },
        "dbfb40ffef92488ba3c6075e646c272c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc572ffd1af54f46bf220efe536092cb",
            "placeholder": "​",
            "style": "IPY_MODEL_0ca37488639840edb42c5737740d76eb",
            "value": "config.json: 100%"
          }
        },
        "ef92216f6f2a408dada9a6dab9ff644b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ccd936d37bf47b3997ff3d184b45022",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_694229b06ae74c25804a57c09cd3167b",
            "value": 614
          }
        },
        "ed89e182b5f64cd191190fadde63261f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_135798794a3c45d7bbcf99714b72fc54",
            "placeholder": "​",
            "style": "IPY_MODEL_1900235009b34c24ba830bed7572afea",
            "value": " 614/614 [00:00&lt;00:00, 51.1kB/s]"
          }
        },
        "328c658f1af24008b952a0f72aba0cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc572ffd1af54f46bf220efe536092cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca37488639840edb42c5737740d76eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ccd936d37bf47b3997ff3d184b45022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "694229b06ae74c25804a57c09cd3167b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "135798794a3c45d7bbcf99714b72fc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1900235009b34c24ba830bed7572afea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc06635bc4f433e80902ae1231b9831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_820b86c96bd94a78b4f7c395ef65c07e",
              "IPY_MODEL_631a7877173346348f7ebf16aa7ea902",
              "IPY_MODEL_41004ff5f0f14383b37649ec182c0fa2"
            ],
            "layout": "IPY_MODEL_7ab7433e2cfd44fab9e05780da861005"
          }
        },
        "820b86c96bd94a78b4f7c395ef65c07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2427fb048a54cb694fe559cb21133f4",
            "placeholder": "​",
            "style": "IPY_MODEL_a2d4cb553a7244b58b78365d1f1917e7",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "631a7877173346348f7ebf16aa7ea902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3292623eef426cb466f2426c190205",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f13d775ac88142cb81e2f4fb88f139f7",
            "value": 26788
          }
        },
        "41004ff5f0f14383b37649ec182c0fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a6b90169474220b02b1c83346d17ba",
            "placeholder": "​",
            "style": "IPY_MODEL_5ab576a2790f47daad04a7febcc16350",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 2.39MB/s]"
          }
        },
        "7ab7433e2cfd44fab9e05780da861005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2427fb048a54cb694fe559cb21133f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d4cb553a7244b58b78365d1f1917e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3292623eef426cb466f2426c190205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f13d775ac88142cb81e2f4fb88f139f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06a6b90169474220b02b1c83346d17ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ab576a2790f47daad04a7febcc16350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d24d8fb049c4f88ab9a267996289918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73b123f5b5b64c50bf18b6aa789c88ad",
              "IPY_MODEL_185cc09767d249268fb3cb7f637b7031",
              "IPY_MODEL_c6141babf6de48cda13fa53085ec5f7c"
            ],
            "layout": "IPY_MODEL_ac6564231ac54097a9953c760016908d"
          }
        },
        "73b123f5b5b64c50bf18b6aa789c88ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90f312b935f1461ba001184351b8ced3",
            "placeholder": "​",
            "style": "IPY_MODEL_7d6e3043e02847c7813a312cc3de54e6",
            "value": "Downloading shards: 100%"
          }
        },
        "185cc09767d249268fb3cb7f637b7031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f0ba671d61456bbd9f641d836acfc4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ef8e73904464c0fb1033413c2f07d7c",
            "value": 2
          }
        },
        "c6141babf6de48cda13fa53085ec5f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93da92d7bfc54d9c91641f922240b7b1",
            "placeholder": "​",
            "style": "IPY_MODEL_27a84e7bfb634b2c8f6e66a5df1a8d3f",
            "value": " 2/2 [00:38&lt;00:00, 17.52s/it]"
          }
        },
        "ac6564231ac54097a9953c760016908d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f312b935f1461ba001184351b8ced3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6e3043e02847c7813a312cc3de54e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8f0ba671d61456bbd9f641d836acfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef8e73904464c0fb1033413c2f07d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93da92d7bfc54d9c91641f922240b7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a84e7bfb634b2c8f6e66a5df1a8d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f977a8bc30b45bf91b61a0e48d1a52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a15cdea078a1453393358ee343a74a4a",
              "IPY_MODEL_8019cb7d6f2d4e1396b01a04a1e82ac7",
              "IPY_MODEL_97d0c1d5626449fa9135a7a3393c68c6"
            ],
            "layout": "IPY_MODEL_00dc1df77ed141eeb04286b8fcb0bf12"
          }
        },
        "a15cdea078a1453393358ee343a74a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fb15a50c606476fbc6cae2f1d0450be",
            "placeholder": "​",
            "style": "IPY_MODEL_d6480e397752474da49e929f73edfb3b",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "8019cb7d6f2d4e1396b01a04a1e82ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a7b33d625e42d9bb6693b95ea60d62",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76192ef0e1d4460a88aa0859f2cfc4d6",
            "value": 9976576152
          }
        },
        "97d0c1d5626449fa9135a7a3393c68c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9800bd892ef940ce9a9491d7a9c0ebb5",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5b4ae748be403cb8c07e50584d9c8a",
            "value": " 9.98G/9.98G [00:27&lt;00:00, 225MB/s]"
          }
        },
        "00dc1df77ed141eeb04286b8fcb0bf12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb15a50c606476fbc6cae2f1d0450be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6480e397752474da49e929f73edfb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7a7b33d625e42d9bb6693b95ea60d62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76192ef0e1d4460a88aa0859f2cfc4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9800bd892ef940ce9a9491d7a9c0ebb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5b4ae748be403cb8c07e50584d9c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b48919d61d4a45629e6823c61c51bd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1901f283cb6b4dc5914106290b15b8c9",
              "IPY_MODEL_14a07e3ffd704ebd9e5d123971cbebc9",
              "IPY_MODEL_3d4d1bc814b34c32a4f4f0b36d698c5a"
            ],
            "layout": "IPY_MODEL_b0088d682c6a49149c3e5f7962e62408"
          }
        },
        "1901f283cb6b4dc5914106290b15b8c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_681e4490d9414fb7bdf15e764e62dffe",
            "placeholder": "​",
            "style": "IPY_MODEL_688dfca43d5d4aa781ffe1bd2ded270f",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "14a07e3ffd704ebd9e5d123971cbebc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4ceaef9f037414e9ca680b1db745c6b",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1ddf0b391de421e8e865fb415860576",
            "value": 3500296424
          }
        },
        "3d4d1bc814b34c32a4f4f0b36d698c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7bdf335cc446f686f55a0211db5941",
            "placeholder": "​",
            "style": "IPY_MODEL_6ed0ff3d20304e9ea5fa76496eb06689",
            "value": " 3.50G/3.50G [00:09&lt;00:00, 271MB/s]"
          }
        },
        "b0088d682c6a49149c3e5f7962e62408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681e4490d9414fb7bdf15e764e62dffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688dfca43d5d4aa781ffe1bd2ded270f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4ceaef9f037414e9ca680b1db745c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ddf0b391de421e8e865fb415860576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d7bdf335cc446f686f55a0211db5941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed0ff3d20304e9ea5fa76496eb06689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "272945957c6843bfbdb7e138d952eb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e76eae6654e84d6f9450a4788b8b0be7",
              "IPY_MODEL_a12ffd24728c4b5da2f3c84ece8b0586",
              "IPY_MODEL_6eeb2211ed8043d7a12d84fa07b5bf7e"
            ],
            "layout": "IPY_MODEL_cafc604c7a914f7799f1507a3bfd4a75"
          }
        },
        "e76eae6654e84d6f9450a4788b8b0be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_884872ed68274b2587ef01fb03be49e9",
            "placeholder": "​",
            "style": "IPY_MODEL_9410c6ae56e94814a2710970faeeacbc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a12ffd24728c4b5da2f3c84ece8b0586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad248a058d04faaacffa8378a44513a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5776a4cb1c45415ab238520df7c5c2d4",
            "value": 2
          }
        },
        "6eeb2211ed8043d7a12d84fa07b5bf7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dffa2996d5346bcae3eac7dc71b535f",
            "placeholder": "​",
            "style": "IPY_MODEL_8898bfce9c0148e9b28dba20af8003fd",
            "value": " 2/2 [00:11&lt;00:00,  5.07s/it]"
          }
        },
        "cafc604c7a914f7799f1507a3bfd4a75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884872ed68274b2587ef01fb03be49e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9410c6ae56e94814a2710970faeeacbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ad248a058d04faaacffa8378a44513a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5776a4cb1c45415ab238520df7c5c2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dffa2996d5346bcae3eac7dc71b535f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8898bfce9c0148e9b28dba20af8003fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1ff053401041fd9c008ca6f6618159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_361365d014c246f1b4d79b6f6362c0f8",
              "IPY_MODEL_662e1a95b9364de1838b1a9a8e6d4d9b",
              "IPY_MODEL_2d174f65ea2d49ec8660c301f2650ccc"
            ],
            "layout": "IPY_MODEL_f5237c5359f24fc29a95f48683af434b"
          }
        },
        "361365d014c246f1b4d79b6f6362c0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9485d282c90d47ff97c935cf53f9ba78",
            "placeholder": "​",
            "style": "IPY_MODEL_16a028fe96554b4194b0f7e29d0c5b21",
            "value": "generation_config.json: 100%"
          }
        },
        "662e1a95b9364de1838b1a9a8e6d4d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3c713107afd4e8381c60b2229604e30",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aad8d43438294bd6a3f2dff24c817409",
            "value": 188
          }
        },
        "2d174f65ea2d49ec8660c301f2650ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d1ab7b274044d3b69dd0ca35699ae5",
            "placeholder": "​",
            "style": "IPY_MODEL_56af4a1308314b8aa394d972f79ede36",
            "value": " 188/188 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "f5237c5359f24fc29a95f48683af434b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9485d282c90d47ff97c935cf53f9ba78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a028fe96554b4194b0f7e29d0c5b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3c713107afd4e8381c60b2229604e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad8d43438294bd6a3f2dff24c817409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13d1ab7b274044d3b69dd0ca35699ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56af4a1308314b8aa394d972f79ede36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e95471c7df2e40a0b459b6612582f565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7680a53b73044835a397a9395172bb99",
              "IPY_MODEL_48211845c1554f4da8a749c7d6105d41",
              "IPY_MODEL_e90e668748474ec5b993996627a35287"
            ],
            "layout": "IPY_MODEL_5538009baa014fe08f1bd2690854646d"
          }
        },
        "7680a53b73044835a397a9395172bb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4208bdbd1af74b8a9605124acbff7918",
            "placeholder": "​",
            "style": "IPY_MODEL_f721665ba21d4757972b7f8189762fb8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "48211845c1554f4da8a749c7d6105d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137170840ddd438ebe44bbe788576c14",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a50106cbf0f4039870fd1c5e238c009",
            "value": 2
          }
        },
        "e90e668748474ec5b993996627a35287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2fed2ec393463ebc940054cdf6d0db",
            "placeholder": "​",
            "style": "IPY_MODEL_db8d69ae1f574df4a9fea3da57736702",
            "value": " 2/2 [00:02&lt;00:00,  1.23s/it]"
          }
        },
        "5538009baa014fe08f1bd2690854646d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4208bdbd1af74b8a9605124acbff7918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f721665ba21d4757972b7f8189762fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137170840ddd438ebe44bbe788576c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a50106cbf0f4039870fd1c5e238c009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da2fed2ec393463ebc940054cdf6d0db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8d69ae1f574df4a9fea3da57736702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Chat with Multiple PDF's Llama 2 + Pinecone + LangChain**"
      ],
      "metadata": {
        "id": "1tkXEA1_Tuen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step: 01: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "6bW_xszwT3kr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O5x2UqVLz3ED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6125de78-f6ba-44c5-86a0-5d6655e76ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.345-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.9 (from langchain)\n",
            "  Downloading langchain_core-0.0.9-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.345 langchain-core-0.0.9 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.11.17)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=deea0a85ed111acf6f3ad1646093abe066e1e6957f135bab411509c3b14efe63\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.1\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->xformers) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.22.post7\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.25.0 bitsandbytes-0.41.2.post2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install sentence_transformers\n",
        "!pip install pdf2image\n",
        "!pip install pypdf\n",
        "!pip install xformers\n",
        "!pip install bitsandbytes accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w0UTf9FstBBh",
        "outputId": "4d412581-b442-4612-e26c-4a2f2846e5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "l5z5BLvsUAzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "nnJgwY63z-Fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79e175a-6917-4a94-e679-48a00e7b81cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline, PromptTemplate\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "H7bqnhsK0AcE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Load the PDF Files**"
      ],
      "metadata": {
        "id": "GD8-ktmrUWEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = PyPDFDirectoryLoader(\"/content/drive/MyDrive/Qasper_Archive_First_set_100_Papers\")\n",
        "loader = PyPDFDirectoryLoader(\"/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "oeFhv1G3Mvwn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBByYClY2PRZ",
        "outputId": "7868773b-fd93-4089-dcc1-fb92045d7874"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxCi_PaWMxDZ",
        "outputId": "7e58b3ad-df2d-4a38-db26-70e261bbd29d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Learning to Compose Neural Networks for Question Answering\\nJacob Andreas andMarcus Rohrbach andTrevor Darrell andDan Klein\\nDepartment of Electrical Engineering and Computer Sciences\\nUniversity of California, Berkeley\\n{jda,rohrbach,trevor,klein }@eecs.berkeley.edu\\nAbstract\\nWe describe a question answering model that\\napplies to both images and structured knowl-\\nedge bases. The model uses natural lan-\\nguage strings to automatically assemble neu-\\nral networks from a collection of composable\\nmodules. Parameters for these modules are\\nlearned jointly with network-assembly param-\\neters via reinforcement learning, with only\\n(world, question, answer) triples as supervi-\\nsion. Our approach, which we term a dynamic\\nneural module network , achieves state-of-the-\\nart results on benchmark datasets in both vi-\\nsual and structured domains.\\n1 Introduction\\nThis paper presents a compositional, attentional\\nmodel for answering questions about a variety of\\nworld representations, including images and struc-\\ntured knowledge bases. The model translates from\\nquestions to dynamically assembled neural net-\\nworks, then applies these networks to world rep-\\nresentations (images or knowledge bases) to pro-\\nduce answers. We take advantage of two largely\\nindependent lines of work: on one hand, an exten-\\nsive literature on answering questions by mapping\\nfrom strings to logical representations of meaning;\\non the other, a series of recent successes in deep\\nneural models for image recognition and captioning.\\nBy constructing neural networks instead of logical\\nforms, our model leverages the best aspects of both\\nlinguistic compositionality and continuous represen-\\ntations.\\nOur model has two components, trained jointly:\\nﬁrst, a collection of neural “modules” that can be\\nfreely composed (Figure 1a); second, a network lay-\\nout predictor that assembles modules into complete\\ndeep networks tailored to each question (Figure 1b).\\nWhat cities are in Georgia?AtlantaandlookupGeorgiafindcity\\nGeorgiaAtlantaMontgomeryKnowledge sourcerelateinNetwork layout (Section 4.2)find[city]lookup[Georgia]relate[in]and(b)Module inventory (Section 4.1)findlookupandrelate(a)(c)\\n(d)Figure 1: A learned syntactic analysis (a) is used to assemble a\\ncollection of neural modules (b) into a deep neural network (c),\\nand applied to a world representation (d) to produce an answer.\\nPrevious work has used manually-speciﬁed modular\\nstructures for visual learning (Andreas et al., 2016).\\nHere we:\\n•learn a network structure predictor jointly with\\nmodule parameters themselves\\n•extend visual primitives from previous work to\\nreason over structured world representations\\nTraining data consists of (world, question, answer)\\ntriples: our approach requires no supervision of net-\\nwork layouts. We achieve state-of-the-art perfor-\\nmance on two markedly different question answer-\\ning tasks: one with questions about natural im-\\nages, and another with more compositional ques-\\ntions about United States geography.1\\n2 Deep networks as functional programs\\nWe begin with a high-level discussion of the kinds\\nof composed networks we would like to learn.\\n1We have released our code at http://github.com/\\njacobandreas/nmn2arXiv:1601.01705v4  [cs.CL]  7 Jun 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 0}), Document(page_content='Andreas et al. (2016) describe a heuristic ap-\\nproach for decomposing visual question answering\\ntasks into sequence of modular sub-problems. For\\nexample, the question What color is the bird? might\\nbe answered in two steps: ﬁrst, “where is the bird?”\\n(Figure 2a), second, “what color is that part of the\\nimage?” (Figure 2c). This ﬁrst step, a generic mod-\\nulecalled find , can be expressed as a fragment of\\na neural network that maps from image features and\\na lexical item (here bird) to a distribution over pix-\\nels. This operation is commonly referred to as the\\nattention mechanism , and is a standard tool for ma-\\nnipulating images (Xu et al., 2015) and text repre-\\nsentations (Hermann et al., 2015).\\nThe ﬁrst contribution of this paper is an exten-\\nsion and generalization of this mechanism to enable\\nfully-differentiable reasoning about more structured\\nsemantic representations. Figure 2b shows how the\\nsame module can be used to focus on the entity\\nGeorgia in a non-visual grounding domain; more\\ngenerally, by representing every entity in the uni-\\nverse of discourse as a feature vector, we can obtain\\na distribution over entities that corresponds roughly\\nto a logical set-valued denotation.\\nHaving obtained such a distribution, existing neu-\\nral approaches use it to immediately compute a\\nweighted average of image features and project back\\ninto a labeling decision—a describe module (Fig-\\nure 2c). But the logical perspective suggests a num-\\nber of novel modules that might operate on atten-\\ntions: e.g. combining them (by analogy to conjunc-\\ntion or disjunction) or inspecting them directly with-\\nout a return to feature space (by analogy to quantiﬁ-\\ncation, Figure 2d). These modules are discussed in\\ndetail in Section 4. Unlike their formal counterparts,\\nthey are differentiable end-to-end, facilitating their\\nintegration into learned models. Building on previ-\\nous work, we learn behavior for a collection of het-\\nerogeneous modules from (world, question, answer)\\ntriples.\\nThe second contribution of this paper is a model\\nfor learning to assemble such modules composition-\\nally. Isolated modules are of limited use—to ob-\\ntain expressive power comparable to either formal\\napproaches or monolithic deep networks, they must\\nbe composed into larger structures. Figure 2 shows\\nsimple examples of composed structures, but for\\nrealistic question-answering tasks, even larger net-\\nblack\\tand\\twhite\\nGeorgiaAtlantaMontgomeryGeorgia\\nAtlanta\\nMontgomery\\nexiststrue\\nfindbirddescribecolor\\nfindstate(a)(b)(c)(d)Figure 2: Simple neural module networks, corresponding to\\nthe questions What color is the bird? andAre there any states?\\n(a) A neural find module for computing an attention over\\npixels. (b) The same operation applied to a knowledge base.\\n(c) Using an attention produced by a lower module to identify\\nthe color of the region of the image attended to. (d) Performing\\nquantiﬁcation by evaluating an attention directly.\\nworks are required. Thus our goal is to automati-\\ncally induce variable-free, tree-structured computa-\\ntion descriptors. We can use a familiar functional\\nnotation from formal semantics (e.g. Liang et al.,\\n2011) to represent these computations.2We write\\nthe two examples in Figure 2 as\\n(describe[color] find[bird])\\nand\\n(exists find[state])\\nrespectively. These are network layouts : they spec-\\nify a structure for arranging modules (and their lex-\\nical parameters) into a complete network. Andreas\\net al. (2016) use hand-written rules to deterministi-\\ncally transform dependency trees into layouts, and\\nare restricted to producing simple structures like the\\nabove for non-synthetic data. For full generality, we\\nwill need to solve harder problems, like transform-\\ningWhat cities are in Georgia? (Figure 1) into\\n(and\\nfind[city]\\n(relate[in] lookup[Georgia]))\\nIn this paper, we present a model for learning to se-\\nlect such structures from a set of automatically gen-\\nerated candidates. We call this model a dynamic\\nneural module network .\\n2But note that unlike formal semantics, the behavior of the\\nprimitive functions here is itself unknown.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 1}), Document(page_content='3 Related work\\nThere is an extensive literature on database ques-\\ntion answering, in which strings are mapped to log-\\nical forms, then evaluated by a black-box execu-\\ntion model to produce answers. Supervision may be\\nprovided either by annotated logical forms (Wong\\nand Mooney, 2007; Kwiatkowski et al., 2010; An-\\ndreas et al., 2013) or from (world, question, answer)\\ntriples alone (Liang et al., 2011; Pasupat and Liang,\\n2015). In general the set of primitive functions\\nfrom which these logical forms can be assembled is\\nﬁxed, but one recent line of work focuses on induc-\\ning new predicates functions automatically, either\\nfrom perceptual features (Krishnamurthy and Kol-\\nlar, 2013) or the underlying schema (Kwiatkowski\\net al., 2013). The model we describe in this paper\\nhas a uniﬁed framework for handling both the per-\\nceptual and schema cases, and differs from existing\\nwork primarily in learning a differentiable execution\\nmodel with continuous evaluation results.\\nNeural models for question answering are also a\\nsubject of current interest. These include approaches\\nthat model the task directly as a multiclass classiﬁ-\\ncation problem (Iyyer et al., 2014), models that at-\\ntempt to embed questions and answers in a shared\\nvector space (Bordes et al., 2014) and attentional\\nmodels that select words from documents sources\\n(Hermann et al., 2015). Such approaches generally\\nrequire that answers can be retrieved directly based\\non surface linguistic features, without requiring in-\\ntermediate computation. A more structured ap-\\nproach described by Yin et al. (2015) learns a query\\nexecution model for database tables without any nat-\\nural language component. Previous efforts toward\\nunifying formal logic and representation learning in-\\nclude those of Grefenstette (2013), Krishnamurthy\\nand Mitchell (2013), Lewis and Steedman (2013),\\nand Beltagy et al. (2013).\\nThe visually-grounded component of this work\\nrelies on recent advances in convolutional net-\\nworks for computer vision (Simonyan and Zisser-\\nman, 2014), and in particular the fact that late convo-\\nlutional layers in networks trained for image recog-\\nnition contain rich features useful for other vision\\ntasks while preserving spatial information. These\\nfeatures have been used for both image captioning\\n(Xu et al., 2015) and visual QA (Yang et al., 2015).Most previous approaches to visual question an-\\nswering either apply a recurrent model to deep rep-\\nresentations of both the image and the question (Ren\\net al., 2015; Malinowski et al., 2015), or use the\\nquestion to compute an attention over the input im-\\nage, and then answer based on both the question and\\nthe image features attended to (Yang et al., 2015;\\nXu and Saenko, 2015). Other approaches include\\nthe simple classiﬁcation model described by Zhou\\net al. (2015) and the dynamic parameter prediction\\nnetwork described by Noh et al. (2015). All of\\nthese models assume that a ﬁxed computation can\\nbe performed on the image and question to compute\\nthe answer, rather than adapting the structure of the\\ncomputation to the question.\\nAs noted, Andreas et al. (2016) previously con-\\nsidered a simple generalization of these attentional\\napproaches in which small variations in the net-\\nwork structure per-question were permitted, with\\nthe structure chosen by (deterministic) syntactic pro-\\ncessing of questions. Other approaches in this gen-\\neral family include the “universal parser” sketched\\nby Bottou (2014), the graph transformer networks\\nof Bottou et al. (1997), the knowledge-based neu-\\nral networks of Towell and Shavlik (1994) and the\\nrecursive neural networks of Socher et al. (2013),\\nwhich use a ﬁxed tree structure to perform further\\nlinguistic analysis without any external world rep-\\nresentation. We are unaware of previous work that\\nsimultaneously learns both parameters for and struc-\\ntures of instance-speciﬁc networks.\\n4 Model\\nRecall that our goal is to map from questions and\\nworld representations to answers. This process in-\\nvolves the following variables:\\n1.wa world representation\\n2.xa question\\n3.yan answer\\n4.za network layout\\n5.θa collection of model parameters\\nOur model is built around two distributions: a lay-\\nout modelp(z|x;θℓ)which chooses a layout for a\\nsentence, and a execution model pz(y|w;θe)which\\napplies the network speciﬁed by ztow.\\nFor ease of presentation, we introduce these mod-\\nels in reverse order. We ﬁrst imagine that zis always', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 2}), Document(page_content='observed, and in Section 4.1 describe how to evalu-\\nate and learn modules parameterized by θewithin\\nﬁxed structures. In Section 4.2, we move to the real\\nscenario, where zis unknown. We describe how to\\npredict layouts from questions and learn θeandθℓ\\njointly without layout supervision.\\n4.1 Evaluating modules\\nGiven a layout z, we assemble the corresponding\\nmodules into a full neural network (Figure 1c), and\\napply it to the knowledge representation. Interme-\\ndiate results ﬂow between modules until an answer\\nis produced at the root. We denote the output of the\\nnetwork with layout zon input world wasJzKw;\\nwhen explicitly referencing the substructure of z, we\\ncan alternatively write Jm(h1,h2)Kfor a top-level\\nmodulemwith submodule outputs h1andh2. We\\nthen deﬁne the execution model:\\npz(y|w) = ( JzKw)y (1)\\n(This assumes that the root module of zproduces\\na distribution over labels y.) The set of possible\\nlayoutszis restricted by module type constraints :\\nsome modules (like find above) operate directly on\\nthe input representation, while others (like describe\\nabove) also depend on input from speciﬁc earlier\\nmodules. Two base types are considered in this pa-\\nper are Attention (a distribution over pixels or enti-\\nties) and Labels (a distribution over answers).\\nParameters are tied across multiple instances of\\nthe same module, so different instantiated networks\\nmay share some parameters but not others. Modules\\nhave both parameter arguments (shown in square\\nbrackets) and ordinary inputs (shown in parenthe-\\nses). Parameter arguments, like the running bird\\nexample in Section 2, are provided by the layout,\\nand are used to specialize module behavior for par-\\nticular lexical items. Ordinary inputs are the re-\\nsult of computation lower in the network. In ad-\\ndition to parameter-speciﬁc weights, modules have\\nglobal weights shared across all instances of the\\nmodule (but not shared with other modules). We\\nwriteA,a,B,b,... for global weights and ui,vifor\\nweights associated with the parameter argument i.\\n⊕and⊙denote (possibly broadcasted) elementwise\\naddition and multiplication respectively. The com-\\nplete set of global weights and parameter-speciﬁc\\nweights constitutes θe.Every module has access tothe world representation, represented as a collection\\nof vectorsw1,w2,... (orWexpressed as a matrix).\\nThe nonlinearity σdenotes a rectiﬁed linear unit.\\nThe modules used in this paper are shown below,\\nwith names and type constraints in the ﬁrst row and a\\ndescription of the module’s computation following.\\nLookup (→Attention )\\nlookup[ i]produces an attention focused entirely at the\\nindexf(i), where the relationship fbetween words\\nand positions in the input map is known ahead of time\\n(e.g. string matches on database ﬁelds).\\nJlookup[ i]K=ef(i) (2)\\nwhereeiis the basis vector that is 1in theith position\\nand 0 elsewhere.\\nFind (→Attention )\\nfind[ i]computes a distribution over indices by con-\\ncatenating the parameter argument with each position\\nof the input feature map, and passing the concatenated\\nvector through a MLP:\\nJfind[ i]K=softmax (a⊙σ(Bvi⊕CW⊕d))(3)\\nRelate (Attention→Attention )\\nrelate directs focus from one region of the input to\\nanother. It behaves much like the find module, but\\nalso conditions its behavior on the current region of\\nattentionh. Let ¯w(h) =∑\\nkhkwk, wherehkis the\\nkthelement ofh. Then,\\nJrelate[ i](h)K=softmax (a⊙\\nσ(Bvi⊕CW⊕D¯w(h)⊕e)) (4)\\nAnd (Attention *→Attention )\\nandperforms an operation analogous to set intersec-\\ntion for attentions. The analogy to probabilistic logic\\nsuggests multiplying probabilities:\\nJand(h1,h2,...)K=h1⊙h2⊙··· (5)\\nDescribe (Attention→Labels )\\ndescribe[ i]computes a weighted average of wunder\\nthe input attention. This average is then used to predict\\nan answer representation. With ¯was above,\\nJdescribe[ i](h)K=softmax (Aσ(B¯w(h)+vi))(6)\\nExists (Attention→Labels )\\nexists is the existential quantiﬁer, and inspects the\\nincoming attention directly to produce a label, rather\\nthan an intermediate feature vector like describe :\\nJexists] (h)K=softmax((\\nmax\\nkhk)\\na+b)\\n(7)', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 3}), Document(page_content='What cities are in Georgia?whatcitybeinGeorgia\\nfind[city]relate[in]lookup[Georgia]relate[in]...lookup[Georgia]find[city]and(a)(b)(c)(d)relate[in]lookup[Georgia]Figure 3: Generation of layout candidates. The input sentence\\n(a) is represented as a dependency parse (b). Fragments of this\\ndependency parse are then associated with appropriate modules\\n(c), and these fragments are assembled into full layouts (d).\\nWithzobserved, the model we have described\\nso far corresponds largely to that of Andreas et al.\\n(2016), though the module inventory is different—\\nin particular, our new exists andrelate modules\\ndo not depend on the two-dimensional spatial struc-\\nture of the input. This enables generalization to non-\\nvisual world representations.\\nLearning in this simpliﬁed setting is straightfor-\\nward. Assuming the top-level module in each layout\\nis adescribe orexists module, the fully- instan-\\ntiated network corresponds to a distribution over la-\\nbels conditioned on layouts. To train, we maximize∑\\n(w,y,z )logpz(y|w;θe)directly. This can be under-\\nstood as a parameter-tying scheme, where the deci-\\nsions about which parameters to tie are governed by\\nthe observed layouts z.\\n4.2 Assembling networks\\nNext we describe the layout model p(z|x;θℓ). We\\nﬁrst use a ﬁxed syntactic parse to generate a small\\nset of candidate layouts, analogously to the way\\na semantic grammar generates candidate semantic\\nparses in previous work (Berant and Liang, 2014).\\nA semantic parse differs from a syntactic parse\\nin two primary ways. First, lexical items must bemapped onto a (possibly smaller) set of semantic\\nprimitives. Second, these semantic primitives must\\nbe combined into a structure that closely, but not ex-\\nactly, parallels the structure provided by syntax. For\\nexample, state andprovince might need to be identi-\\nﬁed with the same ﬁeld in a database schema, while\\nall states have a capital might need to be identiﬁed\\nwith the correct ( in situ ) quantiﬁer scope.\\nWhile we cannot avoid the structure selection\\nproblem, continuous representations simplify the\\nlexical selection problem. For modules that accept\\na vector parameter, we associate these parameters\\nwith words rather than semantic tokens, and thus\\nturn the combinatorial optimization problem asso-\\nciated with lexicon induction into a continuous one.\\nNow, in order to learn that province andstate have\\nthe same denotation, it is sufﬁcient to learn that their\\nassociated parameters are close in some embedding\\nspace—a task amenable to gradient descent. (Note\\nthat this is easy only in an optimizability sense,\\nand not an information-theoretic one—we must still\\nlearn to associate each independent lexical item with\\nthe correct vector.) The remaining combinatorial\\nproblem is to arrange the provided lexical items into\\nthe right computational structure. In this respect,\\nlayout prediction is more like syntactic parsing than\\nordinary semantic parsing, and we can rely on an\\noff-the-shelf syntactic parser to get most of the way\\nthere. In this work, syntactic structure is provided by\\nthe Stanford dependency parser (De Marneffe and\\nManning, 2008).\\nThe construction of layout candidates is depicted\\nin Figure 3, and proceeds as follows:\\n1. Represent the input sentence as a dependency\\ntree.\\n2. Collect all nouns, verbs, and prepositional\\nphrases that are attached directly to a wh-word\\nor copula.\\n3. Associate each of these with a layout frag-\\nment: Ordinary nouns and verbs are mapped\\nto a single find module. Proper nouns to a sin-\\nglelookup module. Prepositional phrases are\\nmapped to a depth-2 fragment, with a relate\\nmodule for the preposition above a find mod-\\nule for the enclosed head noun.\\n4. Form subsets of this set of layout fragments.\\nFor each subset, construct a layout candidate by', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 4}), Document(page_content='joining all fragments with an andmodule, and\\ninserting either a measure ordescribe module\\nat the top (each subset thus results in two parse\\ncandidates.)\\nAll layouts resulting from this process feature a\\nrelatively ﬂat tree structure with at most one con-\\njunction and one quantiﬁer. This is a strong sim-\\nplifying assumption, but appears sufﬁcient to cover\\nmost of the examples that appear in both of our\\ntasks. As our approach includes both categories, re-\\nlations and simple quantiﬁcation, the range of phe-\\nnomena considered is generally broader than pre-\\nvious perceptually-grounded QA work (Krishna-\\nmurthy and Kollar, 2013; Matuszek et al., 2012).\\nHaving generated a set of candidate parses, we\\nneed to score them. This is a ranking problem;\\nas in the rest of our approach, we solve it using\\nstandard neural machinery. In particular, we pro-\\nduce an LSTM representation of the question, a\\nfeature-based representation of the query, and pass\\nboth representations through a multilayer perceptron\\n(MLP). The query feature vector includes indicators\\non the number of modules of each type present, as\\nwell as their associated parameter arguments. While\\none can easily imagine a more sophisticated parse-\\nscoring model, this simple approach works well for\\nour tasks.\\nFormally, for a question x, lethq(x)be an LSTM\\nencoding of the question (i.e. the last hidden layer of\\nan LSTM applied word-by-word to the input ques-\\ntion). Let{z1,z2,...}be the proposed layouts for\\nx, and letf(zi)be a feature vector representing the\\nith layout. Then the score s(zi|x)for the layout ziis\\ns(zi|x) =a⊤σ(Bhq(x) +Cf(zi) +d)(8)\\ni.e. the output of an MLP with inputs hq(x)and\\nf(zi), and parameters θℓ={a,B,C,d}. Finally,\\nwe normalize these scores to obtain a distribution:\\np(zi|x;θℓ) =es(zi|x)/n∑\\nj=1es(zj|x)(9)\\nHaving deﬁned a layout selection module\\np(z|x;θℓ)and a network execution model\\npz(y|w;θe), we are ready to deﬁne a model\\nfor predicting answers given only (world, question)\\npairs. The key constraint is that we want to min-\\nimize evaluations of pz(y|w;θe)(which involvesexpensive application of a deep network to a large\\ninput representation), but can tractably evaluate\\np(z|x;θℓ)for allz(which involves application\\nof a shallow network to a relatively small set of\\ncandidates). This is the opposite of the situation\\nusually encountered semantic parsing, where calls\\nto the query execution model are fast but the set of\\ncandidate parses is too large to score exhaustively.\\nIn fact, the problem more closely resembles the\\nscenario faced by agents in the reinforcement learn-\\ning setting (where it is cheap to score actions, but\\npotentially expensive to execute them and obtain re-\\nwards). We adopt a common approach from that lit-\\nerature, and express our model as a stochastic pol-\\nicy. Under this policy, we ﬁrst sample a layoutz\\nfrom a distribution p(z|x;θℓ), and then apply zto\\nthe knowledge source and obtain a distribution over\\nanswersp(y|z,w;θe).\\nAfterzis chosen, we can train the execution\\nmodel directly by maximizing logp(y|z,w;θe)with\\nrespect toθeas before (this is ordinary backprop-\\nagation). Because the hard selection of zis non-\\ndifferentiable, we optimize p(z|x;θℓ)using a policy\\ngradient method. The gradient of the reward surface\\nJwith respect to the parameters of the policy is\\n∇J(θℓ) =E[∇logp(z|x;θℓ)·r] (10)\\n(this is the REINFORCE rule (Williams, 1992)). Here\\nthe expectation is taken with respect to rollouts of\\nthe policy, and ris the reward. Because our goal is\\nto select the network that makes the most accurate\\npredictions, we take the reward to be identically the\\nnegative log-probability from the execution phase,\\ni.e.\\nE[(∇logp(z|x;θℓ))·logp(y|z,w;θe)] (11)\\nThus the update to the layout-scoring model at each\\ntimestep is simply the gradient of the log-probability\\nof the chosen layout, scaled by the accuracy of that\\nlayout’s predictions. At training time, we approxi-\\nmate the expectation with a single rollout, so at each\\nstep we update θℓin the direction (∇logp(z|x;θℓ))·\\nlogp(y|z,w;θe)for a singlez∼p(z|x;θℓ).θeand\\nθℓare optimized using ADADELTA (Zeiler, 2012)\\nwithρ= 0.95,ε= 1e−6and gradient clipping at a\\nnorm of 10.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 5}), Document(page_content='What is in the sheep’s ear? What color is she\\nwearing?What is the man\\ndragging?\\n(describe[what]\\n(and find[sheep]\\nfind[ear]))(describe[color]\\nfind[wear])(describe[what]\\nfind[man])\\ntag white boat (board)\\nFigure 4: Sample outputs for the visual question answering\\ntask. The second row shows the ﬁnal attention provided as in-\\nput to the top-level describe module. For the ﬁrst two exam-\\nples, the model produces reasonable parses, attends to the cor-\\nrect region of the images (the ear and the woman’s clothing),\\nand generates the correct answer. In the third image, the verb is\\ndiscarded and a wrong answer is produced.\\n5 Experiments\\nThe framework described in this paper is general,\\nand we are interested in how well it performs on\\ndatasets of varying domain, size and linguistic com-\\nplexity. To that end, we evaluate our model on tasks\\nat opposite extremes of both these criteria: a large\\nvisual question answering dataset, and a small col-\\nlection of more structured geography questions.\\n5.1 Questions about images\\nOur ﬁrst task is the recently-introduced Visual Ques-\\ntion Answering challenge (VQA) (Antol et al.,\\n2015). The VQA dataset consists of more than\\n200,000 images paired with human-annotated ques-\\ntions and answers, as in Figure 4.\\nWe use the VQA 1.0 release, employing the de-\\nvelopment set for model selection and hyperparam-\\neter tuning, and reporting ﬁnal results from the eval-\\nuation server on the test-standard set. For the ex-\\nperiments described in this section, the input feature\\nrepresentations wiare computed by the the ﬁfth con-\\nvolutional layer of a 16-layer VGGNet after pooling\\n(Simonyan and Zisserman, 2014). Input images are\\nscaled to 448×448 before computing their represen-\\ntations. We found that performance on this task wastest-dev test-std\\nYes/No Number Other All All\\nZhou (2015) 76.6 35.0 42.6 55.7 55.9\\nNoh (2015) 80.7 37.2 41.7 57.2 57.4\\nYang (2015) 79.3 36.6 46.1 58.7 58.9\\nNMN 81.2 38.0 44.0 58.6 58.7\\nD-NMN 81.1 38.6 45.5 59.4 59.4\\nTable 1: Results on the VQA test server. NMN is the\\nparameter-tying model from Andreas et al. (2015), and D-NMN\\nis the model described in this paper.\\nbest if the candidate layouts were relatively simple:\\nonly describe ,andandfind modules are used, and\\nlayouts contain at most two conjuncts.\\nOne weakness of this basic framework is a difﬁ-\\nculty modeling prior knowledge about answers (of\\nthe form most bears are brown ). This kinds of lin-\\nguistic “prior” is essential for the VQA task, and\\neasily incorporated. We simply introduce an extra\\nhidden layer for recombining the ﬁnal module net-\\nwork output with the input sentence representation\\nhq(x)(see Equation 8), replacing Equation 1 with:\\nlogpz(y|w,x) = (Ahq(x) +BJzKw)y (12)\\n(Now modules with output type Labels should\\nbe understood as producing an answer embedding\\nrather than a distribution over answers.) This allows\\nthe question to inﬂuence the answer directly.\\nResults are shown in Table 1. The use of dynamic\\nnetworks provides a small gain, most noticeably on\\n”other” questions. We achieve state-of-the-art re-\\nsults on this task, outperforming a highly effective\\nvisual bag-of-words model (Zhou et al., 2015), a\\nmodel with dynamic network parameter prediction\\n(but ﬁxed network structure) (Noh et al., 2015), a\\nmore conventional attentional model (Yang et al.,\\n2015), and a previous approach using neural mod-\\nule networks with no structure prediction (Andreas\\net al., 2016).\\nSome examples are shown in Figure 4. In general,\\nthe model learns to focus on the correct region of the\\nimage, and tends to consider a broad window around\\nthe region. This facilitates answering questions like\\nWhere is the cat? , which requires knowledge of the\\nsurroundings as well as the object in question.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 6}), Document(page_content='Accuracy\\nModel GeoQA GeoQA+Q\\nLSP-F 48 –\\nLSP-W 51 –\\nNMN 51.7 35.7\\nD-NMN 54.3 42.9\\nTable 2: Results on the GeoQA dataset, and the GeoQA\\ndataset with quantiﬁcation. Our approach outperforms both a\\npurely logical model (LSP-F) and a model with learned percep-\\ntual predicates (LSP-W) on the original dataset, and a ﬁxed-\\nstructure NMN under both evaluation conditions.\\n5.2 Questions about geography\\nThe next set of experiments we consider focuses\\non GeoQA, a geographical question-answering\\ntask ﬁrst introduced by Krishnamurthy and Kollar\\n(2013). This task was originally paired with a vi-\\nsual question answering task much simpler than the\\none just discussed, and is appealing for a number\\nof reasons. In contrast to the VQA dataset, GeoQA\\nis quite small, containing only 263 examples. Two\\nbaselines are available: one using a classical se-\\nmantic parser backed by a database, and another\\nwhich induces logical predicates using linear clas-\\nsiﬁers over both spatial and distributional features.\\nThis allows us to evaluate the quality of our model\\nrelative to other perceptually grounded logical se-\\nmantics, as well as strictly logical approaches.\\nThe GeoQA domain consists of a set of entities\\n(e.g. states, cities, parks) which participate in vari-\\nous relations (e.g. north-of, capital-of). Here we take\\nthe world representation to consist of two pieces: a\\nset of category features (used by the find module)\\nand a different set of relational features (used by the\\nrelate module). For our experiments, we use a sub-\\nset of the features originally used by Krishnamurthy\\net al. The original dataset includes no quantiﬁers,\\nand treats the questions What cities are in Texas?\\nandAre there any cities in Texas? identically. Be-\\ncause we are interested in testing the parser’s ability\\nto predict a variety of different structures, we intro-\\nduce a new version of the dataset, GeoQA+Q, which\\ndistinguishes these two cases, and expects a Boolean\\nanswer to questions of the second kind.\\nResults are shown in Table 2. As in the orig-\\ninal work, we report the results of leave-one-\\nenvironment-out cross-validation on the set of 10 en-Is Key Largo an island?\\n(exists (and lookup[key-largo] find[island]))\\nyes: correct\\nWhat national parks are in Florida?\\n(and find[park] (relate[in] lookup[florida]))\\neverglades : correct\\nWhat are some beaches in Florida?\\n(exists (and lookup[beach]\\n(relate[in] lookup[florida])))\\nyes(daytona-beach): wrong parse\\nWhat beach city is there in Florida?\\n(and lookup[beach] lookup[city]\\n(relate[in] lookup[florida]))\\n[none] (daytona-beach): wrong module behavior\\nFigure 5: Example layouts and answers selected by the model\\non the GeoQA dataset. For incorrect predictions, the correct\\nanswer is shown in parentheses.\\nvironments. Our dynamic model (D-NMN) outper-\\nforms both the logical (LSP-F) and perceptual mod-\\nels (LSP-W) described by (Krishnamurthy and Kol-\\nlar, 2013), as well as a ﬁxed-structure neural mod-\\nule net (NMN). This improvement is particularly\\nnotable on the dataset with quantiﬁers, where dy-\\nnamic structure prediction produces a 20% relative\\nimprovement over the ﬁxed baseline. A variety of\\npredicted layouts are shown in Figure 5.\\n6 Conclusion\\nWe have introduced a new model, the dynamic neu-\\nral module network , for answering queries about\\nboth structured and unstructured sources of informa-\\ntion. Given only (question, world, answer) triples\\nas training data, the model learns to assemble neu-\\nral networks on the ﬂy from an inventory of neural\\nmodels, and simultaneously learns weights for these\\nmodules so that they can be composed into novel\\nstructures. Our approach achieves state-of-the-art\\nresults on two tasks. We believe that the success of\\nthis work derives from two factors:\\nContinuous representations improve the expres-\\nsiveness and learnability of semantic parsers : by re-\\nplacing discrete predicates with differentiable neural\\nnetwork fragments, we bypass the challenging com-\\nbinatorial optimization problem associated with in-\\nduction of a semantic lexicon. In structured world', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 7}), Document(page_content='representations, neural predicate representations al-\\nlow the model to invent reusable attributes and re-\\nlations not expressed in the schema. Perhaps more\\nimportantly, we can extend compositional question-\\nanswering machinery to complex, continuous world\\nrepresentations like images.\\nSemantic structure prediction improves general-\\nization in deep networks : by replacing a ﬁxed net-\\nwork topology with a dynamic one, we can tailor the\\ncomputation performed to each problem instance,\\nusing deeper networks for more complex questions\\nand representing combinatorially many queries with\\ncomparatively few parameters. In practice, this re-\\nsults in considerable gains in speed and sample efﬁ-\\nciency, even with very little training data.\\nThese observations are not limited to the question\\nanswering domain, and we expect that they can be\\napplied similarly to tasks like instruction following,\\ngame playing, and language generation.\\nAcknowledgments\\nJA is supported by a National Science Foundation\\nGraduate Fellowship. MR is supported by a fellow-\\nship within the FIT weltweit-Program of the German\\nAcademic Exchange Service (DAAD). This work\\nwas additionally supported by DARPA, AFRL, DoD\\nMURI award N000141110688, NSF awards IIS-\\n1427425 and IIS-1212798, and the Berkeley Vision\\nand Learning Center.\\nReferences\\nJacob Andreas, Andreas Vlachos, and Stephen Clark.\\n2013. Semantic parsing as machine translation. In\\nProceedings of the Annual Meeting of the Association\\nfor Computational Linguistics , Soﬁa, Bulgaria.\\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\\nDan Klein. 2016. Neural module networks. In Pro-\\nceedings of the Conference on Computer Vision and\\nPattern Recognition .\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\\nDevi Parikh. 2015. VQA: Visual question answer-\\ning. In Proceedings of the International Conference\\non Computer Vision .\\nIslam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-\\nrette, Katrin Erk, and Raymond Mooney. 2013. Mon-\\ntague meets markov: Deep semantics with probabilis-\\ntic logical form. Proceedings of the Joint Conferenceon Distributional and Logical Semantics , pages 11–\\n21.\\nJonathan Berant and Percy Liang. 2014. Semantic pars-\\ning via paraphrasing. In Proceedings of the Annual\\nMeeting of the Association for Computational Linguis-\\ntics, volume 7, page 92.\\nAntoine Bordes, Sumit Chopra, and Jason Weston. 2014.\\nQuestion answering with subgraph embeddings. Pro-\\nceedings of the Conference on Empirical Methods in\\nNatural Language Processing .\\nL´eon Bottou, Yoshua Bengio, and Yann Le Cun. 1997.\\nGlobal training of document processing systems us-\\ning graph transformer networks. In Proceedings of the\\nConference on Computer Vision and Pattern Recogni-\\ntion, pages 489–494. IEEE.\\nL´eon Bottou. 2014. From machine learning to machine\\nreasoning. Machine learning , 94(2):133–149.\\nMarie-Catherine De Marneffe and Christopher D Man-\\nning. 2008. The Stanford typed dependencies repre-\\nsentation. In Proceedings of the International Confer-\\nence on Computational Linguistics , pages 1–8.\\nEdward Grefenstette. 2013. Towards a formal distribu-\\ntional semantics: Simulating logical calculi with ten-\\nsors. Joint Conference on Lexical and Computational\\nSemantics .\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In Advances in Neural Information\\nProcessing Systems , pages 1684–1692.\\nMohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,\\nRichard Socher, and Hal Daum ´e III. 2014. A neu-\\nral network for factoid question answering over para-\\ngraphs. In Proceedings of the Conference on Empiri-\\ncal Methods in Natural Language Processing .\\nJayant Krishnamurthy and Thomas Kollar. 2013. Jointly\\nlearning to parse and perceive: connecting natural lan-\\nguage to the physical world. Transactions of the Asso-\\nciation for Computational Linguistics .\\nJayant Krishnamurthy and Tom Mitchell. 2013. Vec-\\ntor space semantic parsing: A framework for compo-\\nsitional vector space models. In Proceedings of the\\nACL Workshop on Continuous Vector Space Models\\nand their Compositionality .\\nTom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-\\nter, and Mark Steedman. 2010. Inducing probabilis-\\ntic CCG grammars from logical form with higher-\\norder uniﬁcation. In Proceedings of the Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 1223–1233, Cambridge, Massachusetts.\\nTom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke\\nZettlemoyer. 2013. Scaling semantic parsers with on-\\nthe-ﬂy ontology matching. In Proceedings of the Con-', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 8}), Document(page_content='ference on Empirical Methods in Natural Language\\nProcessing .\\nMike Lewis and Mark Steedman. 2013. Combining\\ndistributional and logical semantics. Transactions of\\nthe Association for Computational Linguistics , 1:179–\\n192.\\nPercy Liang, Michael Jordan, and Dan Klein. 2011.\\nLearning dependency-based compositional semantics.\\nInProceedings of the Human Language Technology\\nConference of the Association for Computational Lin-\\nguistics , pages 590–599, Portland, Oregon.\\nMateusz Malinowski, Marcus Rohrbach, and Mario Fritz.\\n2015. Ask your neurons: A neural-based approach to\\nanswering questions about images. In Proceedings of\\nthe International Conference on Computer Vision .\\nCynthia Matuszek, Nicholas FitzGerald, Luke Zettle-\\nmoyer, Liefeng Bo, and Dieter Fox. 2012. A joint\\nmodel of language and perception for grounded at-\\ntribute learning. In International Conference on Ma-\\nchine Learning .\\nHyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han.\\n2015. Image question answering using convolutional\\nneural network with dynamic parameter prediction.\\narXiv preprint arXiv:1511.05756 .\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables. In\\nProceedings of the Annual Meeting of the Association\\nfor Computational Linguistics .\\nMengye Ren, Ryan Kiros, and Richard Zemel. 2015. Ex-\\nploring models and data for image question answer-\\ning. In Advances in Neural Information Processing\\nSystems .\\nK Simonyan and A Zisserman. 2014. Very deep con-\\nvolutional networks for large-scale image recognition.\\narXiv preprint arXiv:1409.1556 .\\nRichard Socher, John Bauer, Christopher D. Manning,\\nand Andrew Y . Ng. 2013. Parsing with compositional\\nvector grammars. In Proceedings of the Annual Meet-\\ning of the Association for Computational Linguistics .\\nGeoffrey G Towell and Jude W Shavlik. 1994.\\nKnowledge-based artiﬁcial neural networks. Artiﬁcial\\nIntelligence , 70(1):119–165.\\nRonald J Williams. 1992. Simple statistical gradient-\\nfollowing algorithms for connectionist reinforcement\\nlearning. Machine learning , 8(3-4):229–256.\\nYuk Wah Wong and Raymond J. Mooney. 2007. Learn-\\ning synchronous grammars for semantic parsing with\\nlambda calculus. In Proceedings of the Annual Meet-\\ning of the Association for Computational Linguistics ,\\nvolume 45, page 960.\\nHuijuan Xu and Kate Saenko. 2015. Ask, attend\\nand answer: Exploring question-guided spatial atten-\\ntion for visual question answering. arXiv preprint\\narXiv:1511.05234 .Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\\nAaron Courville, Ruslan Salakhutdinov, Richard\\nZemel, and Yoshua Bengio. 2015. Show, attend\\nand tell: Neural image caption generation with visual\\nattention. In International Conference on Machine\\nLearning .\\nZichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,\\nand Alex Smola. 2015. Stacked attention net-\\nworks for image question answering. arXiv preprint\\narXiv:1511.02274 .\\nPengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.\\n2015. Neural enquirer: Learning to query tables.\\narXiv preprint arXiv:1512.00965 .\\nMatthew D Zeiler. 2012. ADADELTA: An\\nadaptive learning rate method. arXiv preprint\\narXiv:1212.5701 .\\nBolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar,\\nArthur Szlam, and Rob Fergus. 2015. Simple base-\\nline for visual question answering. arXiv preprint\\narXiv:1512.02167 .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 9}), Document(page_content='Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing\\nShashi Narayan, Siva Reddy and Shay B. Cohen\\nSchool of Informatics, University of Edinburgh\\n10 Crichton Street, Edinburgh, EH8 9LE, UK\\nshashi.narayan@ed.ac.uk ,siva.reddy@ed.ac.uk ,scohen@inf.ed.ac.uk\\nAbstract\\nOne of the limitations of semantic parsing ap-\\nproaches to open-domain question answering\\nis the lexicosyntactic gap between natural lan-\\nguage questions and knowledge base entries\\n– there are many ways to ask a question, all\\nwith the same answer. In this paper we pro-\\npose to bridge this gap by generating para-\\nphrases of the input question with the goal that\\nat least one of them will be correctly mapped\\nto a knowledge-base query. We introduce a\\nnovel grammar model for paraphrase genera-\\ntion that does not require any sentence-aligned\\nparaphrase corpus. Our key idea is to leverage\\nthe ﬂexibility and scalability of latent-variable\\nprobabilistic context-free grammars to sample\\nparaphrases. We do an extrinsic evaluation of\\nour paraphrases by plugging them into a se-\\nmantic parser for Freebase. Our evaluation\\nexperiments on the WebQuestions benchmark\\ndataset show that the performance of the se-\\nmantic parser improves over strong baselines.\\n1 Introduction\\nSemantic parsers map sentences onto logical forms\\nthat can be used to query databases (Zettlemoyer\\nand Collins, 2005; Wong and Mooney, 2006), in-\\nstruct robots (Chen and Mooney, 2011), extract in-\\nformation (Krishnamurthy and Mitchell, 2012), or\\ndescribe visual scenes (Matuszek et al., 2012). In\\nthis paper we consider the problem of semantically\\nparsing questions into Freebase logical forms for\\nthe goal of question answering. Current systems\\naccomplish this by learning task-speciﬁc grammars\\n(Berant et al., 2013), strongly-typed CCG gram-\\nmars (Kwiatkowski et al., 2013; Reddy et al., 2014),or neural networks without requiring any grammar\\n(Yih et al., 2015). These methods are sensitive to the\\nwords used in a question and their word order, mak-\\ning them vulnerable to unseen words and phrases.\\nFurthermore, mismatch between natural language\\nand Freebase makes the problem even harder. For\\nexample, Freebase expresses the fact that “Czech is\\nthe ofﬁcial language of Czech Republic” (encoded\\nas a graph), whereas to answer a question like “What\\ndo people in Czech Republic speak?” one should in-\\nferpeople in Czech Republic refers to Czech Repub-\\nlicandWhat refers to the language andspeak refers\\nto the predicate ofﬁcial language .\\nWe address the above problems by using para-\\nphrases of the original question. Paraphrasing\\nhas shown to be promising for semantic pars-\\ning (Fader et al., 2013; Berant and Liang, 2014;\\nWang et al., 2015). We propose a novel frame-\\nwork for paraphrasing using latent-variable PCFGs\\n(L-PCFGs). Earlier approaches to paraphrasing\\nused phrase-based machine translation for text-\\nbased QA (Duboue and Chu-Carroll, 2006; Rie-\\nzler et al., 2007), or hand annotated grammars\\nfor KB-based QA (Berant and Liang, 2014). We\\nﬁnd that phrase-based statistical machine transla-\\ntion (MT) approaches mainly produce lexical para-\\nphrases without much syntactic diversity, whereas\\nour grammar-based approach is capable of produc-\\ning both lexically and syntactically diverse para-\\nphrases. Unlike MT based approaches, our system\\ndoes not require aligned parallel paraphrase corpora.\\nIn addition we do not require hand annotated gram-\\nmars for paraphrase generation but instead learn the\\ngrammar directly from a large scale question corpus.arXiv:1601.06068v2  [cs.CL]  5 Aug 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 0}), Document(page_content='The main contributions of this paper are two fold.\\nFirst, we present an algorithm ( §2) to generate para-\\nphrases using latent-variable PCFGs. We use the\\nspectral method of Narayan and Cohen (2015) to\\nestimate L-PCFGs on a large scale question tree-\\nbank. Our grammar model leads to a robust and an\\nefﬁcient system for paraphrase generation in open-\\ndomain question answering. While CFGs have been\\nexplored for paraphrasing using bilingual parallel\\ncorpus (Ganitkevitch et al., 2013), ours is the ﬁrst\\nimplementation of CFG that uses only monolingual\\ndata. Second, we show that generated paraphrases\\ncan be used to improve semantic parsing of ques-\\ntions into Freebase logical forms ( §3). We build on\\na strong baseline of Reddy et al. (2014) and show\\nthat our grammar model competes with MT base-\\nline even without using any parallel paraphrase re-\\nsources.\\n2 Paraphrase Generation Using\\nGrammars\\nOur paraphrase generation algorithm is based on\\na model in the form of an L-PCFG. L-PCFGs are\\nPCFGs where the nonterminals are reﬁned with la-\\ntent states that provide some contextual information\\nabout each node in a given derivation. L-PCFGs\\nhave been used in various ways, most commonly\\nfor syntactic parsing (Prescher, 2005; Matsuzaki et\\nal., 2005; Petrov et al., 2006; Cohen et al., 2013;\\nNarayan and Cohen, 2015; Narayan and Cohen,\\n2016).\\nIn our estimation of L-PCFGs, we use the spectral\\nmethod of Narayan and Cohen (2015), instead of us-\\ning EM, as has been used in the past by Matsuzaki\\net al. (2005) and Petrov et al. (2006). The spectral\\nmethod we use enables the choice of a set of feature\\nfunctions that indicate the latent states, which proves\\nto be useful in our case. It also leads to sparse gram-\\nmar estimates and compact models.\\nThe spectral method works by identifying feature\\nfunctions for “inside” and “outside” trees, and then\\nclusters them into latent states. Then it follows with\\na maximum likelihood estimation step, that assumes\\nthe latent states are represented by clusters obtained\\nthrough the feature function clustering. For more de-\\ntails about these constructions, we refer the reader to\\nCohen et al. (2013) and Narayan and Cohen (2015).\\nThe rest of this section describes our paraphrasegeneration algorithm.\\n2.1 Paraphrases Generation Algorithm\\nWe deﬁne our paraphrase generation task as a sam-\\npling problem from an L-PCFG Gsyn, which is esti-\\nmated from a large corpus of parsed questions. Once\\nthis grammar is estimated, our algorithm follows a\\npipeline with two major steps.\\nWe ﬁrst build a word lattice Wqfor the input ques-\\ntionq.1We use the lattice to constrain our para-\\nphrases to a speciﬁc choice of words and phrases\\nthat can be used. Once this lattice is created, a gram-\\nmarG′\\nsynis then extracted from Gsyn. This grammar\\nis constrained to the lattice.\\nWe experiment with three ways of constructing\\nword lattices: na ¨ıve word lattices representing the\\nwords from the input question only, word lattices\\nconstructed with the Paraphrase Database (Ganitke-\\nvitch et al., 2013) and word lattices constructed with\\na bi-layered L-PCFG, described in §2.2. For exam-\\nple, Figure 1 shows an example word lattice for the\\nquestion What language do people in Czech Repub-\\nlic speak? using the lexical and phrasal rules from\\nthe PPDB.2\\nOnceG′\\nsynis generated, we sample paraphrases of\\nthe input question q. These paraphrases are further\\nﬁltered with a classiﬁer to improve the precision of\\nthe generated paraphrases.\\nL-PCFG Estimation We train the L-PCFG Gsyn\\non the Paralex corpus (Fader et al., 2013). Par-\\nalex is a large monolingual parallel corpus, contain-\\ning 18 million pairs of question paraphrases with\\n2.4M distinct questions in the corpus. It is suit-\\nable for our task of generating paraphrases since\\nits large scale makes our model robust for open-\\ndomain questions. We construct a treebank by pars-\\ning 2.4M distinct questions from Paralex using the\\nBLLIP parser (Charniak and Johnson, 2005).3\\nGiven the treebank, we use the spectral algorithm\\nof Narayan and Cohen (2015) to learn an L-PCFG\\n1Word lattices, formally weighted ﬁnite state automata, have\\nbeen used in previous works for paraphrase generation (Langk-\\nilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al.,\\n2003; Quirk et al., 2004). We use an unweighted variant of\\nword lattices in our algorithm.\\n2For our experiments, we extract rules from the PPDB-\\nSmall to maintain the high precision (Ganitkevitch et al., 2013).\\n3We ignore the Paralex alignments for training Gsyn.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 1}), Document(page_content=\"what kindjust whatwhatexactly whatwhat sortlanguagelinguisticdopeoplemembers of the publichuman beingspeople 'sthe populationthe citizensinCzechCzech RepublicCzechthe Czech RepublicCzeRepublicspeaktalking aboutexpress itselftalk aboutto talkis speaking?Figure 1: An example word lattice for the question What language do people in Czech Republic speak? using the\\nlexical and phrasal rules from the PPDB.\\nfor constituency parsing to learn Gsyn. We follow\\nNarayan and Cohen (2015) and use the same fea-\\nture functions for the inside and outside trees as\\nthey use, capturing contextual syntactic information\\nabout nonterminals. We refer the reader to Narayan\\nand Cohen (2015) for more detailed description of\\nthese features. In our experiments, we set the num-\\nber of latent states to 24.\\nOnce we estimate Gsynfrom the Paralex corpus,\\nwe restrict it for each question to a grammar G′\\nsynby\\nkeeping only the rules that could lead to a derivation\\nover the lattice. This step is similar to lexical prun-\\ning in standard grammar-based generation process\\nto avoid an intermediate derivation which can never\\nlead to a successful derivation (Koller and Striegnitz,\\n2002; Narayan and Gardent, 2012).\\nParaphrase Sampling Sampling a question from\\nthe grammar G′\\nsynis done by recursively sampling\\nnodes in the derivation tree, together with their la-\\ntent states, in a top-down breadth-ﬁrst fashion. Sam-\\npling from the pruned grammar G′\\nsynraises an is-\\nsue of oversampling words that are more frequent\\nin the training data. To lessen this problem, we fol-\\nlow a controlled sampling approach where sampling\\nis guided by the word lattice Wq. Once a word w\\nfrom a path einWqis sampled, all other parallel\\nor conﬂicting paths to eare removed from Wq. For\\nexample, generating for the word lattice in Figure\\n1, when we sample the word citizens , we drop out\\nthe paths “human beings” ,“people’s” ,“the pop-\\nulation” ,“people” and “members of the public”\\nfromWqand accordingly update the grammar. The\\ncontrolled sampling ensures that each sampled ques-\\ntion uses words from a single start-to-end path in\\nWq. For example, we could sample a question whatis Czech Republic ’s language? by sampling words\\nfrom the path (what, language, do, people ’s, in,\\nCzech, Republic, is speaking, ?) in Figure 1. We\\nrepeat this sampling process to generate multiple po-\\ntential paraphrases.\\nThe resulting generation algorithm has multiple\\nadvantages over existing grammar generation meth-\\nods. First, the sampling from an L-PCFG grammar\\nlessens the lexical ambiguity problem evident in lex-\\nicalized grammars such as tree adjoining grammars\\n(Narayan and Gardent, 2012) and combinatory cate-\\ngorial grammars (White, 2004). Our grammar is not\\nlexicalized, only unary context-free rules are lexi-\\ncalized. Second, the top-down sampling restricts the\\ncombinatorics inherent to bottom-up search (Shieber\\net al., 1990). Third, we do not restrict the generation\\nby the order information in the input. The lack of\\norder information in the input often raises the high\\ncombinatorics in lexicalist approaches (Kay, 1996).\\nIn our case, however, we use sampling to reduce\\nthis problem, and it allows us to produce syntacti-\\ncally diverse questions. And fourth, we impose no\\nconstraints on the grammar thereby making it eas-\\nier to maintain bi-directional (recursive) grammars\\nthat can be used both for parsing and for generation\\n(Shieber, 1988).\\n2.2 Bi-Layered L-PCFGs\\nAs mentioned earlier, one of our lattice types is\\nbased on bi-layered PCFGs introduced here.\\nIn their traditional use, the latent states in L-\\nPCFGs aim to capture syntactic information. We in-\\ntroduce here the use of an L-PCFG with two layers\\nof latent states: one layer is intended to capture the\\nusual syntactic information, and the other aims to\\ncapture semantic and topical information by using a\", metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 2}), Document(page_content='SBARQ-33-403\\nSQ-8-925\\nNN-41-854\\nnochebuenaAUX-22-300\\nisWHNP-7-291\\nNN-45-142\\ndayWP-7-254\\nwhatSBARQ-30-403\\nSQ-8-709\\nNN-41-854\\nnochebuenaAUX-12-300\\nisWRB-42-707\\nwhenSBARQ-24-403\\nSQ-17-709\\nJJ-18-579\\ncelebratedSQ-15-931\\nNN-30-854\\nnochebuenaAUX-29-300\\nisWRB-42-707\\nwhen\\nFigure 2: Trees used for bi-layered L-PCFG training. The questions what day is nochebuena ,when is nochebuena\\nandwhen is nochebuena celebrated are paraphrases from the Paralex corpus. Each nonterminal is decorated with a\\nsyntactic label and two identiﬁers, e.g., for WP-7-254, WP is the syntactic label assigned by the BLLIP parser, 7 is the\\nsyntactic latent state, and 254 is the semantic latent state.\\nlarge set of states with speciﬁc feature functions.4\\nTo create the bi-layered L-PCFG, we again use\\nthe spectral algorithm of Narayan and Cohen (2015)\\nto estimate a grammar Gparfrom the Paralex corpus.\\nWe use the word alignment of paraphrase question\\npairs in Paralex to map inside and outside trees of\\neach nonterminals in the treebank to bag of word\\nfeatures. The number of latent states we use is 1,000.\\nOnce the two feature functions (syntactic in Gsyn\\nand semantic in Gpar) are created, each nontermi-\\nnal in the training treebank is assigned two latent\\nstates (cluster identiﬁers). Figure 2 shows an exam-\\nple annotation of trees for three paraphrase questions\\nfrom the Paralex corpus. We compute the parame-\\nters of the bi-layered L-PCFG Glayered with a simple\\nfrequency count maximum likelihood estimate over\\nthis annotated treebank. As such, Glayered is a com-\\nbination ofGsynandGpar, resulting in 24,000 latent\\nstates (24 syntactic x 1000 semantic).\\nConsider an example where we want to gen-\\nerate paraphrases for the question what day is\\nnochebuena . Parsing it with Glayered will lead to\\nthe leftmost hybrid structure as shown in Figure 2.\\nThe assignment of the ﬁrst latent states for each non-\\nterminals ensures that we retrieve the correct syn-\\ntactic representation of the sentence. Here, how-\\never, we are more interested in the second latent\\nstates assigned to each nonterminals which capture\\nthe paraphrase information of the sentence at vari-\\nous levels. For example, we have a unary lexical rule\\n(NN- *-142 day) indicating that we observe day\\nwithNNof the paraphrase type 142. We could use\\nthis information to extract unary rules of the form\\n(NN- *-142w)in the treebank that will generate\\n4For other cases of separating syntax from semantics in a\\nsimilar way, see Mitchell and Steedman (2015).wordswwhich are paraphrases to day. Similarly,\\nany node WHNP- *-291 in the treebank will gener-\\nate paraphrases for what day ,SBARQ- *-403 , for\\nwhat day is nochebuena . This way we will be able\\nto generate paraphrases when is nochebuena and\\nwhen is nochebuena celebrated as they both have\\nSBARQ- *-403 as their roots.5\\nTo generate a word lattice Wqfor a given question\\nq, we parseqwith the bi-layered grammar Glayered .\\nFor each rule of the form X-m1-m2→win the bi-\\nlayered tree with X∈P,m1∈{1,..., 24},m2∈\\n{1,..., 1000}andwa word inq, we extract rules\\nof the form X-∗-m2→w′fromGlayered such that\\nw′̸=w. For each such (w,w′), we add a path w′\\nparallel towin the word lattice.\\n2.3 Paraphrase Classiﬁcation\\nOur sampling algorithm overgenerates paraphrases\\nwhich are incorrect. To improve its precision, we\\nbuild a binary classiﬁer to ﬁlter the generated para-\\nphrases. We randomly select 100 distinct questions\\nfrom the Paralex corpus and generate paraphrases\\nusing our generation algorithm with various lattice\\nsettings. We randomly select 1,000 pairs of input-\\nsampled sentences and manually annotate them as\\n“correct” or “incorrect” paraphrases.6We train our\\nclassiﬁer on this manually created training data.7We\\n5We found out that our Gpargrammar is not ﬁne-grained\\nenough and often merges different paraphrase information into\\nthe same latent state. This problem is often severe for nontermi-\\nnals at the top level of the bilayered tree. Hence, we rely only\\non unary lexical rules (the rules that produce terminal nodes) to\\nextract paraphrase patterns in our experiments.\\n6We have 154 positive and 846 negative paraphrase pairs.\\n7We do not use the paraphrase pairs from the Paralex corpus\\nto train our classiﬁer, as they do not represent the distribution\\nof our sampled paraphrases and the classiﬁer trained on them\\nperforms poorly.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 3}), Document(page_content='follow Madnani et al. (2012), who used MT metrics\\nfor paraphrase identiﬁcation, and experiment with 8\\nMT metrics as features for our binary classiﬁer. In\\naddition, we experiment with a binary feature which\\nchecks if the sampled paraphrase preserves named\\nentities from the input sentence. We use WEKA\\n(Hall et al., 2009) to replicate the classiﬁer of Mad-\\nnani et al. (2012) with our new feature. We tune the\\nfeature set for our classiﬁer on the development data.\\n3 Semantic Parsing using Paraphrasing\\nIn this section we describe how the paraphrase al-\\ngorithm is used for converting natural language to\\nFreebase queries. Following Reddy et al. (2014), we\\nformalize the semantic parsing problem as a graph\\nmatching problem, i.e., ﬁnding the Freebase sub-\\ngraph (grounded graph) that is isomorphic to the in-\\nput question semantic structure (ungrounded graph).\\nThis formulation has a major limitation that can\\nbe alleviated by using our paraphrase generation al-\\ngorithm. Consider the question What language do\\npeople in Czech Republic speak? . The ungrounded\\ngraph corresponding to this question is shown in\\nFigure 3(a). The Freebase grounded graph which re-\\nsults in correct answer is shown in Figure 3(d). Note\\nthat these two graphs are non-isomorphic making it\\nimpossible to derive the correct grounding from the\\nungrounded graph. In fact, at least 15% of the ex-\\namples in our development set fail to satisfy isomor-\\nphic assumption. In order to address this problem,\\nwe use paraphrases of the input question to gener-\\nate additional ungrounded graphs, with the aim that\\none of those paraphrases will have a structure iso-\\nmorphic to the correct grounding. Figure 3(b) and\\nFigure 3(c) are two such paraphrases which can be\\nconverted to Figure 3(d) as described in ??.\\nFor a given input question, ﬁrst we build un-\\ngrounded graphs from its paraphrases. We con-\\nvert these graphs to Freebase graphs. To learn this\\nmapping, we rely on manually assembled question-\\nanswer pairs. For each training question, we ﬁrst\\nﬁnd the set of oracle grounded graphs—Freebase\\nsubgraphs which when executed yield the correct\\nanswer—derivable from the question’s ungrounded\\ngraphs. These oracle graphs are then used to train\\na structured perceptron model. These steps are dis-\\ncussed in detail below.3.1 Ungrounded Graphs from Paraphrases\\nWe use G RAPH PARSER (Reddy et al., 2014) to con-\\nvert paraphrases to ungrounded graphs. This conver-\\nsion involves three steps: 1) parsing the paraphrase\\nusing a CCG parser to extract syntactic derivations\\n(Lewis and Steedman, 2014), 2) extracting logi-\\ncal forms from the CCG derivations (Bos et al.,\\n2004), and 3) converting the logical forms to an un-\\ngrounded graph.8The ungrounded graph for the ex-\\nample question and its paraphrases are shown in Fig-\\nure 3(a), Figure 3(b) and Figure 3(c), respectively.\\n3.2 Grounded Graphs from Ungrounded\\nGraphs\\nThe ungrounded graphs are grounded to Freebase\\nsubgraphs by mapping entity nodes, entity-entity\\nedges and entity type nodes in the ungrounded\\ngraph to Freebase entities, relations and types,\\nrespectively. For example, the graph in Fig-\\nure 3(b) can be converted to a Freebase graph in\\nFigure 3(d) by replacing the entity node Czech\\nRepublic with the Freebase entity C ZECH RE-\\nPUBLIC , the edge (speak.arg 2, speak.in) between\\nxand Czech Republic with the Freebase re-\\nlation (location.country.ofﬁcial language.2, lo-\\ncation.country.ofﬁcial language.1) , the type\\nnode language with the Freebase type lan-\\nguage.human language , and the TARGET node\\nremains intact. The rest of the nodes, edges and\\ntypes are grounded to null. In a similar fashion,\\nFigure 3(c) can be grounded to Figure 3(d), but\\nnot Figure 3(a) to Figure 3(d). If no paraphrase is\\nisomorphic to the target grounded grounded graph,\\nour grounding fails.\\n3.3 Learning\\nWe use a linear model to map ungrounded graphs\\nto grounded ones. The parameters of the model\\nare learned from question-answer pairs. For ex-\\nample, the question What language do people in\\nCzech Republic speak? paired with its answer\\n{CZECH LANGUAGE}. In line with most work on\\nquestion answering against Freebase, we do not rely\\non annotated logical forms associated with the ques-\\ntion for training and treat the mapping of a question\\nto its grounded graph as latent.\\n8Please see Reddy et al. (2014) for more details.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 4}), Document(page_content='language target people\\nx e1 y e2Czech\\nRepublicspeak\\n.arg2speak\\n.arg1people\\n.in.arg1people\\n.in.arg2\\ntypetype(a) Input sentence: What language do people in Czech Republic speak?\\nlanguage target\\nx e1Czech\\nRepubliclanguage\\n.’s.arg1language\\n.’s.arg2\\ntype\\n(c) Paraphrase: What is Czech\\nRepublic’s language?\\nlanguage\\n.human languagetarget\\nx mCzech\\nRepubliclocation.country\\n.oﬃcial language.2location.country\\n.oﬃcial language.1\\ntype(d) Freebase grounded graph\\npeople y\\ntarget e1\\nx e1\\nlanguage e1\\nCzech\\nRepublicspeak.arg2speak.arg1speak.in speak.arg1\\nspeak.arg2\\nspeak.intypetype (b) Paraphrase: What language do\\npeople speak in Czech Republic?\\nFigure 3: Ungrounded graphs for an input question and its paraphrases along with its correct grounded graph. The\\ngreen squares indicate NL or Freebase entities, the yellow rectangles indicate unary NL predicates or Freebase types,\\nthe circles indicate NL or Freebase events, the edge labels indicate binary NL predicates or Freebase relations, and the\\nred diamonds attach to the entity of interest (the answer to the question).\\nLetqbe a question, let pbe a paraphrase, let ube\\nan ungrounded graph for p, and letgbe a grounded\\ngraph formed by grounding the nodes and edges of\\nuto the knowledge base K(throughout we use Free-\\nbase as the knowledge base). Following Reddy et al.\\n(2014), we use beam search to ﬁnd the highest scor-\\ning tuple of paraphrase, ungrounded and grounded\\ngraphs (ˆp,ˆu,ˆg)under the model θ∈Rn:\\n(ˆp,ˆu,ˆg) = arg max\\n(p,u,g )θ·Φ(p,u,g,q,K),\\nwhere Φ(p,u,g,q,K)∈Rndenotes the features for\\nthe tuple of paraphrase, ungrounded and grounded\\ngraphs. The feature function has access to the para-\\nphrase, ungrounded and grounded graphs, the origi-\\nnal question, as well as to the content of the knowl-\\nedge base and the denotation |g|K(the denotation of\\na grounded graph is deﬁned as the set of entities or\\nattributes reachable at its TARGET node). See ??for\\nthe features employed. The model parameters are\\nestimated with the averaged structured perceptron\\n(Collins, 2002). Given a training question-answer\\npair(q,A), the update is:\\nθt+1←θt+Φ(p+,u+,g+,q,K)−Φ(ˆp,ˆu,ˆg,q,K),\\nwhere (p+,u+,g+)denotes the tuple of gold para-\\nphrase, gold ungrounded and grounded graphs forq. Since we do not have direct access to the gold\\nparaphrase and graphs, we instead rely on the set of\\noracle tuples ,OK,A(q), as a proxy:\\n(p+,u+,g+) = arg max\\n(p,u,g )∈OK,A(q)θ·Φ(p,u,g,q,K),\\nwhereOK,A(q)is deﬁned as the set of tuples ( p,u,\\ng) derivable from the question q, whose denotation\\n|g|Khas minimal F1-loss against the gold answer A.\\nWe ﬁnd the oracle graphs for each question a priori\\nby performing beam-search with a very large beam.\\n4 Experimental Setup\\nBelow, we give details on the evaluation dataset and\\nbaselines used for comparison. We also describe the\\nmodel features and provide implementation details.\\n4.1 Evaluation Data and Metric\\nWe evaluate our approach on the WebQuestions\\ndataset (Berant et al., 2013). WebQuestions con-\\nsists of 5,810 question-answer pairs where ques-\\ntions represents real Google search queries. We use\\nthe standard train/test splits, with 3,778 train and\\n2,032 test questions. For our development experi-\\nments we tune the models on held-out data consist-\\ning of 30% training questions, while for ﬁnal testing', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 5}), Document(page_content='we use the complete training data. We use average\\nprecision (avg P.), average recall (avg R.) and aver-\\nage F 1(avg F 1) proposed by Berant et al. (2013) as\\nevaluation metrics.9\\n4.2 Baselines\\nORIGINAL We use G RAPH PARSER without para-\\nphrases as our baseline. This gives an idea about the\\nimpact of using paraphrases.\\nMT We compare our paraphrasing models with\\nmonolingual machine translation based model for\\nparaphrase generation (Quirk et al., 2004; Wubben\\net al., 2010). In particular, we use Moses (Koehn et\\nal., 2007) to train a monolingual phrase-based MT\\nsystem on the Paralex corpus. Finally, we use Moses\\ndecoder to generate 10-best distinct paraphrases for\\nthe test questions.\\n4.3 Implementation Details\\nEntity Resolution For WebQuestions, we use\\n8 handcrafted part-of-speech patterns (e.g., the pat-\\ntern(DT)?(JJ.?|NN.?){0,2}NN.? matches\\nthe noun phrase the big lebowski ) to identify candi-\\ndate named entity mention spans. We use the Stan-\\nford CoreNLP caseless tagger for part-of-speech\\ntagging (Manning et al., 2014). For each candidate\\nmention span, we retrieve the top 10 entities accord-\\ning to the Freebase API.10We then create a lattice in\\nwhich the nodes correspond to mention-entity pairs,\\nscored by their Freebase API scores, and the edges\\nencode the fact that no joint assignment of entities\\nto mentions can contain overlapping spans. We take\\nthe top 10 paths through the lattice as possible en-\\ntity disambiguations. For each possibility, we gener-\\naten-best paraphrases that contains the entity men-\\ntion spans. In the end, this process creates a total of\\n10nparaphrases. We generate ungrounded graphs\\nfor these paraphrases and treat the ﬁnal entity dis-\\nambiguation and paraphrase selection as part of the\\nsemantic parsing problem.11\\nGRAPH PARSER Features. We use the features\\nfrom Reddy et al. (2014). These include edge align-\\n9https://github.com/percyliang/sempre/\\nblob/master/scripts/evaluation.py\\n10http://developers.google.com/freebase/\\n11To generate ungrounded graphs for a paraphrase, we treat\\neach entity mention as a single word.ments and stem overlaps between ungrounded and\\ngrounded graphs, and contextual features such as\\nword and grounded relation pairs. In addition to\\nthese features, we add two new real-valued features\\n– the paraphrase classiﬁer’s score and the entity dis-\\nambiguation lattice score.\\nBeam Search We use beam search to infer the\\nhighest scoring graph pair for a question. The search\\noperates over entity-entity edges and entity type\\nnodes of each ungrounded graph. For an entity-\\nentity edge, there are two operations: ground the\\nedge to a Freebase relation, or skip the edge. Sim-\\nilarly, for an entity type node, there are two opera-\\ntions: ground the node to a Freebase type, or skip\\nthe node. We use a beam size of 100 in all our ex-\\nperiments.\\n5 Results and Discussion\\nIn this section, we present results from ﬁve dif-\\nferent systems for our question-answering experi-\\nments: ORIGINAL ,MT,NAIVE ,PPDB and BILAY -\\nERED . First two are baseline systems. Other three\\nsystems use paraphrases generated from an L-PCFG\\ngrammar. NAIVE uses a word lattice with a sin-\\ngle start-to-end path representing the input question\\nitself, PPDB uses a word lattice constructed using\\nthe PPDB rules, and BILAYERED uses bi-layered L-\\nPCFG to build word lattices. Note that NAIVE does\\nnot require any parallel resource to train, PPDB re-\\nquires an external paraphrase database, and BILAY -\\nERED , like MT, needs a parallel corpus with para-\\nphrase pairs. We tune our classiﬁer features and\\nGRAPH PARSER features on the development data.\\nWe use the best setting from tuning for evaluation\\non the test data.\\nResults on the Development Set Table 1 shows\\nthe results with our best settings on the develop-\\nment data. We found that oracle scores improve sig-\\nniﬁcantly with paraphrases. ORIGINAL achieves an\\noracle score of 65.1whereas with paraphrases we\\nachieve an F 1greater than 70across all the mod-\\nels. This shows that with paraphrases we elimi-\\nnate substantial mismatch between Freebase and un-\\ngrounded graphs. This trend continues for the ﬁnal\\nprediction with the paraphrasing models performing\\nbetter than the ORIGINAL .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 6}), Document(page_content='All our proposed paraphrasing models beat the\\nMTbaseline. Even the NAIVE model which does not\\nuse any parallel or external resource surpass the MT\\nbaseline in the ﬁnal prediction. Upon error analy-\\nsis, we found that the MTmodel produce too simi-\\nlar paraphrases, mostly with only inﬂectional varia-\\ntions. For the question What language do people in\\nCzech Republic speak , the top ten paraphrases pro-\\nduced by MTare mostly formed by replacing words\\nlanguage with languages , do with does,people with\\nperson andspeak with speaks . These paraphrases\\ndo not address the structural mismatch problem. In\\ncontrast, our grammar based models generate syn-\\ntactically diverse paraphrases.\\nOur PPDB model performs best across the para-\\nphrase models (avg F 1=47.9). We attribute its suc-\\ncess to the high quality paraphrase rules from the\\nexternal paraphrase database. For the BILAYERD\\nmodel we found 1,000 latent semantic states is not\\nsufﬁcient for modeling topical differences. Though\\nMTcompetes with NAIVE and BILAYERED , the per-\\nformance of NAIVE is highly encouraging since it\\ndoes not require any parallel corpus. Furthermore,\\nwe observe that the MTmodel has larger search\\nspace. The number of oracle graphs – the number\\nof ways in which one can produce the correct Free-\\nbase grounding from the ungrounded graphs of the\\ngiven question and its paraphrases – is higher for\\nMT(77.2) than the grammar-based models (50–60).\\nResults on the Test Set Table 2 shows our ﬁnal\\nresults on the test data. We get similar results on the\\ntest data as we reported on the development data.\\nAgain, the PPDB model performs best with an F 1\\nscore of 47.7. The baselines, ORIGINAL and MT,\\nlag with scores of 45.0and47.1, respectively. We\\nalso present the results of existing literature on this\\ndataset. Among these, Berant and Liang (2014) also\\nuses paraphrasing but unlike ours it is based on a\\ntemplate grammar (containing 8 grammar rules) and\\nrequires logical forms beforehand to generate para-\\nphrases. Our PPDB outperforms Berant and Liang’s\\nmodel by 7.8 F 1points. Yih et al. (2015) and Xu et\\nal. (2016) use neural network models for semantic\\nparsing, in addition to using sophisticated entity res-\\nolution (Yang and Chang, 2015) and a very large un-\\nsupervised corpus as additional training data. Note\\nthat we use G RAPH PARSER as our semantic parsingMethod avg oracle\\nF1# oracle\\ngraphsavg F 1\\nORIGINAL 65.1 11.0 44.7\\nMT 71.5 77.2 47.0\\nNAIVE 71.2 53.6 47.5\\nPPDB 71.8 59.8 47.9\\nBILAYERED 71.6 55.0 47.1\\nTable 1: Oracle statistics and results on the WebQues-\\ntions development set.\\nMethod avg P. avg R. avg F 1\\nBerant and Liang ’14 40.5 46.6 39.9\\nBast and Haussmann ’15 49.8 60.4 49.4\\nBerant and Liang ’15 50.4 55.7 49.7\\nReddy et al. ’16 49.0 61.1 50.3\\nYih et al. ’15 52.8 60.7 52.5\\nXu et al. ’16 53.1 65.5 53.3\\nThis paper\\nORIGINAL 53.2 54.2 45.0\\nMT 48.0 56.9 47.1\\nNAIVE 48.1 57.7 47.2\\nPPDB 48.4 58.1 47.7\\nBILAYERED 47.0 57.6 47.2\\nTable 2: Results on WebQuestions test dataset.\\nframework for evaluating our paraphrases extrinsi-\\ncally. We leave plugging our paraphrases to other\\nexisting methods and other tasks for future work.\\nError Analysis The upper bound of our para-\\nphrasing methods is in the range of 71.2–71.8. We\\nexamine the reason where we lose the rest. For the\\nPPDB model, the majority (78.4%) of the errors are\\npartially correct answers occurring due to incom-\\nplete gold answer annotations or partially correct\\ngroundings. Note that the partially correct ground-\\nings may include incorrect paraphrases. 13.5% are\\ndue to mismatch between Freebase and the para-\\nphrases produced, and the rest (8.1%) are due to\\nwrong entity annotations.\\n6 Conclusion\\nWe described a grammar method to generate para-\\nphrases for questions, and applied it to a question\\nanswering system based on semantic parsing. We\\nshowed that using paraphrases for a question an-\\nswering system is a useful way to improve its per-\\nformance. Our method is rather generic and can be\\napplied to any question answering system.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 7}), Document(page_content='Acknowledgements\\nThe authors would like to thank Nitin Madnani for\\nhis help with the implementation of the paraphrase\\nclassiﬁer. We would like to thank our anonymous re-\\nviewers for their insightful comments. This research\\nwas supported by an EPSRC grant (EP/L02411X/1),\\nthe H2020 project SUMMA (under grant agreement\\n688139), and a Google PhD Fellowship for the sec-\\nond author.\\nReferences\\n[Barzilay and Lee2003] Regina Barzilay and Lillian Lee.\\n2003. Learning to paraphrase: An unsupervised ap-\\nproach using multiple-sequence alignment. In Pro-\\nceedings of NAACL-HLT .\\n[Bast and Haussmann2015] Hannah Bast and Elmar\\nHaussmann. 2015. More accurate question answering\\non Freebase. In Proceedings of CIKM .\\n[Berant and Liang2014] Jonathan Berant and Percy\\nLiang. 2014. Semantic parsing via paraphrasing. In\\nProceedings of ACL .\\n[Berant and Liang2015] Jonathan Berant and Percy\\nLiang. 2015. Imitation learning of agenda-based\\nsemantic parsers. Transactions of the Association for\\nComputational Linguistics , 3:545–558.\\n[Berant et al.2013] Jonathan Berant, Andrew Chou, Roy\\nFrostig, and Percy Liang. 2013. Semantic parsing on\\nFreebase from question-answer pairs. In Proceedings\\nof EMNLP .\\n[Bos et al.2004] Johan Bos, Stephen Clark, Mark Steed-\\nman, James R. Curran, and Julia Hockenmaier. 2004.\\nWide-coverage semantic representations from a CCG\\nparser. In Proceedings of COLING .\\n[Charniak and Johnson2005] Eugene Charniak and Mark\\nJohnson. 2005. Coarse-to-ﬁne n-best parsing and\\nmaxent discriminative reranking. In Proceedings of\\nACL.\\n[Chen and Mooney2011] David L. Chen and Raymond J.\\nMooney. 2011. Learning to interpret natural lan-\\nguage navigation instructions from observations. In\\nProceedings of AAAI .\\n[Cohen et al.2013] Shay B. Cohen, Karl Stratos, Michael\\nCollins, Dean P. Foster, and Lyle Ungar. 2013.\\nExperiments with spectral learning of latent-variable\\nPCFGs. In Proceedings of NAACL .\\n[Collins2002] Michael Collins. 2002. Discriminative\\ntraining methods for hidden markov models: Theory\\nand experiments with perceptron algorithms. In Pro-\\nceedings of EMNLP .[Duboue and Chu-Carroll2006] Pablo Ariel Duboue and\\nJennifer Chu-Carroll. 2006. Answering the question\\nyou wish they had asked: The impact of paraphrasing\\nfor question answering. In Proceedings of NAACL-\\nHLT.\\n[Fader et al.2013] Anthony Fader, Luke Zettlemoyer, and\\nOren Etzioni. 2013. Paraphrase-driven learning for\\nopen question answering. In Proceedings of ACL .\\n[Ganitkevitch et al.2013] Juri Ganitkevitch, Benjamin\\nVan Durme, and Chris Callison-Burch. 2013.\\nPPDB: The Paraphrase Database. In Proceedings of\\nNAACL-HLT .\\n[Hall et al.2009] Mark Hall, Eibe Frank, Geoffrey\\nHolmes, Bernhard Pfahringer, Peter Reutemann,\\nand Ian H Witten. 2009. The WEKA data mining\\nsoftware: An update. ACM SIGKDD Explorations\\nNewsletter , 11(1):10–18.\\n[Kay1996] Martin Kay. 1996. Chart generation. In Pro-\\nceedings of ACL .\\n[Koehn et al.2007] Philipp Koehn, Hieu Hoang, Alexan-\\ndra Birch, Chris Callison-Burch, Marcello Federico,\\nNicola Bertoldi, Brooke Cowan, Wade Shen, Chris-\\ntine Moran, Richard Zens, Chris Dyer, Ond ˇrej Bo-\\njar, Alexandra Constantin, and Evan Herbst. 2007.\\nMoses: Open source toolkit for statistical machine\\ntranslation. In Proceedings of ACL .\\n[Koller and Striegnitz2002] Alexander Koller and\\nKristina Striegnitz. 2002. Generation as dependency\\nparsing. In Proceedings of ACL .\\n[Krishnamurthy and Mitchell2012] Jayant Krishna-\\nmurthy and Tom Mitchell. 2012. Weakly supervised\\ntraining of semantic parsers. In Proceedings of\\nEMNLP .\\n[Kwiatkowski et al.2013] Tom Kwiatkowski, Eunsol\\nChoi, Yoav Artzi, and Luke Zettlemoyer. 2013.\\nScaling semantic parsers with on-the-ﬂy ontology\\nmatching. In Proceedings of EMNLP .\\n[Langkilde and Knight1998] Irene Langkilde and Kevin\\nKnight. 1998. Generation that exploits corpus-\\nbased statistical knowledge. In Proceedings of ACL-\\nCOLING .\\n[Lewis and Steedman2014] Mike Lewis and Mark Steed-\\nman. 2014. A* CCG parsing with a supertag-factored\\nmodel. In Proceedings of EMNLP .\\n[Madnani et al.2012] Nitin Madnani, Joel Tetreault, and\\nMartin Chodorow. 2012. Re-examining machine\\ntranslation metrics for paraphrase identiﬁcation. In\\nProceedings of NAACL-HLT .\\n[Manning et al.2014] Christopher D. Manning, Mihai\\nSurdeanu, John Bauer, Jenny Finkel, Steven J.\\nBethard, and David McClosky. 2014. The Stanford\\nCoreNLP natural language processing toolkit. In Pro-\\nceedings of ACL .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 8}), Document(page_content='[Matsuzaki et al.2005] Takuya Matsuzaki, Yusuke Miyao,\\nand Jun’ichi Tsujii. 2005. Probabilistic CFG with la-\\ntent annotations. In Proceedings of ACL .\\n[Matuszek et al.2012] Cynthia Matuszek, Nicholas\\nFitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter\\nFox. 2012. A joint model of language and perception\\nfor grounded attribute learning. In Proceedings of\\nICML .\\n[Mitchell and Steedman2015] Jeff Mitchell and Mark\\nSteedman. 2015. Orthogonality of syntax and se-\\nmantics within distributional spaces. In Proceedings\\nof ACL .\\n[Narayan and Cohen2015] Shashi Narayan and Shay B.\\nCohen. 2015. Diversity in spectral learning for nat-\\nural language parsing. In Proceedings of EMNLP .\\n[Narayan and Cohen2016] Shashi Narayan and Shay B.\\nCohen. 2016. Optimizing spectral learning for pars-\\ning. In Proceedings of ACL .\\n[Narayan and Gardent2012] Shashi Narayan and Claire\\nGardent. 2012. Structure-driven lexicalist generation.\\nInProceedings of COLING .\\n[Pang et al.2003] Bo Pang, Kevin Knight, and Daniel\\nMarcu. 2003. Syntax-based alignment of multiple\\ntranslations: Extracting paraphrases and generating\\nnew sentences. In Proceedings of NAACL-HLT .\\n[Petrov et al.2006] Slav Petrov, Leon Barrett, Romain\\nThibaux, and Dan Klein. 2006. Learning accurate,\\ncompact, and interpretable tree annotation. In Pro-\\nceedings of COLING-ACL .\\n[Prescher2005] Detlef Prescher. 2005. Head-driven pcfgs\\nwith latent-head statistics. In Proceedings of IWPT .\\n[Quirk et al.2004] Chris Quirk, Chris Brockett, and\\nWilliam B. Dolan. 2004. Monolingual machine trans-\\nlation for paraphrase generation. In Proceedings of\\nEMNLP .\\n[Reddy et al.2014] Siva Reddy, Mirella Lapata, and Mark\\nSteedman. 2014. Large-scale semantic parsing with-\\nout question-answer pairs. Transactions of the Associ-\\nation for Computational Linguistics , 2:377–392.\\n[Reddy et al.2016] Siva Reddy, Oscar T ¨ackstr ¨om,\\nMichael Collins, Tom Kwiatkowski, Dipanjan Das,\\nMark Steedman, and Mirella Lapata. 2016. Trans-\\nforming Dependency Structures to Logical Forms for\\nSemantic Parsing. Transactions of the Association for\\nComputational Linguistics , 4:127–140.\\n[Riezler et al.2007] Stefan Riezler, Alexander Vasserman,\\nIoannis Tsochantaridis, Vibhu Mittal, and Yi Liu.\\n2007. Statistical machine translation for query expan-\\nsion in answer retrieval. In Proceedings of ACL .\\n[Shieber et al.1990] Stuart M. Shieber, Gertjan van No-\\nord, Fernando C. N. Pereira, and Robert C. Moore.\\n1990. Semantic head-driven generation. Computa-\\ntional Linguistics , 16(1):30–42.[Shieber1988] Stuart M. Shieber. 1988. A uniform archi-\\ntecture for parsing and generation. In Proceedings of\\nCOLING .\\n[Wang et al.2015] Yushi Wang, Jonathan Berant, and\\nPercy Liang. 2015. Building a semantic parser\\novernight. In Proceedings of ACL .\\n[White2004] Michael White. 2004. Reining in ccg chart\\nrealization. In Anja Belz, Roger Evans, and Paul Pi-\\nwek, editors, Natural Language Generation , volume\\n3123 of Lecture Notes in Computer Science , pages\\n182–191. Springer Berlin Heidelberg.\\n[Wong and Mooney2006] Yuk Wah Wong and Ray-\\nmond J. Mooney. 2006. Learning for semantic pars-\\ning with statistical machine translation. In Proceed-\\nings of NAACL .\\n[Wubben et al.2010] Sander Wubben, Antal van den\\nBosch, and Emiel Krahmer. 2010. Paraphrase gener-\\nation as monolingual translation: Data and evaluation.\\nInProceedings of INLG .\\n[Xu et al.2016] Kun Xu, Siva Reddy, Yansong Feng,\\nSongfang Huang, and Dongyan Zhao. 2016. Ques-\\ntion Answering on Freebase via Relation Extraction\\nand Textual Evidence. In Proceedings of ACL .\\n[Yang and Chang2015] Yi Yang and Ming-Wei Chang.\\n2015. S-MART: Novel tree-based structured learning\\nalgorithms applied to tweet entity linking. In Proceed-\\nings of ACL .\\n[Yih et al.2015] Wen-tau Yih, Ming-Wei Chang, Xi-\\naodong He, and Jianfeng Gao. 2015. Semantic pars-\\ning via staged query graph generation: Question an-\\nswering with knowledge base. In Proceedings of ACL .\\n[Zettlemoyer and Collins2005] Luke S. Zettlemoyer and\\nMichael Collins. 2005. Learning to map sentences\\nto logical form: Structured classiﬁcation with proba-\\nbilistic categorial grammars. In Proceedings of UAI .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.06068.pdf', 'page': 9}), Document(page_content='Detecting and Extracting Events from Text\\nDocuments\\nJUGAL KALITA, University of Colorado, Colorado Springs\\nEvents of various kinds are mentioned and discussed in text documents, whether they are books,\\nnews articles, blogs or microblog feeds. The paper starts by giving an overview of how events\\nare treated in linguistics and philosophy. We follow this discussion by surveying how events and\\nassociated information are handled in computationally. In particular, we look at how textual\\ndocuments can be mined to extract events and ancillary information. These days, it is mostly\\nthrough the application of various machine learning techniques. We also discuss applications of\\nevent detection and extraction systems, particularly in summarization, in the medical domain\\nand in the context of Twitter posts. We end the paper with a discussion of challenges and future\\ndirections.\\nCategories and Subject Descriptors: ... [ ...]: ...\\nGeneral Terms: ...\\nAdditional Key Words and Phrases: Information Retrieval, Event Detection\\n1. INTRODUCTION\\nAmong the several senses that The Oxford English Dictionary1, the most venerable\\ndictionary of English, provides for the word event , are the following.\\n1athe (actual or contemplated) fact of anything happening; the occurrence of.\\n2aanything that happens, or is contemplated as happening; an incident, occurrence .\\n2d(In modern use, chieﬂy restricted to) occurrences of some importance .\\nAlthough an event may refer to anything that happens, we are usually interested\\nin occurrences that are of some importance. We want to extract such events from\\ntextual documents. In order to extract important events or events of a speciﬁc\\ntype, it is likely that we have to identify all events in a document to start with.\\nConsider the ﬁrst paragraphs of the article on the Battle of Fredericksburg in the\\nEnglish Wikipedia, accessed on May 5, 2012. We have highlighted the “events” in\\nthe paragraph.\\nThe Battle of Fredericksburg was fought December 11–15, 1862, in and\\naround Fredericksburg, Virginia, between General Robert E. Lee’s Con-\\n1http://www/oed.com\\nJugal Kalita is a professor in the Department of Computer Science, University of Colorado, Col-\\norado Springs, CO 80918 USA .\\nPermission to make digital/hard copy of all or part of this material without fee for personal\\nor classroom use provided that the copies are not made or distributed for proﬁt or commercial\\nadvantage, the ACM copyright/server notice, the title of the publication, and its date appear, and\\nnotice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish,\\nto post on servers, or to redistribute to lists requires prior speciﬁc permission and/or a fee.\\nc⃝2016 ACM 1529-3785/2016/0700-0001 $5.00\\nACM Transactions, Vol. V, No. N, January 2016, Pages 1–62.arXiv:1601.04012v1  [cs.CL]  15 Jan 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 0}), Document(page_content='2· ...\\nfederate Army of Northern Virginia and the Union Army of the Po-\\ntomac, commanded by Maj. Gen. Ambrose E. Burnside. The Union\\narmy’s futile frontal assaults on December 13 against entrenched Con-\\nfederate defenders on the heights behind the city is remembered as one\\nof the most one-sided battles of the American Civil War, with Union\\ncasualties more than twice as heavy as those suﬀered by the Confed-\\nerates.\\nThe paragraph contains two fairly long sentences with several “events”, men-\\ntioned using the following words: fought ,commanded ,assaults ,entrenched ,remem-\\nbered ,casualties andsuﬀered . Some of these “events” are described in terms of verbs\\nwhereas the others are in terms of nouns. Here fought ,commanded ,assaults ,bat-\\ntlesdeﬁnitely seem to be “events” that have durations or are durative .Entrenched\\nseems to talk about a state, whereas it is possible that suﬀered talks about some-\\nthing punctual (i.e., takes a moment or point of time) or can be durative (i.e., takes\\na longer period of time) as well. The act of remembering by an individual is usually\\nconsidered to happen momentarily, i.e., forgotten things come back to mind at an\\ninstant of time. But, in this paragraph it is given in passive voice and hence, it is\\nunclear who the actor is, possibly a lot diﬀerent people at diﬀerent points of time.\\nThus, depending on who is asked, the “events” picked out may be slightly dif-\\nferent, but the essence is that there are several events mentioned in the paragraph\\nand the objective in event extraction is to extract as many of them as possible in\\nan automated fashion. For example, someone may not pick out remembered as an\\nevent that took place. Some others may not want to say that entrenched is an event.\\nIn addition, if one is asked to pick an important event, responses may vary from\\nperson to person. Finally, if one is asked to summarize the paragraph, depending\\non the person asked the summary may vary. A summary prepared by the author\\nof this article is given below.\\nThe Battle of Fredericksburg, fought December 11-12, 1862, was one of\\nthe most one-sided battles of the American Civil War, with heavy Union\\ncasualties.\\nObviously, there are many other possibilities for summarization. However, the idea\\nis that identiﬁcation of events and their participants may play a signiﬁcant role in\\nsummarizing a document.\\nThis paper discusses the extraction of events and their attributes from unstruc-\\ntured English text. It is an survey of research in extracting event descriptions from\\ntextual documents. In addition, we discuss how the idea of event extraction can\\nbe used in application domains such as summarization of a document. We also\\ndiscuss application of event extraction in the biomedical domain and in the context\\nof Twitter messages.\\nThe rest of the paper is organized in the following manner. Section 2 provides a\\ndescription of research in linguistics and philosophy. The author believes that such\\na background, at least at a basic level, is necessary to understand and develop the\\napproaches and algorithms for automatic computational detection and extraction\\nof events and their participants from textual documents. Section 4 discusses ap-\\nproaches used in extracting events from textual documents. Most approaches these\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 1}), Document(page_content='...· 3\\ndays use machine learning techniques.\\n2. EVENTS IN LINGUISTICS AND PHILOSOPHY\\nReal world events are things that take place or happen. In this section, we present\\nan overview of how real events are represented in terms of language. In particular,\\nwe discuss classiﬁcation of events and features necessary for such classiﬁcation.\\nWe follow this by presenting the preferred way among philosophers to represent\\nevents in terms of logic. We bring this section to an end by presenting some of the\\nstructures ascribed to events by linguists or philosophers working at an abstract\\nlevel.\\nThe reason for the inclusion of this section in the paper is to set the context\\nfor the discussions in the following sections on the practical task of extracting\\nevents. Practical systems do not usually follow linguistic niceties although they\\ndraw inspiration from linguistics or philosophy.\\n2.1 Classifying Events\\nThere have been many attempts at classifying linguistic events. Below, we brieﬂy\\ndiscuss a few. The primary focus when linguists discuss events is on the verb present\\nin a sentence. Nouns, adjectives and other elements present in a sentence provide\\narguments for the verb.\\nAristotle (as presented in [Barnes et al. 1984]) classiﬁed verbs that denote some-\\nthing happening into three classes: actuality ,movement and action . An actuality\\nrepresents the existence of a thing or things; this is called state by others (e.g.,\\n[Rosen 1999]). An examples of actuality can be seen in the sentence Jon is ill . A\\nmovement is an incomplete process or something that takes time but doesn’t have\\nan inherent end. An example of movement is seen in the sentence Jon is running .\\nAn action is something that takes time and has an inherent end. An example of an\\naction is present in the sentence Jon is building a house . In other words, Aristotle\\ndistinguished between states and events and then events.\\n[Kenny 2003] lists verbs that belong to the three Aristotelian classes and develops\\nmembership criteria for the classes. Kenny renamed the classes as states ,activi-\\nties(actions without inherent end) and performances (actions with inherent ends).\\nKenny’s membership criteria are based on semantic entailments about whether the\\nevent can be considered to have taken place when it is still in progress. For example,\\nduring any point when we say Jon is running , we can consider that the activity of\\nrunning has taken place. In other words Jon is running entails Jon has run . Thus,\\nrunis an activity . In contrast, when we say Joh is taking the ﬁnal , we cannot say\\nthat Jon has taken the ﬁnal. In other words, the ﬁrst does not entail the second.\\nThus, the main diﬀerence between an activity and a performance is what is called\\ndelimitation . A delimited event has a natural end.\\n[Vendler 1967] developed a 4-way classiﬁcation scheme for linguistic events and\\n[Dowty 1979] developed a set of criteria for membership in the classes. The classes\\nenumerated by Dowty are: states ,activities ,achievements and accomplishments .\\nThe deﬁnitions are given below.\\n—Activities : Events that take place over a certain period of time, but do not\\nnecessarily have a ﬁxed termination point. Examples; Jon walked for an hour ,\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 2}), Document(page_content='4· ...\\nLinguist Nomenclature used\\nAristotle Actuality, Movement, Action\\nKenny State, Activity, Performance\\nDowty State, Activity, Accomplishment, Achievement\\nSmith State, Semelfactive, Activity, Accomplishment, Achievement\\nTable I. Nomenclatures used by linguists to classify events\\nand Jon is driving the car .\\n—Accomplishments : Events that happen over a certain period of time and then\\nend. Examples: Jon built a house in a month , and Jon is taking the ﬁnal .\\n—Achievements : These are events that occur instantaneously and lack continuous\\ntenses. Examples: Jon ﬁnished the ﬁnal in 45 minutes and The vase broke .\\n—States : These are non-actions that hold for a certain period of time, but lack\\ncontinuous tenses. Examples: Jon knows the answer and Jon likes Mary .\\n[Smith 1997] adopts the same classiﬁcation as Vendler and Dowty, but divides\\nachievements into two classes. The ﬁrst one is still called achievements , but the\\nsecond one is called semelfactives . In this new scheme, achievements are instanta-\\nneous (that is, the beginning of the event is the same as its end) culminating events,\\nbut semelfactives are events with no duration that result in no change of state. An\\nexample of a semelfactive is: Jon knocked on the door .\\nTable I presents the nomenclatures introduced by various linguists in one place.\\nThere are many variations of the schemes given here, although we do not discuss\\nthem in this paper.\\nIn the early work on event classiﬁcation, Aristotle, Vendler and others assume\\nthat what needs to be classiﬁed is the verb. However, many have concluded that\\nit is impossible to classify a verb into a speciﬁc class. It is more appropriate to\\nsay that a clause containing an event has a class, and the classiﬁcation of such a\\nclause depends not only upon the verb, but also on other material present in the\\nclause [Rosen 1996; Dowty 1979; 1991; Ritter and Rosen 1996]. In other words,\\nthe classiﬁcation must be compositional or must depend on various features of the\\nclause, not exclusively verb-based. There is also substantial evidence that sentence\\nmaterial other than the verb can change the overall event type. For example,\\naddition of a direct object can change an activity to an accomplishment [Rosen\\n1999], as in the following examples.\\n—Bill ran for ﬁve minutes/*in ﬁve minutes : activity\\n—Bill ran the mile *for 5 minutes/in 5 minutes : accomplishment\\n2.2 Parameters of Event Classes\\nMany authors in linguistics have delved deeper into the nature of event classes and\\nhave tried to come up with features or characteristics that can be used to identify\\nwhether something (verb or a clause) belongs to a particular event class or not.\\nThese features or characteristics are necessary to describe the structure of events\\nin a theoretical sense. Description of event structure usually refers to the actual\\nwords used (lexical features or characteristics) and also the structure of clause or\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 3}), Document(page_content='...· 5\\nsentence (syntactic features or characteristics). Identiﬁcation of such features may\\nbe described as ﬁnding parameters of event types or parameterization of event types .\\nA lot of the work on parameterization of event types/classes use the classes\\nespoused by Vendler. These include [Verkuyl 1996; Carlson 1981; Moens 1987;\\nHoeksema 1983; Mourelatos 1978; Ter Meulen 1983; 1997] and others. We will only\\nbrieﬂy touch upon such work in this paper. Our objective is to impress upon the\\nreader that identiﬁcation of features of event classes is considered an important\\ntask by linguists.\\nFor example, [Verkuyl 1996] describes Vendler’s classes with two binary features\\nor parameters: continuousness : whether an event has duration, and boundedness :\\nwhether an event has a (natural) terminal point or endpoint. Using these two\\nfeatures, the four Vendler classes can be parameterized as follows.\\nstate : -bounded, -continuous\\nactivity : -bounded, +continuous\\nachievement : +bounded, -continuous\\naccomplishment : +bounded, +continuous\\n[Hoeksema 1983; Mourelatos 1978] introduce the notion of countability while\\ndiscussing event classes. This is similar to the mass-count opposition in nouns.\\nTerminating events can be counted, but non-terminating processes cannot. Hoek-\\nsema introduces two binary features: count andduration to obtain Vendler’s classes\\nas seen below. The feature duration refers to whether the event takes place over\\ntime.\\nstate : -count, -duration\\nactivity : -count, +duration\\nachievement : +count, -duration\\naccomplishment : +count, +duration\\n[Moens 1987] reﬁnes Vendler’s classes by adding a class much like Smith’s semelfac-\\ntives [Smith 1997]. He suggests that, in addition to states, there are four event types:\\nculmination ,culminated process ,point , and process . He uses two binary features\\nor parameters: consequence identifying termination or culmination, and atomic or\\nnon-atomic (which Moens called extended ). Atomic is also called momentous or\\npointed . Moen’s classiﬁcation is given below, along with the features and examples.\\nculmination : +consequence, +atomic (examples: recognize ,win the race )\\nculminated process : +consequence, -atomic (examples: build a house )\\npoint : -consequence, +atomic (example: hiccup ,tap,wink)\\nprocess : -consequence, -atomic (example: run,swim ,play the piano )\\nstate : (examples: understand ,love,resemble )\\nMoens also claims that culminated process is an event class whose members are\\nmade up of smaller atomic units. In particular, a culminated process is a process\\nwith a consequent state. This insight that events can be decomposed into sub-events\\nwas used later by others working on the lexical analysis of events e.g., [Pustejovsky\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 4}), Document(page_content='6· ...\\nLinguist Event features identiﬁed\\nVerkuyl bounded, continuous\\nHoeksema count, duration\\nMoens consequence, atomic\\nTable II. Features used by linguists to classify events\\n1991b; 1991a]. Others such as [van Voorst 1988; Grimshaw 1990; Tenny 1994] have\\nclaimed that arguments of verbs are related to sub-events.\\nWe summarize the various features that linguists have used to classify events in\\nTable II. Of course, we do not discuss many other proposals for features in this brief\\ndiscussion. Classiﬁcation of events and their parameterization of verbs or predicates\\n(or clauses) are only the ﬁrst steps in developing a deeper linguistic understanding\\nof events. In particular, in order to understand the linguistic representation of\\nevents, linguists need to go beyond classiﬁcation schemes.\\n2.3 Events in Logical Representation of Semantics\\nMathematical logic is used to represent the semantics of language. In particular,\\nwe use logic to represent the meaning of single sentences. Early work on events,\\ne.g., Panini (as discussed by [Parsons 1990] and [Hamilton et al. 1961]) stated\\nthat language encodes two kinds of information– actions and non-actions . Verbs\\nrepresent actions and nouns represent non-actions or things.\\n[Davidson 2001] proposes that one needs an event variable eto represent events in\\nmathematical logic. This variable eis used to represent relations represented by the\\nevent denoted by the verb and other constituents in the sentence, such as modiﬁers.\\nDavidson claims that logically speaking, events are like things in that they can be\\nrepresented by a variable and this variable can be modiﬁed and quantiﬁed. A\\nquestion that arises is: how many arguments should an event predicate (in logic)\\ntake [Kenny 2003]? Just like nominal modiﬁers modify nouns, event modiﬁers can\\nmodify event predicates. An event predicate can take any number of modiﬁers\\njust like noun (nominal) modiﬁers. Examples of event modiﬁers are: time, place,\\nmanner and instrument. Davidson proposed that an event predicate may take one\\nor more required arguments (is this true?) and any number of adjuncts or optional\\nmodiﬁers. Consider the following examples from [Davidson 2001]. The English\\nsentence and the corresponding logical representation or logical form is given for\\neach example.\\na.John buttered the toast.\\n∃e buttered (Jones, the toast, e )\\nb.John buttered the toast slowly.\\n∃e buttered (Jones, the toast, e )∧slowly (e)\\nc.John buttered the toast slowly, in the bathroom.\\n∃e buttered (Jones, the toast, e )∧slowly (e)∧inthebathroom (e)\\nd.John buttered the toast slowly, in the bathroom, with a knife.\\n∃e buttered (Jones, the toast, e )∧slowly (e)∧inthebathroom (e)∧with aknife (e)\\ne.John buttered the toast slowly, in the bathroom, with a knife, at midnight.\\n∃e buttered (Jones, the toast, e )∧slowly (e)∧inthebathroom (e)∧with aknife (e)∧\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 5}), Document(page_content='...· 7\\natmidnight (e)\\nThus we can see that Davidson’s approach places the event variable ein the main\\npredicate of a clause and distributes it among the modiﬁers of the clause in logical\\nrepresentation. In writing the meaning in Davidsonian logic, the author creates\\npredicates such as thetoast andinthebathroom , just for illustration, without\\ngoing into details.\\nDavidsonian representation allows events to be represented in logic (logical se-\\nmantics) without requiring verbs to have multiple arities, i.e., without taking diﬀer-\\nent arguments in diﬀerent situations. Because the event is represented as a variable,\\nthe event variable ecan be included in the representation of logical meaning of each\\nmodiﬁer or adjunct. Another beneﬁt is that using Davidson’s representation, one\\ncan analyze events represented syntactically as nouns (nominals) or verbs [Parsons\\n1990]. For example, one can refer to an event using the verb to burn or the noun a\\nburn. Parsons also observes that using a variable to represent an event allows quan-\\ntiﬁcation over events the same way quantiﬁcation applies to things. The following\\nexamples are from [Parsons 1990].\\na.In every burning, oxygen is consumed .\\n∀e burning (e)→∃e′(consuming (e′)∧object (e, oxygen )∧in(e, e′)\\nb.Agatha burned the wood .\\n∃e burning (e)∧subject (e, Agatha )∧object (e, wood )\\nc.Oxygen was consumed .\\n∃e′consuming (e′)∧object (e′, oxygen )\\nWe do not go into details of containment of events as expressed by inin the ﬁrst ex-\\nample above, and also the representation of passives as in the third example above.\\nIn these three examples, the author uses predicates such as object andsubject which\\nrepresent more ﬁne-grained relationship with the main predicate (corresponding to\\nthe verb usually) than the examples earlier. Extending this work, [Parsons 1990;\\nHigginbotham 1985; Vlach 1981] have demonstrated that using Davidson’s evari-\\nable allows one to express tense dependency between perception verbs and their\\ninﬁnitival compliments in a natural way.\\n[Parsons 1990] extends Davidson’s approach to logical representation by adding\\nan extra term corresponding to the event type of the predicate. He distinguishes be-\\ntween two types of eventualities: eventualities that culminate called Culcontaining\\nachievements and accomplishments, and those that do not, called Hold containing\\nstates and activities.\\na.John buttered the toast .\\n∃e buttering (e)∧agent (e, Jones )∧theme (e, toast )∧(∃t(t < now∧Cul(e, t))\\nb.Mary knows Fred.\\n∃e knowing (e)∧experiencer (e, Mary )∧theme (e, Fred )∧Hold (e, now ))\\nIn the logical representation in these examples, the author uses predicates such\\nastheme ,agent and experiencer which are usually are called cases in linguistics\\n[Fillmore 1977]. In addition, the author uses a variable tto express time. now is a\\nspecial indexical variable. We do not give detailed discussions of these ﬁne points\\nhere.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 6}), Document(page_content='8· ...\\n[Hobbs 1985] also proposes a logical form based on Davidson’s approach. The\\nmain motivation behind Hobb’s approach is to be able to produce satisfactory\\nsemantic representation when an event is expressed as a noun, or when we want\\nto express the meaning of tenses, modalities, and adverbial modiﬁers. He also\\nexplains how so-called opaque adverbials likealmost in the sentence, John is almost\\na man. can be represented by the Davidsonian approach, which Hobbs extends.\\nHe also shows how the ambiguity between de re andde dicto meanings of sentences\\n[Quine 1956] that discuss beliefs can be explained by his approach to logical form\\nrepresentation of sentences. The representation by Hobbs is quite similar to other\\nsuch representations based on Davidson, although there are some ﬁne points of\\ndiﬀerences, that we do not discuss here. From a practical point of view, several\\nresearch eﬀorts in computational linguistics have adopted Hobb’s logical form, and\\none such recent approach is by [Rudinger and Van Durme 2014] who attempt to\\nmap Stanford dependency parses [De Marneﬀe et al. 2006] into Hobbsian logical\\nform, and discover that sometimes it is possible to do so, but in other cases the\\nmapping requires semantic information that is not present in such dependencies\\nindentiﬁed by the Stanford parser.\\n2.4 Event structure\\nEarly eﬀorts at identiﬁcation of event structure in linguistics was usually limited\\nto explaining essential grammatical phenomena. However, others later proposed\\ncomplex structures that go beyond simple structures such as Davidson’s approach\\nof representing an event by a single logical variable and its components by additional\\npredicates. Understanding the structure of an event entails (i) understanding the\\nargument structure of the word (or, phrase) used to express the event in surface\\nform, (ii) understanding the components in the conceptual or semantic description\\nof an event, and (iii) understanding the relation or mapping between syntactic\\nrealization of an event and its conceptual components. In fact, analysis of argument\\nstructure includes all three steps and requires ﬁnding the relation between meaning\\nof a verb (or a clause) and the syntactic realization of arguments. [Grimshaw 1990;\\nWilliams 1981] introduce argument structure as a distinct level of representation in\\nlinguistics. Other prominent representations proposed include f-structures [Bresnan\\n1982], linear event structures [van Voorst 1988], lexical conceptual structures (LCS)\\n[Jackendoﬀ 1985; Rappaport and Levin 1988] and two related structures: event\\nstructures and qualia structures for arguments [Pustejovsky 1991a].\\nThere are two sides to event structure: syntactic and semantic. When specifying\\nevent structure, at the semantic level, the description must be richer than semantic\\nrole descriptions [Gruber 1965; Fillmore 1968]. [Levin 1985] argues that named roles\\nor thematic roles are too coarse-grained to provide useful semantic interpretation\\nof a sentence. It is also necessary to capture semantic distinctions in a much more\\nﬁne-grained manner compared to prior theories of [Katz and Fodor 1963; Wilks\\n1975; Quillian 1968]. ***A sentence or two on these theories*** By this time it\\nwas clear that sophisticated approaches to specifying event structure must build\\nupon the rich taxonomy of verb classes [Levin 1985] and descriptive vocabulary\\nwork [Talmy 1985] and [Jackendoﬀ 1985].\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 7}), Document(page_content='...· 9\\n2.4.1 Lexicalization Patterns by Talmy. [Talmy 1985] discusses the systematic\\nrelations in language between meaning and surface expression. In particular, Talmy\\nfocuses on verbs and in particular, verbs that describe motion or location. He\\nsketches a “motion” event in order to explore issues in lexicalization. The basic\\nmotion event consists of one object called ﬁgure moving or located with respect to\\nanother object called the referent or the ground . The motion event has additional\\ncomponents such as path and motion ,manner and cause . Talmy gives examples of\\ncases where the verb at once can express, in addition to the action or motion, one\\nor more of ﬁgure, path, manner or cause. If a sematic component such as manner\\nor cause is expressed directly by the verb, it is called conﬂation of manner (or\\ncause) into the verb. Some verbs incorporate aspect, which represents the “pattern\\nof distribution of at ion though time.” In some languages, verbs can incorporate\\npersonation as well. Personation is a speciﬁcation of the person involved, e.g., self\\nor non-self. Some verbs incorporate what is called valence , where in conceptualizing\\nan event that involves several diﬀerent entities in distinct roles, a verb is able to\\ndirect greater attention to some one of these entities that to the others, or perhaps\\nadopt a speciﬁc perspective. Sometimes, semantic components are not incorporated\\ninto the verb, but are expressed through what Talmy calls satellites . A satellite\\nis an immediate constituent of the verb root other than inﬂections, auxiliaries or\\nnominal arguments.\\nTalmy enumerates 35 diﬀerent semantic components. In addition to the six listed\\nabove, these include main purpose, result, polarity, aspect, personation, temporal\\nand spatial setting, gender, valence, etc. Talmy also isolates surface elements within\\na verb complex such as the root verb, inﬂections, appositions, subordinate clauses\\nand satellites. He then examines which semantic elements are expressed by which\\nsurface elements. He ﬁnds that the relationship is mostly not one-to-one. A com-\\nbination of semantic elements may be expressed by a single surface element, or a\\nsingle semantic element by a combination of surface elements. In a similar manner,\\nsemantic elements of diﬀerent types can be expressed by the same type of surface\\nelements or by several diﬀerent ones.\\nTalmy’s work does not enumerate lexical entries for speciﬁc verbs, but provides\\ndetailed discussion on semantic facets of meanings of a verb. The main thrust of\\nTalmy’s work is to demonstrate that semantic elements and surface elements relate\\nto each other in speciﬁc patterns, both typological and universal. In work prior\\nto Talmy’s, most work has treated language’s lexical elements as atomic givens,\\nwithout involving semantic components that comprise them. These studies treated\\nthe properties that such whole forms can manifest, in particular, word order, gram-\\nmatical relations and case roles. Talmy’s cross-linguistic study determines semantic\\ncomponents’ surface presence, site (their host constituent or grammatical relation)\\nand combination within a site. In addition, Talmy’s tracing of surface occurrence\\npatterns extends beyond treating single semantic component at a time to treating\\na concurrent set of components.\\nLexical semantics must strive to represent at least some of the various seman-\\ntic components that Talmy enumerates. In addition, it must incorporate ways of\\nmapping from syntax to semantics or vice versa. In a very simple system, a set of\\ndetailed rules may be able to enumerate the mappings from syntax to semantics\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 8}), Document(page_content='10· ...\\nand vice versa. In a complex modern system, it is necessary that a machine learn-\\ning technique will automatically acquire the mappings. This usually requires a lot\\nof labeled examples for a machine learning program to learn such mappings. We\\ndiscuss some such as eﬀorts later in the paper.\\n2.4.2 Jackendoﬀ’s Lexical Conceptual Structure.\\n2.4.3 Generative Lexicon by Pustejovsky. Consider the following illustrative ex-\\namples.\\na)Mary walked.\\nb)Mary walked to the store.\\nc)Mary walked for 30 minutes.\\nSentence a)describes a process, which is an activity of of indeﬁnite length, i.e., the\\nsentence does not say how long the activity of walking took. Although b)does not\\ngive an explicit time duration for the walking event, it depicts an accomplishment\\nand provides a logical culmination to the duration of the event of walking because\\nthe event is over when Mary reached the store. Sentence c)talks about a bounded\\nprocess in which, the event of walking terminating although it does not provide an\\nexplicit termination point, but provides a bound to the time extent of the activity\\nin terms of a duration adverbial. This example motivates the observation that\\nthe use of prepositional phrases or duration adverbials can change the (aspectual)\\nclassiﬁcation of an event. To explain such phenomena better, it is beneﬁcial to have\\nmore complex event structures or lexical analysis of event words. [van Voorst 1988]\\nhypothesizes that the direct object plays a role in delimitation of an event, i.e.,\\nwhether it has a culmination or not. [Pustejovsky 1991b; 1991a] builds upon such\\nobservations and hypothesizes that it is necessary to know the how an event can be\\nbroken down into sub-events. He provides the following reasons for sub-eventual\\nanalysis.\\n—Sub-eventual analysis of predicates allows verbal decomposition leading to more\\ncomplex lexical semantics.\\n—Scope of adverbial modiﬁcation, for some adverbials, can be explained better\\nusing event sub-structures.\\n—Semantic arguments of items within a complex event structure can be mapped\\nonto argument structures better.\\nPustejovsky describes a generative lexicon in the sense that meanings are described\\nin terms of a limited number of so-called generative devices or primitives by drawing\\nupon Aristotle’s species of opposition [Lloyd 1968]. For example, to express the\\nmeaning of the word closed as in The door is closed ,The door closed orJohn closed\\nthe door , one needs the concept of opposition between closed and not-closed . This\\nessential opposition in the meaning of a lexical item is described by Pustejovsky\\nin terms of what is called the qualia structure of the lexical item. Thus, there are\\nthree primary components to the event structure proposed by Pustejovsky.\\n—Event type : The event type of the lexical item is given in terms of the classiﬁcation\\nschemes discussed earlier.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 9}), Document(page_content='...· 11\\n—Rules for event composition : Since an event may be expressed by more than a\\nsingle verb, the meanings of several lexical items may have to be composed to\\nobtain a description. For example, how does PP attachment change the meaning\\nof the central event in context?\\n—Mapping rules from event structure to argument structure : Pustejovsky describes\\na number of rules or principles for such mapping. These rules describe how how\\nsemantic participants are realized syntactically.\\nPustejovsky provides lexical meaning in terms of four separate structures.\\n—Argument structure : The behavior of a word as a function, with its arity. This\\nprovides the predicate argument structure for a word, which speciﬁes how it maps\\nto syntax.\\n—Event structure : It identiﬁes a speciﬁc event type for a word or a phrase, following\\n[Vendler 1967].\\n—Qualia structure : It provides the essential attributes of an object that need to be\\nexpressed lexically.\\n—Inheritance structure : It speciﬁes how the word is globally related to other con-\\ncepts in the lexicon.\\nIn summary, Pustejovsky endows complexity to lexical entries for verbs as well\\nas non-verbs so that semantic weight does not fall on verbs alone in the lexicon and\\nwhen composing the meaning of a sentence from its constituents. Pustejovsky’s\\napproach also reduces the number of lexical entries necessary for individual verbs\\nbecause the lexical entries become more general. Pustejovosky focuses on creating\\nmore muscular compositional semantics rather than decomposing a verb’s meaning\\ninto a speciﬁed number of primitives.\\n2.5 Semantic Arguments and Syntactic Positions\\nFrequently, speciﬁc semantic arguments of a verb (also called thematic arguments)\\nappear in characteristic syntactic positions. This has led to theories or proposals\\nregarding mapping between the two. These theories state that speciﬁc semantic\\narguments belong in speciﬁc syntactic positions and that there is 1-1 relationship\\nbetween semantic argument and (initial2) syntactic position. Such proposals or the-\\nories include the Universal Alignment Hypothesis [Perlmutter 1978] and Uniformity\\nof Theta Assignment Hypothesis [Baker 1988]. These are supposed to be universal\\nin that they applied across languages and across verbs. For example, agents appear\\nin subject positions across languages and verbs. This mapping is thus universal.\\nHowever, other mappings are not so universal. For example, the theme can appear\\nin object, subject or indirect object position; and the experiencer can appear in\\nsubject or object position.\\nA theory that explains lexicon-to-syntax mapping also needs to explain the ex-\\nistence of argument alterations. In other words, it should explain the possibility\\nthat the same semantic role can appear in diﬀerent syntactic positions for the same\\nverb. Usually, linguists classify verbs into a number of semantic classes (diﬀerent\\n2The syntactic or surface position of a constituent may change under certain circumstances, e.g.,\\nwhen one formulates a question from a canonical or declarative sentence.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 10}), Document(page_content='12· ...\\n(verb :orth \"abandon\" :subc ((np-pp :pval (\"to\")) (np)))\\n(noun :oath \"abandon\" :features ((countable :pval (\"with\"))))\\n(verb :orth \"abstain\" :subc ((intrans) (pp :pval (\"from\")) (p-in-sc :pval (\"from\"))))\\n(verb :oath \"accept\" :subc ((np) (that-s) (np-as-np)))\\n(verb :orth promote :subc ( (np-pp :pval (to for into from)) (np-pp-pp\\n:pval (for to into from)) (possing)(np)(np-as-np)(np-tobe)))\\nFig. 1. Sample Comlex verb entries\\nfrom the ones we talked about earlier) and for each class, a set of mapping relations\\nand a set of argument alterations are speciﬁed [Levin 1993; Levin and Hovav 1995;\\nPinker 1989]. However, other researchers claim that such semantic classiﬁcation is\\ndiﬃcult to obtain because semantically similar verbs may behave diﬀerently across\\nlanguages [Rosen 1984], a given verb in a language may have multiple syntactic\\nrealizations [Rosen 1984; 1996], and semantically similar verbs may allow several\\nsyntactic realizations [Rosen 1996].\\n3. LEXICAL RESOURCES FOR ACTION OR EVENT REPRESENTATION\\nThe discussions on lexical representation of verbs so far have been based on eﬀorts\\nwhere a small number of examples were studied intently by linguists before making\\nthe various proposals. Starting the 1980s but more so in the 1990s, when computer\\nscientists started to focus more on analysis of large text corpora, it became evident\\nto some that the lexical analysis of pure linguists can be extended by knowledge\\ngathered from such corpora. This led to development of the Comlex lexicon [Grish-\\nman et al. 1994], WordNet [Miller 1995; Fellbaum 2010], VerbNet [Schuler 2005],\\nFrameNet [Fillmore and Baker 2001a; 2001b; Fillmore et al. 2003; Fillmore 2006]\\nand other resources. Some of these may have started without an automatic analy-\\nsis of corpora, but soon corpora were used to reﬁne and enhance the initial lexical\\nresources. Comlex was a substantial resource whose creators spent a lot of eﬀort\\nin enumerating subcategorization features. WordNet is a large lexical resource or\\nontology, which encompasses words from all categories. WordNet includes verbs,\\nbut is not verb-speciﬁc. VerbNet, of course, is focussed on verbs alone. FrameNet\\nis also focussed on verbs. Both VerbNet and FrameNet attempt to represent all\\nverbs, not only those which are used to represent “events”. However, the term\\nevent itself is not clearly deﬁned and most anything that is described by a verb can\\nbe considered an event in some context or another.\\n3.1 Comlex and Nomlex Lexicons\\nComlex was created at New York University as a computational lexicon provid-\\ning detailed syntactic information on approximately 38,000 words [Grishman et al.\\n1994]. Of course, not all of these were verbs or words that describe actions. The\\nfeature set Comlex provided were more detailed than commerically available dic-\\ntionaries at the time such as the Oxford Advanced Learner’s Dictionary (OALD)\\n[Hornby 1980] and Longman’s Dictionary of Contemporary Englisch (LDOCE) [Pro\\n1981]. The initial word list was derived from OALD. The lexicon used a Lisp-like\\nnotation for dictionary entries. We see some sample entries for verbs in Comlex in\\nFigure 1.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 11}), Document(page_content='...· 13\\n(vp-frame s :cs ((s 2 :that-comp optional))\\n:gs (:subject 1 :comp 2)\\n:ex \"they thought (that) he was always late\")\\n(vp-frame to-inf-sc :cs ((vp :2 :mood to-infinitive :subject 1))\\n:features (:control subject)\\n:gs (:subject 1 :comp 2)\\n:ex \"1 wanted to come.\")\\n(vp-frame to-inf-rs :cs ((vp 2 :mood to-infinitlve :subject 1))\\n:features (:raising subject)\\n:gs (:subject () :comp 2)\\n:ex \"they seemed to wilt.\")\\nFig. 2. Sample Comlex Subcategorization Frames\\nComlex paid particular attention to providing detailed subcategorization or com-\\nplement information for verbs, and nouns and adjectives that take complements.\\nComlex was inﬂuenced by prior work on lexicon such as the Brandeis Verb Lexi-\\ncon [Grimshaw and Jackendoﬀ 1981], the ACUILEX project [Sanﬁlippo 1994], the\\nNYU Lingustic String Project [Sager 1981], the OALD and the LDOCE, and it\\nincorporated distinctions made in these dictionaries. Comlex had 92 diﬀerent sub-\\ncategorization features for verbs. The features recorded diﬀerences in grammatical\\nfunctions as well as constituent structure. In particular, Comlex captured four\\ndiﬀerent types of control: subject control, object control, variable control and ar-\\nbitrary control. It was also able to express the fact that a verb may have diﬀerent\\ncontrol features for diﬀerent complement structures, or diﬀerent prepositions within\\nthe complement. Figure 2 shows a few complements used in Comlex. Here :csis\\nthe constituent structure, :gsis the grammatical structure and :exare examples.\\nThe authors created a initial lexicon manually and then reﬁned it using a variety\\nof sources, both commercial and corpus-based.\\nThe Nomlex dictionary of nominalizations was also developed at NYU [Macleod\\net al. 1998; Meyers et al. 1998]. It enumerated allowed complements for nomi-\\nnalizations, and also related nominal complements of the corresponding verbs. A\\nnominalization is the noun form of a verb. For example, the verb promote is nom-\\ninalized as nominalization . Similarly, the nominalizations of the verb appoint are\\nappointment and appointee . Nomlex entries are similar in syntax to Comlex en-\\ntries. Each Nomlex entry has a :nom-type feature which speciﬁes four types of\\nnominalizations: action ( appointment ,destruction ) or state ( knowledge ), subject\\n(teacher ), object ( appointee ) and verb-part for those nominalizations that incorpo-\\nrate a verbal particle ( takeover ). Meyers et al. [Meyers et al. 1998] presented a\\nprocedure what mapped syntactic and semantic information for an active clause\\ncontaining a verb e.g., ( IBM appointed Alice Smith as vice president ) into a set of\\npatterns for nominalization ( IBM’s appointment of Alice Smith as vice president or\\nAlice Smith’s appointment as vice president ). The lexical entry for the verb appoint\\nused in Comlex is given in Figure 1. The lexical entry in Nomlex for the action\\nnominalization appointment is given in Figure 3.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 12}), Document(page_content='14· ...\\n(nom :orth promotion :verb promote\\n:nom-type((verb-nom))\\n:verb-subj ((n-n-mod) (det-poss))\\n:verb-subc ((nom-np :object ((det-poss)(n-n-mod)(pp-of)))\\n(nom-np-as-np :object ((det-poss) (pp-of)))\\n(nom-possing :nom-subc ((p-possing :pval (of))))\\n(nom-np-pp :object ((det-poss) (n-n-mod) (pp-of))\\n:pval (into from for to))\\n(nom-np-pp-pp:object ((det-poss) (n-n-mod) (pp-of))\\n:pval (for into to) :pval2 (from))))\\nFig. 3. The lexical entry for the action nominalization appointment in Nomlex. The entry for the\\nverb appoint is given in Figure 1\\n.\\n3.2 Levin’s Verb Classes\\nLevin’s verb classes [Levin 1993] explicitly provide the syntax for each class, but do\\nnot provide semantic components. The classes are based on the ability or inability\\nof a verb to occur in pairs of syntactic frames, with the assumption that syntactic\\nframes reﬂect the underlying semantics. For example, break verbs and cutverbs\\nare similar because they can all take part in transitive and middle constructions.\\nHowever, only break verbs can occur in simple intransitive constructs. Similarly,\\ncutverbs can occur in conative constructs and break verbs cannot. The explanation\\ngiven is that cutdescribes a sequence of actions that result in the goal of separating\\nan object into pieces. It is possible that one can perform the actions without\\nachieving the result (e.g., John cut at the loaf ). For break , the result is a changed\\nstate where the object becomes separated into pieces. If the result is not achieved,\\nwe cannot say that the action of breaking took place. The examples below are\\ntaken from [Kipper et al. 2000].\\n—Transitive Construction : (a) John broke the window. , (b) John cut the bread.\\n—Middle Construction : (a) Glass breaks easily. , (b) This loaf cuts easily .\\n—Intransitive Construction : (a) The window broke. , (b) * The bread cut.\\n—Conative Construction : (a) * John broke at the window. , (b) John valiantly cut\\nat the frozen loaf, but his knife was too dull to make a dent in it.\\nLevin’s original classes had some inconsistencies. For example, many verbs were\\nlisted in multiple classes, some of which had conﬂicting syntactic frames. [Dang\\net al. 1998] reﬁned the original classiﬁcation to remove some of these problems\\nto build a more ﬁne-grained, syntactically and semantically coherent reﬁned class\\ncalled intersective Levin classes . Levin’s classes also are focussed mostly on verbs\\ntaking noun (NP) and prepositional phrase (PP) complements, and are weak on\\ncoverage of ADJP, ADVP, sentential complement, etc. VerbNet is built using these\\nclasses.\\nOrganization of verbs into such classes capture generalizations about their prop-\\nerties. Such classes also help create better NLP systems. Many NLP systems\\nbeneﬁt from using the mapping from surface realization of arguments to predicate-\\nargument structure that is available in such classes. These classes also capture\\nabstractions (e.g., syntactic and semantic properties) and as a result, they are\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 13}), Document(page_content='...· 15\\nhelpful in many operational contexts where the available corpora are small in size\\nand thus, it is not possible to extract detailed lexical information about verbs from\\nsuch small corpora. The predictive power of the classes can compensate for the\\nlack of suﬃcient data. Lexical classes have been helpful in tasks such as subcatego-\\nrization acquisition [Dorr 1997; Prescher et al. 2000; Korhonen and Briscoe 2004],\\nautomatic verb acquisition [Swift 2005], semantic role labeling [Swier and Steven-\\nson 2004], and word sense disambiguation [Dang 2004]. ***Add newer citations for\\napplication. Look at after 2004 proceedings of NAACL-HLT***\\n3.3 WordNet\\nThe WordNet project [Miller 1995; Fellbaum 2010] started in the mid-1980s at\\nPrinceton University and over time, has become the most widely used lexical re-\\nsource in English, especially when one needs a lexical resource that can be used\\nby a program. Wordnet was primarily designed as a semantic network and later\\nmodiﬁed to be a lexical database.\\nWordNet groups words into synsets (synonym set) and contains relations among\\nthese synsets. A synset contains all the word forms that can refer to a given\\nconcept or sense. For each sense of each word, WordNet also provides a short,\\ngeneral deﬁnition called its gloss and example usages.\\nAs the name hints, the WordNet can be thought of as a large graph where the\\nwords and synsets are nodes. These nodes linked by edges that represent lexical\\nand semantic-conceptual links, which we discuss brieﬂy below. Individual words\\nmay also be linked with antonym links. Superclass-subclass relations link entire\\nsynsets. WordNet has entries for verbs, nouns, adjectives and adverbs.\\nTo get a better feel for what WordNet is like, let us look at the online version\\nof WordNet3at Priceton University. When we search for the word assault in the\\nonline WordNet, the results come in two parts: noun and verb, because assault can\\nbe either a verb or a noun. The results that show up for verb are given in Figure\\n4. The verb senses of assault belongs to three synsets. In other words, it has three\\nsenses or can refer to three diﬀerent concepts. Each sunset is composed of several\\nverbs. The second of these synsets contains one sense of each of the verbs assail ,\\nset on and attack .\\nA verb may have four types of entries in WordNet: hypernyms, toponyms, entail-\\nment and coordinate terms. These terms are deﬁned here. A verb Yis ahypernym\\nof the verb Xif the activity Xis a (kind of) Y. For example, to perceive is an\\nhypernym of to listen . A verb Yis a troponym of the verb Xif the activity Yis\\ndoing Xin some manner. For example, to lisp is a troponym of to talk . A verb Yis\\nentailed by Xif by doing Xone must be doing Y. For example, to sleep is entailed\\nbyto snore .Coordinate terms are those verbs that share a common hypernym,\\ne.g., to lisp and to yell . If we want to see the direct troponym of the second synset\\nfor the verb meaning of assault , we get what we see in Figure 5.\\nWordNet has been used in many applications. However, it is most commonly\\nused as a computational lexicon or “ontology” of English (or, another language)\\nfor word sense disambiguation, a task that assigns the most appropriate senses\\n(i.e. synsets) to words in speciﬁc contexts. Although WordNet is large and de-\\n3available at http://wordnetweb.princeton.edu\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 14}), Document(page_content='16· ...\\nS: (v) assail, assault, set on, attack (attack someone physically or\\nemotionally)\\nS: (v) rape, ravish, violate, assault, dishonor, dishonour, outrage (force\\n(someone) to have sex against their will)\\nS: (v) attack, round, assail, lash out, snipe, assault (attack in speech or\\nwriting)\\nFig. 4. Online WordNet search for the word attack . We show only the verb entries, with gloss for\\neach entry.\\nS: (v) bait (attack with dogs or set dogs upon)\\nS: (v) sic, set (urge to attack someone)\\nS: (v) bulldog (attack viciously and ferociously)\\nS: (v) rush (attack suddenly)\\nS: (v) blindside (attack or hit on or from the side where the\\nattacked person’s view is obstructed)\\nS: (v) savage (attack brutally and fiercely)\\nS: (v) reassail (assail again)\\nS: (v) jump (make a sudden physical attack on)\\nS: (v) beset, set upon ((5) )\\nS: (v) rape, ravish, violate, assault, dishonor, dishonour, outrage\\n(force (someone) to have sex against their will)\\nS: (v) desecrate, profane, outrage, violate (violate the sacred\\ncharacter of a place or language)\\nS: (v) molest (harass or assault sexually; make indecent advances to)\\nFig. 5. The direct toponym set for the second sunset for the verb assault seen in Figure\\n.\\ntailed, WordNet does not have information required by NLP applications such\\nas predicate-argument structure. Although WordNet contains a suﬃciently wide\\nrange of common words, it does not cover special domain vocabulary. It is general\\nin nature, and therefore diﬃcult to use if specialized vocabulary is needed. Also,\\nWordNet senses are sometimes overly ﬁne-grained even for human beings and as a\\nresults, some researcher argue that it cannot achieve very high performance in the\\ntasks where it is applied. Although WordNet is the most widely used online lexical\\ndatabase in NLP applications, is also limited in its coverage of verbs.\\nThe English WordNet currently contains approximately 117,659 synsets, each\\nsunset corresponding to a sense of a word. It has 11,529 verbs that belong to\\n13,767 synsets. It also contains 117,798 nouns that belong to 82,115 synsets. Word-\\nNets have been developed or are being developed in a large number of languages\\nsuch as Catalan, French, Spanish, Japanese, Chinese, Danish, Korean and Russian.\\nNotable collaborative eﬀorts include Euro Wordnet [Vossen 2004; 1998a; 1998b],\\nAsian Wordnet [Charoenporn et al. 2008; Robkop et al. 2010; Sornlertlamvanich\\net al. 2009] and Indo WordNet [Sinha et al. 2006] projects. The Indo WordNet\\nfocuses on 18 major languages of India. For example, as of June 20124there are\\n15,000 synsets in the Assamese WordNet, 24,000 in Bengali, 16,000 in Bodo, 27,000\\nin Gujarati, and 31,500 in Oriya. WordNets in most other languages are not as\\n4http://en.wikipedia.org/wiki/IndoWordNet\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 15}), Document(page_content='...· 17\\nsophisticated as the one in English.\\n3.4 FrameNet\\nFrameNet [Baker et al. 1998; Fillmore et al. 2003; Ruppenhofer et al. 2006] is\\nanother substantial publicly available lexical resource that has come into existence\\nindependently. It is based on the theory of frame semantics [Fillmore 1976; Petruck\\n1996; Fillmore and Baker 2001a; 2001b; Fillmore 2006] where a frame corresponds\\nto a stereo-typical scenario involving an interaction and participants , where partic-\\nipants play some kind of roles. The idea is that the meanings of most words are\\nbest understood in context. FrameNet proposes a small context, called a semantic\\nframe , a description of a type of event, relation or entity and the participants in\\nit. A frame has a name and this name is used to identify a semantic relation that\\ngroups together the semantic roles.\\nAlthough frames mostly correspond to verbs, there are frames that can be iden-\\ntiﬁed by nouns and adjectives. FrameNet also has a large number of annotated sen-\\ntences. Each annotated sentence exempliﬁes a possible syntactic realization of the\\nsemantic role associated with a frame for a given target word. FrameNet extracts\\nsyntactic features and corresponding semantic roles from all annotated sentences\\nin the FrameNet corpus, it builds a large set of rules that encode possible syntactic\\nrealizations of semantic frames.\\nFrameNet aims to document the range of semantic and syntactic combinatory\\npossibilities— valences –of each word in each of its senses, through computer-assisted\\nannotation of example sentences and automatic tabulation of the annotation re-\\nsults. The FrameNet lexical database, currently contains more than 10,000 lexical\\nunits (deﬁned below), more than 6,000 of which are fully annotated, in nearly 800\\nhierarchically-related semantic frames, exempliﬁed in more than 170,000 annotated\\nsentences. See the FrameNet website5for the latest statistics. FrameNet has been\\nused as a semantic role labeling, used in applications such as information extrac-\\ntion, machine translation, event recognition, sentiment analysis, etc., like the other\\npublicly available lexical resources.\\nAn example of a frame is Attack . This frame has several frame elements . The\\ncore frame elements are assailant and victim . There are a large number of non-\\ncore frame elements. These include Circumstances ,Containing event ,Direction ,\\nDuration ,Explanation ,Frequency ,Manner ,Means ,Place ,Purpose ,Result ,Time ,\\nWeapon , etc. For each of these frame elements there can be seen in one or more\\nannotated sentences. Here is an example annotated sentence.\\n[Assailant The gang ]ASSAULTED [V ictim him]\\n[Time during the drive to Rickmansworth ][Place in Hertfordshire ]...(1)\\nThe frame Attack is associated with a large number of associated units. These\\ninclude verbs and nouns. Example verbs are ambush ,assail ,assault ,attack ,bomb ,\\nbombard ,charge ,hit,inﬁltrate ,invade ,raid,storm and strike . Examples of nouns\\nareairstrike ,ambush ,assailant ,assault ,attack , etc. The frame Attack inherits from\\n5https://framenet.icsi.berkeley.edu/fndrupal/home\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 16}), Document(page_content='18· ...\\na frame called Intentionally aﬀect . It is inherited by frames Besieging ,Counterat-\\ntack,Invading and Suicide attack .\\nFrameNet annotates each frame element (or its representation, actually) in at\\nleast three layers: a frame element name (e.g., Food), a grammatical function (e.g.,\\nObject) and a phrase type (e.g., NP). Only the frame elements are shown in the\\nWeb-based interface to reduce visual clutter, although all three are available in the\\nXML downloads. FrameNet has deﬁned more than 1000 semantic frames. These\\nframes are linked together using frame relations which relate more general frames\\nto speciﬁc ones. This allows for reasoning about events and intentional actions.\\nBecause frames are semantic, they are often similar across languages. For exam-\\nple, frames about buying and selling involve frame elements Buyer ,Seller ,Goods\\nand Money in every language. FrameNets have been developed for languages such\\nas Portuguese, German, Spanish, Chinese, Swedish and Japanese.\\nAt the current time, there are 1159 frames in FrameNet. There are approximately\\n9.6 frame elements per frame. There are 12595 lexical units of which 5135 are nouns,\\n4816 are verbs, 2268 are adjectives. There are 12.1 lexical units per frame.\\nThere have been some attempts at extending the coverage of FrameNet. One such\\neﬀort is by [Rastogi and Van Durme 2014] who use a new broad-coverage lexical-\\nsemantic resource called PPDB to add lemmas as pontential triggers for a frame\\nand to automatically rewrite existing example sentences with these new triggers.\\nPPDB, The Paraphrase Database, is a lexical, phrasal and syntactic paraphrase\\ndatabase [Ganitkevitch et al. 2013]. They use PPDB’s lexical rules along with a\\n5-gram Kneser-Ney smoothed language model trained using KenLM [Heaﬁeld et al.\\n2013] on the raw English sequence of the Annotated Gigaword corpus [Napoles\\net al. 2012].\\n3.5 PropBank\\nPropBank [Kingsbury and Palmer 2002; 2003; Palmer et al. 2005] is an annotated\\ncorpus of verb propositions and their arguments. PropBank does not annotate\\nevents or states of aﬀairs described using nouns. PropBank-style annotations usu-\\nally are closer to the syntactic level, whereas FrameNet-style annotations are more\\nsemantically motivated although, as discussed earlier, FrameNet provides layers of\\nannotations including syntactic parses. PropBank annotates one million words of\\nthe Wall Street Journal portion of the Penn Treebank [ ?] with predicate-argument\\nstructure for verbs using semantic role labels for each verb argument.\\nAlthough the same tags are used across all verbs (viz., Arg0, Arg1, ···, Arg5),\\nthese tags have verb-speciﬁc meaning. FrameNet requires that the use of a given\\nargument label is consistent across diﬀerent uses of a speciﬁc verb, including its\\nsyntactic alternations. Thus, Arg1 (italicized) in “John broke the window broke”\\nis the same window that is annotated as the Arg1 in “ The window broke” even\\nthough it is the syntactic subject in one case and the syntactic object in another.\\nFrameNet does not guarantee that an argument label is used consistently across\\ndiﬀerent verbs. For example, Arg2 is used as label to designate the destination of\\nthe verb “bring”, but the extent of the verb “rise”. Generally, the arguments are\\nsimply listed in the order of their prominence for each verb. However, PropBank\\ntries to use Arg0 as the consistent label for the “prototypical agent” and Arg1 for\\nthe “prototypical patient” as discussed in [Dowty 1991].\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 17}), Document(page_content='...· 19\\nPropBank Role Meaning of PropBank Role VerbNet (Theta) Role\\nArg0 attacker Agent\\nArg1 entity attacked Theme\\nArg2 attribute Predicate\\nExample: Mr. Baldwin is attacking the greater problem: lack of ringers.\\nPropBank Role Value\\nArg0 Mr. Baldwin\\nArg1 the greater problem: lack of ringers\\nTable III. PropBank entry for attack.01 , a sense of the verb attack\\npurchase buy sell\\nArg0: buyer Arg0: buyer Arg0: seller\\nArg1: thing bought Arg1: thing bought Arg1: thing sold\\nArg2: seller Arg2: seller Arg2: buyer\\nArg3: prince paid Arg3: price paid Arg3: price paid\\nArg4: benefactive Arg4: benefactive Arg4: benefactive\\nTable IV. Roles for the verbs purchase ,buyand sellin PropBank\\nPropBank divides words into lexemes using a very coarse-grained sense disam-\\nbiguation scheme. Two senses are considered distinct only if their argument labels\\nare diﬀerent. In PropBank each word sense is called a frameset . PropBank’s model\\nof predicate-argument structure diﬀers from dependency parsing. In dependency\\nparsing, each phrase can be dependent only on one other phrase. But, in Prop-\\nBank, a single phrase can be arguments to several predicates. PropBank provides\\na lexicon which divides each word into coarse-grained senses or framesets, and pro-\\nvides examples usages in a variety of contexts. For example, the to make an attack,\\ncriticize strongly sense of the predicate lemma (or, verb) attack is given in Table\\nIII along with an example.\\nPropBank tries to keep rolesets consistent across related verbs. Thus, for exam-\\nple, the buyroleset is similar to the purchase and sellrolesets. See Table IV, taken\\nfrom [Kingsbury and Palmer 2002].\\nOne can clearly see that it may be possible to merge such similar framesets\\ntogether to obtain something similar to the verb roles in FrameNet’s Commerce\\nframeset.\\nAlthough similar, PropBank diﬀers from FrameNet we have discussed earlier\\nin several ways. PropBank is a resource focussed on verbs whereas FrameNet is\\nfocussed on frame semantics that generalizes descriptions across similar verbs as\\nwell as nouns and other words (e.g., adjectives) as discussed earlier. PropBank was\\ncreated with the idea of serving as training data to be used with machine learning\\nalgorithms for the task of semantic role labeling. It requires all arguments to a verb\\nto be syntactic constituents in nature. In addition, PropBank diﬀerentiates among\\nsenses of a verb if the senses take diﬀerent sets of arguments. There is a claim that\\ndue to such diﬀerences, semantic role labeling is easier using a corpus annotated\\nwith PropBank type annotation compared to FrameNet type annotation.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 18}), Document(page_content='20· ...\\n3.6 VerbNet\\nVerbNet [Kipper et al. 2000; Kipper et al. 2000; Kipper-Schuler 2005] attempts to\\nprovide a deﬁnitive resource for lexical entries for English verbs. It is compatible\\nwith WordNet, but has lexical entries with explicit syntactic and semantic infor-\\nmation about verbs, using Levin’s verb classes [Levin 1993]. It uses verb classes\\nto capture generalizations and for eﬃcient encoding of the lexicon. Its syntactic\\nframes for verb classes are represented using a ﬁne-grained variation of Lexicalized\\nTree Adjoining Grammers [Joshi 1985; Schabes 1990; Dang et al. 1998] augmented\\nwith semantic predicates, allowing for creating compositional meanings for more\\ncomplex constituents such as phrases and clauses. VerbNet provides traditional\\nsemantic information such as thematic roles and semantic predicates, with syntac-\\ntic frames and selectional restrictions. it also allows for extension of verb meaning\\nthrough adjunction of particular syntactic phrases.\\nA verb entry corresponds to a set of classes, corresponding to the diﬀerent senses\\nof the verb. For each verb sense, there is a verb class as well as speciﬁc selectional\\nrestrictions and semantic characteristics that may not be captured by class mem-\\nbership. VerbNet also contains references to WordNet synsets. Verb classes capture\\ngeneralizations about verb behavior. Each verb class lists the thematic roles that\\nthe predicate-argument structure of its members allows, and provides descriptions\\nof the syntactic frames corresponding to allowed constructs, with selectional re-\\nstrictions given for each argument in each frame. Verb classes are hierarchically\\norganized. It required some manual restructuring of Levin’s classes. Each event\\nEis decomposed into a three-part structure according to [Moens 1987; Moens and\\nSteedman 1988]. VernNet uses a time function for each predicate specifying whether\\nthe predicate is true during the preparatory, culmination or consequent/result stage\\nof an event. This structure allows VerbNet to express the semantics of classes of\\nverbs like Change of State verbs. For example, in the case of the verb break , it\\nis important to distinguish between the state of the object before the end of the\\naction and the new state that results afterwards.\\nTable V is an example of a simpliﬁed VerbNet entry from its website6. The\\noriginal VerbNet was extended using extensions proposed by [Korhonen and Briscoe\\n2004]. This resulted in the addition of a large number of new classes, and also a\\nmuch more comprehensive coverage of English verbs. Table VI provides statistics\\nof VerbNet’s coverage in its initial version, VerbNet as described in [Kipper et al.\\n2000; Kipper et al. 2000; Kipper et al. 2008], and its current version as in its oﬃcial\\nWebsite.\\nThe absence of any lexicon or resource that provides for accurate and compre-\\nhensive predicate-argument structure (or semantic role labels) for English verbs has\\nbeen long considered a critical element that was needed to produce robust natural\\nlanguage processors. This was shown clearly by [Han et al. 2000] who evaluated\\nan English-Korean machine translation system. The authors showed that among\\nseveral factors impacting on the low quality of translations, one that was most inﬂu-\\nential was the inability to predicate-argument structure. Even with a grammatical\\nparse of the source sentence ad complete vocabulary coverage, the translation was\\n6http://verbs.colorado.edu/ ~mpalmer/projects/verbnet.html\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 19}), Document(page_content='...· 21\\nClass Hit-18.1\\nRoles and Restrictions: Agent[+int control] Patient[+concrete] Instrument[+concrete]\\nMembers: bang, bash, hit, kick, ···\\nFrames:\\nName Example Syntax Semantics\\nBasic\\nTransi-\\ntivePaula hit\\nthe ballAgent V\\nPatientcause(Agent, E) manner(during(E), directedmotion,\\nAgent)\\n!contact(during(E), Agent, Patient) man-\\nner(end(E),forceful, Agent)\\ncontact(end(E), Agent, Patient)\\nTable V. Simpliﬁed VerbNet entry for Hit-18.1 class. Every class is numbered.\\nVerbNet1.0 VerbNet [Kip-\\nper et al. 2000]VerbNet now\\nFirst-level classes 191 237 274\\nThematic roles 21 23 23\\nSemantic predicates 64 94 94\\nSelectional restrictions (semantic) 36 36 ?\\nSyntactic restrictions 3 55 55\\nLemmas 3007 3175 3769\\nVerb senses 4173 4526 5257\\nTable VI. VerbNet statistics\\nfrequently bad. This is because, the authors found, that although the parser rec-\\nognized the constituents that are verb arguments, it was unable to precisely assign\\nthe arguments to appropriate positions. This led to garbled translations. Sim-\\nply preserving the proper argument position labels and not changing other things,\\nresulted in substantial improvement in acceptable translations. When using one\\nparser, the improvement was 50%; with a second parser, the improvement was dra-\\nmatic 300%. Thus, the purpose in developing lexical resources such as FrameNet\\nand PropBank, PropBank especially so, is to provide for training data annotated\\nwith predicate-argument positions with labels. Such data can be used with machine\\nlearning techniques.\\n3.7 Combining FrameNet, VerbNet and WordNet\\nThere have been attempts to integrate lexical resources to obtain more robust re-\\nsources with wider coverage. We discuss one such eﬀort here. [Shi and Mihalcea\\n2005] integrate FrameNet, VerbNet and WordNet discussed earlier into a single\\nand richer resource with the goal of enabling robust semantic parsing. The reason\\nfor building connections among the three lexical resources is that similar syntactic\\npatterns often introduce diﬀerent semantic interpretations and similar meanings\\ncan be realized in many diﬀerent ways. The improved resource provides three en-\\nhancements: (1) It extends the coverage of FrameNet, (2) It augments VerbNet’s\\nlexicon with frame semantics, and (3) It implements selectional restrictions using\\nWordNet semantic classes. They use knowledge about words and concepts from\\nWordNet, information about diﬀerent situations from FrameNet, and verb lexicon\\nwith selectional restrictions from VerbNet. They extract syntactic features and\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 20}), Document(page_content='22· ...\\ncorresponding semantic roles from all annotated sentences in FrameNet to build a\\nlarge set of rules that encode the possible syntactic realization of semantic frames.\\nThey identify the VerbNet verb class that corresponds to a FrameNet frame and\\nthis allows them to parse sentences that include verbs not covered by FrameNet.\\nThis they do by exploiting a transitivity relation via VerbNet classes: verbs that\\nbelong to the same Levin classes are likely to share the same FrameNet frame, and\\ntheir frame semantics can be analyzed even if not explicitly deﬁned in FrameNet.\\nThey use information from WordNet in several stages in the parsing process. The\\nargument constraints encoded in VerbNet (e.g., + animate, +concrete ) are mapped\\nto WordNet semantic classes, to provide selectional restrictions for better frame\\nselection and role labeling in a semantic parser. In addition, the mapping between\\nWordNet verb entries and FrameNet lexical units allows them to extend the parser\\ncoverage, by assigning common frames to verbs that are related in meaning accord-\\ning to the WordNet semantic hierarchies. The authors found that their mapping\\nalgorithms produced 81.25% correct assignment of VerbNet entries with a correct\\nFrameNet frame. They also were able to map 78.22% VerbNet predicate-argument\\nstructures with some syntactic features and selectional restrictions to the corre-\\nsponding FrameNet semantic roles.\\n3.8 OntoNotes and Other Large-scale Annotated Corpora\\nThe OntoNotes project [Hovy et al. 2006; Sameer et al. 2007; Weischedel 2011]\\nhas created an infrastructure for much richer domain independent representation\\nof shallow meaning for use in natural language processing tasks, including event\\ndetection and extraction, in English, Chinese and Arabic. OntoNotes annotates\\ndocuments at several layers: syntax, propositions, word senses including nominal-\\nizations and eventive noun senses, named entities, ontology linking and co-reference.\\nIt has been designed to be a well-annotated large-scale corpus from which machine\\nlearning programs can learn many diﬀerent aspects of meaning felicitously.\\nOntoNotes uses Penn TreeBank parses [Marcus et al. 1993], PropBank propo-\\nsitional structures [Kingsbury and Palmer 2002; 2003; Palmer et al. 2005] on top\\nof Penn Treebank, and uses the Omega ontology [Philpot et al. 2005] for word\\nsense disambiguation. As we know, the Penn Treebank is annotated with informa-\\ntion from which one can extract predicate-argument structures. The developers of\\nOntoNotes use a parser that recovers these annotations [Gabbard et al. 2006]. The\\nPenn Treebank also has markers for “empty” categories that represent displaced\\nconstituents. Thus, to create OntoNotes, its developers use another parser [Collins\\n1999; 2003] to extract function words. They also use a maximum entropy learner\\nand voted preceptons to recover empty categories. PropBank, as we know, anno-\\ntates the one-million word Wall Street Journal part of the Penn Treebank with\\nsemantic argument structures for verbs. As we have noted earlier, the creators of\\nOntoNote and others have discovered that WordNet’s very ﬁne grained sense dis-\\ntinctions make inter-annotator agreement or good tagging performance diﬃcult. To\\nachieve better performance, OntoNotes uses a method [Palmer et al. 2004; Palmer\\net al. 2007] for sense inventory creation and annotation that includes links between\\ngrouped word senses and the Omega ontology [Philpot et al. 2005]. OntoNotes\\nrepresents sense distinctions in a hierarchical structure, like a decision tree, where\\ncoarse-grained distinctions are made at the root and increasingly ﬁne-grained re-\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 21}), Document(page_content='...· 23\\nstrictions until reaching WordNet senses at the leaves. Sets of senses under speciﬁc\\nnodes of the tree are grouped together into single entries, along with syntactic and\\nsemantic criteria for their groupings; these are presented to annotators for improved\\nannotation agreement, obtaining up to 90% inter-annotator agreement. OntoNote\\nfollows a similar method for annotation of nouns.\\nTo allow access to additional information such as subsumption, property inheri-\\ntance, predicate frames from other sources, links to instances and so on, OntoNotes\\nalso links to an ontology. This requires decomposing the hierarchical structure of\\nOntoNotes into subtrees which then can be inserted at the appropriate concep-\\ntual node in the ontology. OntoNotes represents its terms in the Omega ontology\\n[Philpot et al. 2005]. Omega7has been assembled by merging a variety of sources\\nsuch as WordNet, Mikrokosmos [Mahesh et al. 1995], and a few upper ontologies\\nsuch as DOLCE [Gangemi et al. 2002], SUMO [Niles and Pease 2001], and Penman\\nUpper Model [Hovy 2003]. OntoNote also includes and cross-references verb frames\\nfrom PropBank, FrameNet, WordNet and Lexical Conceptual Structures [Habash\\nand Dorr 2002]. OntoNotes also has coreferences. It connects coreferring instances\\nof speciﬁc referring expressions, primarily NPs that introduce or access a discourse\\nentity.\\nFor the purpose of our paper, it is important to know that OntoNotes tries\\nto annotate nouns that carry predicate structure, e.g., those whose structure is\\nderived from their verbal form. In particular, OntoNotes annotates nominalization\\nand eventive senses of nouns. OntoNotes applies two strict criteria for identifying\\na sense of a noun as a nominalization [Weischedel 2011].\\n—The noun must relate transparently to a verb, and typically display a nominaliz-\\ning morpheme such as -ment (govern/government) ,-ion (contribute/contribution) ,\\nthough it allows some zero-derived nouns such as kill, the noun derived from kill,\\nthe verb.\\n—The noun must be able to be used in a clausal noun phrase, with its core verbal\\narguments related by semantically empty or very “light” licensers, such as geni-\\ntive markers (as in The Roman’s destruction of the city.. or with the verb’s usual\\nparticle or prepositional satellites as in John’s longing for fame and fortune...\\nJust like nominalization senses, OntoNotes has strict deﬁnition of eventive senses.\\nThey have two deﬁnitional criteria (1) and (2), and a diagnostic test (3), for deter-\\nmining if a noun sense is eventive.\\n(1) Activity causing a change of state: A noun sense is eventive when it refers to a\\nsingle unbroken activity or process, occurring during a speciﬁc time period, that\\neﬀects a change in the world of discourse.\\n(2) Reference to activity proper: The noun must refer to the actual activity or\\nprocess, not merely to the result of the activity or the process.\\n(3) The noun patterns with eventive predicates in the “have” test: [Belvin 1993]\\ndescribes the following heuristic lexico-syntactic diagnostic test to apply to many\\nnouns. The test has four parts to it as discussed brieﬂy below.\\n7http://omega.isi.edu\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 22}), Document(page_content='24· ...\\n—Create a natural sounding sentence using the construction X had <NP>where\\n<NP>is a noun phrase headed by the noun in question, e.g., John had a party .\\n—Check if the sentence can be used in present progressive as in John is having\\na party. If the sentence is felicitous, it adds to the noun being inventive. If it\\nsounds odd, it adds to the evidence that the noun is stative.\\n—Check if the sentence can be used in a pseudo-cleft construction such as What\\nJohn did was have a party. If it is felicitous, the noun is more likely to be\\neventive. If not, it is more likely to be stative.\\n—Check if the sentence suggests iterative or habitual action using the simple\\npresent such as John has a party every Friday . If so, it adds evidence that the\\nnoun is eventive. If the sentence suggests that the situation is taking place at\\nthat very moment that it is uttered, it adds evidence that the noun is stative\\nas in John has a cold .\\nIn addition to OntoNotes, there have been other eﬀorts at obtaining large-scale\\nannotated corpora such at the GLARF project [Meyers et al. 2001] that tries to\\ncapture information from various Treebanks and superimpose a predicate argument\\nstructure. The Uniﬁed Linguistic Annotation (ULA) project [Pustejovsky et al.\\n2005] is a collaborative eﬀort that aims to merge PropBank, NomBank, the Penn\\nDiscourse Treebank [Prasad et al. 2008] and TimeBank [Pustejovsky et al. 2003]\\nwith co-reference information.\\n4. EXTRACTING EVENTS FROM TEXTUAL DOCUMENTS\\nDiﬀerent models of events have been used in computational linguistics work geared\\ntoward information extraction.\\n—A type of model that has been carefully developed over many years, treats an\\nevent as a word that points to a node in a network of predominantly temporal\\nrelations. An example is the so-called TimeML event that is found in documents\\nthat are annotated using the TimeML guidelines [Pustejovsky et al. 2003; Sauri\\net al. 2005; Saur´ ı et al. 2005]. In a TimeML annotated corpus, every event is\\nannotated. Thus, when working with the TimeML model, an extraction program\\nattempts to extract every event.\\n—A second type of event model enumerates a few types of events to be extracted\\n(where the types selected in a somewhat ad-hoc manner) with an event being\\ndescribed or pointed to by one or more words, along with additional associated\\nwords or phrases that specify arguments of the event. An example is the ACE\\nmodel of events [], where an event is a complex structure with arguments which\\nthemselves may be complex structures. Event extraction in the context of MUC-7\\n(are there other related MUCs?) or ACE, requires one to extract only a limited\\nnumber of event types, e.g., movement event type, conﬂict event type, justice\\nevent type (see Section... below). The structure for these event types is provided\\nby the contest organizers.\\n—Authors have used very specialized deﬁnitions of events when working with\\nbiomedical text, with details of a few specialized types of biomedical events,\\nalong with the participants.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 23}), Document(page_content='...· 25\\n—When detecting events in informal very short text such as microblogs of Facebook\\nposts, researchers have used deﬁnitions that focus on extracting events from many\\nshort documents over which an event may be described.\\nWe discuss TimeML events next followed by events. We discuss biomedical event\\nextraction in Section 0 ??, and extraction of events from Twitter in Section 0 ??.\\n4.1 TimeML Events\\nTimeML is a rich speciﬁcation language for event and temporal expressions in\\nnatural language text. In the TimeML [Pustejovsky et al. 2003; Sauri et al. 2005]\\nannotation scheme, an event is a general term for situations that happen oroccur .\\nEvents can be punctual or momentary, or last for a period of time. Events in\\nTimeML format may also include predicates describing states orcircumstances in\\nwhich something holds true. Only those states that participate in an opposition\\nstructure, as discussed in Subsection 2.4.3, are annotated. In general, an event can\\nbe expressed in terms of verbs, nominalizations, adjectives, predicative clauses, or\\nprepositional phrases. TimeML allows an event, annotated with the EVENT tag,\\nto be one of seven types: occurrence, state, report, i-action, i-state, aspectual and\\nperception. The ﬁrst ﬁve are special cases. The last two, Occurrence and State are\\nused for general cases that do not fall in the special ones.\\n—Reporting: A reporting event describes an action declaring something, narrating\\nan event, informing about a situation, and so on. Some verbs which express this\\nkind of event are say,report ,tell,explain , and state. An example sentence with\\nthe verb sayisPunongbayan said that the 4,795-foot-high volcano was spewing\\ngases up to 1,800 degrees.\\n—I-Action: Istands for intensional8. According to the TimeML annotation guide-\\nlines, an i-action is a dynamic event that takes an event-denoting argument,\\nwhich must be explicitly present in the text. Examples of verbs that are used to\\nexpress i-actions include attempt ,try,promise and oﬀer. An example sentence\\nwith the verb tryisCompanies such as Microsoft or a combined worldcom MCI\\naretrying to monopolize Internet access.\\n—I-State: I-State stands for intensional state . Like an I-Action, an I-State event\\ntakes an argument that expresses an event. Unlike an I-Action, the I-State class\\nis used for events which are states. An example sentence that uses the verb\\nbelieve isWebelieve that his words cannot distract the world from the facts of\\nIraqi aggression . Other verbs used to express i-states include intend ,want, and\\nthink .\\n—Aspectual: An aspectual predicate takes an event as an argument, and points to\\na part of the temporal structure of the event. Such a part may be the beginning,\\nthe middle or the end of an event. Verbs such as begin ,ﬁnish and continue\\nare such aspectual predicates. An example sentence with the verb begin isAll\\nnon-essential personnel should begin evacuating the sprawling base.\\n8According to the English Wikipedia: In logic and mathematics, an intensional deﬁnition gives\\nthe meaning of a term by specifying all the properties required to come to that deﬁnition, that is,\\nthe necessary and suﬃcient conditions for belonging to the set being deﬁned.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 24}), Document(page_content='26· ...\\n—Perception: This class includes events involving the physical perception of an-\\nother event. Such events are typically expressed by verbs such as see,watch ,\\nglimpse ,hear,listen , and overhear . An example sentence with the verb seeis\\nWitnesses tell Birmingham police they saw a man running.\\n—Occurrence: An occurrence is a general event that occurs or happens in the\\nworld. An example of an occurrence is given in the following sentence: The\\nDefense Ministry said 16 planes have landed so far with protective equipment\\nagainst biological and chemical warfare. The occurrence has been highlighted in\\nbold.\\n—State: A state describes circumstances in which something obtains or holds true.\\nAn example sentence that shows two states is It is the US economic and political\\nembargo which has kept Cuba in a box.\\nTimeML allows one to mark up temporal expressions using the TIMEX3 tag.\\nTemporal expressions are of three types: (a) Fully speciﬁed temporal expressions\\nsuch as June 11, 2013 , (b) Underspeciﬁed temporal expressions such as Monday , (c)\\nDurations such as three days . TimeML uses the SIGNAL tag to annotate sections\\nof text, usually function words, that indicate how temporal objects are related\\nto each other. The material marked by SIGNAL may contain diﬀerent types of\\nlinguistic elements: indicators of temporal relations such as prepositions such as on\\nandduring , other temporal connectives such as when , etc. The TIMEX3 andSIGNAL\\ntags were introduced by [Setzer and Gaizauskas 2000; Setzer 2001].\\nA major innovation of TimeML is the LINK tags that encode relations between\\ntemporal elements of a document and also help establish ordering between the\\nevents in a document. There are three types of links: TLINK showing temporal\\nrelationships between events, or between an event and a time; SLINK or a subor-\\ndination link to show context that introduces relations between two events, or an\\nevent and a signal; ALINK or an aspectual link to show relationship between an\\naspectual event and its argument event. TLINK allows for 13 temporal relations\\nintroduced by [Allen 1983; 1984]. SLINK is used to express contexts such as use\\nof modal verbs, negatives, positive and negative evidential relations, factives which\\nrequire the event argument to be true, and counterfactives which require the event\\nargument to be false. ALINK expresses initiation, culmination, termination or con-\\ntinuation relationships between an event and its argument event. Finally, TimeML\\nis able to express three types of causal relations: an event causing an event, an\\nentity causing an event, and the special situation where the use of the discourse\\nmarker andas a signal to introduce a TLINK indicating that one event happened\\nbefore another as in He kicked the ball and it rose into the air .\\nThe creators of TimeML have spent signiﬁcant eﬀorts to develop a fairly large\\ncorpus annotated with TimeML tags. This corpus is called the TIMEBANK corpus\\n[Pustejovsky et al. 2003] and has 300 annotated articles. This corpus has been used\\nto learn to extract events and temporal relations among events.\\n4.2 ACE Events\\nIn the ACE model, only “interesting” events are annotated in corpora and thus\\nextracted by a trained program. ACE annotators specify the event types they want\\nto be extracted. For example, in one information extraction contest, an ACE 2005\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 25}), Document(page_content='...· 27\\nevent9was of 8 types, each with one has one or more sub-types. The types are\\ngiven below. ***Maybe, give some examples***\\n—Life: Be-born, marry, divorce, injure and die\\n—Movement : Transport\\n—Transaction : Transfer-ownership, Transfer money\\n—Business : Start-organization, Merge-organization, Declare-bankruptcy\\n—Contact : Meet, Phone-write\\n—Conﬂict : Attack, demonstrate\\n—Personnel : Start position, End position, Nominate, Elect, and\\n—Justice : Arrest-Jail, Release-Parole, Trial-Hearing, Charge-Indict, Sue, Convict,\\nSentence, Fine, Execute, Extradite, Acquit, Appeal, Pardon.\\nEach event also has four categorial attributes. The attributes and their values are\\ngiven below.\\n—Modality : Asserted and Other where Other includes, but is not limited to: Be-\\nlieved events; Hypothetical events; Commanded and requested events; Threat-\\nened, Proposed and Discussed events; and Promised events.\\n—Polarity : Positive and Negative.\\n—Genericity : Speciﬁc, Generic\\n—Tense : Past, Present, Future and Unspeciﬁed.\\nACE events have arguments. Each event type has a set of possible argument roles,\\nwhich may be ﬁlled by entities, time expressions or other values. Each event type\\nhas a set of possible argument roles. There are a total of 35 role types although no\\nsingle event can have all 35 roles. A complete description of which roles go with\\nwhich event type can be found in the annotation guidelines for ACE 2005 events10.\\nIn an ACE event, time is noted if when explicitly given.\\nOthers have deﬁned events or event proﬁles themselves to suit their purpose.\\nFor example, Cybulska and Vossen [Cybulska and Vossen 2010; 2011] describe an\\nhistorical information extraction system where they extract event and participant\\ninformation from Dutch historical archives. They extract information using what\\nthey call proﬁles. For example, they have developed 402 proﬁles for event extrac-\\ntion although they use only 22 of them in the reported system. For extraction of\\nparticipants, they use 314 proﬁles. They also 43 temporal proﬁles and 23 location\\nproﬁles to extract temporal and locational information. Proﬁles are created using\\nsemantic and syntactic information as well as information gleaned from Wordnet\\n[Miller 1995].\\n4.2.1 Additional Annotation Schemes.\\n9http://projects.ldc.upenn.edu/ace/docs/English-Events-Guidelines_v5.4.3.pdf\\n10http://projects.ldc.upenn.edu/ace/docs/English-Events-Guidelines_v5.4.3.pdf\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 26}), Document(page_content='28· ...\\n4.2.1.1 ERE Annotations. The ACE annotation scheme, discussed earlier, was\\ndeveloped by NIST in 1999, and the ERE (Entities, Relations and Events) scheme\\nwas deﬁned as a simpler version of ACE [Aguilar et al. 2014]. One of ERE’s goals is\\nalso to make annotating easier and annotations more consistent across annotators.\\nERE attempts to achieve these goals by removing the most problematic annotations\\nin ACE and consolidating others. We will discuss the three types annotations now:\\nEntities, Relations and Events.\\nFor example, consider Entities. ACE and ERE both have Person, Organization,\\nGeo-Political Entity and Location as types of entities. ACE has two additional\\ntypes, Weapon and Vehicle, which ERE does not have. ERE doesn’t distinguish\\nbetween Facility and Location types and merge them into Location. ERE has a\\ntype called Title for titles, honoriﬁcs, roles and professions. ACE has subtypes\\nfor entity mentions, which ERE does not. In addition to subtypes, ACE classiﬁes\\nentity mentions into classes (e.g., Speciﬁc, Generic and Underspeciﬁed), ERE has\\nonly Speciﬁc. ACE and ERE also have diﬀerences in how extents and heads are\\nmarked, and levels of entity mentions.\\nThe purpose of Relation annotation in both ACE and ERE is to extract a repre-\\nsentation of the meaning of the text, not necessarily tied to the underlying syntactic\\nor lexical representation. Both schemes include Physical, Part-Whole, Aﬃliation\\nand Social relations although the details are a bit diﬀerent. Both tag relations inside\\na single sentence and tags only explicit mentions. Nesting of tags is not allowed.\\nEach relation can have up to two ordered Argument slots. Neither model tags nega-\\ntive relations. However, ERE annotates only asserted (”real”) events whereas ACE\\nallows others as well, e.g., Believed Events, Hypothetical Events, Desired Events\\nand Requested Events. There is no explicit trigger word in ACE, which annotates\\nthe full clause that serves as the trigger for a relation whereas ERE attempts to\\nminimize the annotated span by allowing for the tagging of an optional trigger word\\nor phrase. ACE justiﬁes tagging of each Relation by assigning Syntactic Clauses to\\nthem, such as Possessive, PreMod and Coordination. The three types of Relations\\ninn ERE and ACE have sub-types: Physical, Part-Whole, and Social and Aﬃli-\\nation, but ERE collapses ACE types and sub-types to make them more concise,\\npossibly less speciﬁc. [Aguilar et al. 2014] discuss the similarities and diﬀerences\\nbetween ACE and ERE in detail.\\nEvents in both ACE and ERE are deﬁned as ‘speciﬁc occurrences’ involving\\n‘speciﬁc participants’. Like entities and relations, ERE is less speciﬁc and simpliﬁed\\ncompared to ACE. Both annotation schemes annotate the same event types: Life,\\nMovement, Transaction, Business, Conﬂict, Contact, Personnel, and Justice.\\n4.2.1.2 RED Annotations. [Ikuta et al. 2014] use another annotation scheme\\ncalled Richer Event Description (RED), synthesizing co-reference [Pradhan et al.\\n2007; Lee et al. 2012] and THYME-TimeML temporal relations [Styler IV et al.\\n2014]. [Ikuta et al. 2014] discusses challenges in annotating documents with the\\nRED schema, in particular cause-eﬀect relations. The usual way to annotate cause-\\neﬀect relations is using the counter-factual deﬁnition of causation in philosophy\\n[Lewis 1973; Halpern and Pearl 2005]:\\n“X causes Y” means if X had not occurred, Y would not have happened.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 27}), Document(page_content='...· 29\\nHowever, [Ikuta et al. 2014] found that this deﬁnition leads to many diﬃcult and\\nsometimes erroneous annotations, and that’s why while performing RED annota-\\ntions, they used another deﬁnition [Menzies 1999; 2008] which treats causation as\\n“a local relation depending on intrinsic properties of the events and what goes on\\nbetween then, and nothing else”. In particular, the deﬁnition is\\n“X causes Y” means Y was inevitable given X.\\nIn fact, in the annotations performed by [Ikuta et al. 2014], they use the new\\ndeﬁnition to make judgements, but use the old deﬁnition as a precondition to the\\nnew one.\\n4.2.2 TAC-KBP Annotations. The Knowledge Base Population Track (TAC-\\nBKP) was started by NIST in 2009 to evaluate knowledge bases (KBs) created\\nfrom the output of information extraction systems. The primary tasks are a) En-\\ntity linking–linking extracted entities to entities in knowledge bases, and b) Slot\\nﬁlling–adding information to entity proﬁles, information that is missing from the\\nknowledge base [McNamee et al. 2010]. Wikipedia articles have been used as refer-\\nence knowledge bases in evaluating TAC-KBP tasks. For example, given an entity,\\nthe goal is to identify individual nuggets of information using a ﬁxed list of in-\\nventory relations and attributes. For example, given a celebrity name, the task is\\nto identify attributes such as schools attended, occupations, important jobs held,\\nnames of immediate family members, etc., and then insert them into the knowledge\\nbase. Many people compare slot ﬁlling to answering a ﬁxed set of questions, ob-\\ntaining the answers and ﬁlling in the appropriate slots in the knowledge base. Slot\\nﬁlling in TAC-KBP diﬀers from extraction in ACE and ERE notations in several\\nways such as TAC-KBP seeks out information for named entities only, chieﬂy PERs\\nand ORGs, TAC-KBP seeks to obtain values for slots and not mentions, and events\\nare handled as uncorrelated slots, and assessment is like in question-answering.\\nOur focus on this paper has been on extracting events, and we know that to\\nextract events properly, we need to explicitly extract event mentions, and also\\nextract associated attributes such as agents, locations, time of occurrence, duration,\\netc. Rather than explicitly modeling events, TAC-KBP does so implicitly as it\\ncaptures various relations associated with for example the agent of the event. For\\nexample, given a sentence “Jobs is the founder and CEO of Apple”, TAC-KBP\\nmay pick ”Apple” as the focal entity and identify ”Jobs” as the ﬁller of its founder\\nslot, and ”Jobs” as the ﬁller of its CEO slot. However, an ACE or ERE annotation\\nprogram will ideally pick the event as Founding, with Jobs as an argument (say\\nthe ﬁrst argument or arg1, or the Actor) of the event, and ”Apple” as another\\nargument, say arg2.\\n4.3 Extracting Events\\nMany even extraction systems have been built over the years. A big motivator\\nfor development of event extraction systems seem to be various contests that are\\nheld every few years, although there has been considerable amount of non-contest\\nrelated research as well. Although we discuss extraction of events represented by\\nvarious formats, the methods are not really diﬀerent from each other. That is why\\nwe discuss TimeML events in more detail and present the others brieﬂy in this\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 28}), Document(page_content='30· ...\\nsection.\\n4.3.1 Extracting TimeML Events. We describe a few of the approaches that\\nhave been used for extracting TimeML type events. Quite a few papers that at-\\ntempt to do so have been published [Saur´ ı et al. 2005; Bethard and Martin 2006;\\nChambers et al. 2007; Llorens et al. 2010; Grover et al. 2010], and we pick just a\\nfew representative papers.\\n4.3.1.1 The Evita System. : [Saur´ ı et al. 2005] implemented an event and event\\nfeature extraction system called EVITA and showed that a linguistically motivated\\nrule-based system, with some help using statistical disambiguation perfumed well\\non this task. Evita is claimed to be a unique tool within the TimeML framework in\\nthat it is very general, being not based on any pre-established list of event patterns\\nand being domain-independent. Evita can also identify, based on linguistic cues,\\ngrammatical information associated with event referring expressions, such as tense,\\naspect, polarity and modality, as stated in the TimeML speciﬁcation. Evita does\\nnot directly identify event participants, but can work with named entity taggers to\\nlink arguments to events.\\nEvita breaks down the event recognition problem to a number of sub-tasks. Evita\\npreprocesses the input text using the Alembic Workbench POS tagger, lemmatizer\\nto ﬁnd lexical stems, and chunkier to obtain phrase chunks, verbal, nominal and\\nadjectival, the three that are commonly used as event referring expressions [ ?]. For\\neach subtask after pre-processing, it combines linguistic- and statistically-based\\nknowledge. Linguistic knowledge is used in local and limited contexts such as verb\\nphrases and to extract morphological information. Statistical knowledge is used\\nto disambiguate nominal events. The sub-tasks in event recognition in Evita are:\\ndetermination of event candidates and then the events, identiﬁcation of grammatical\\nfeatures of events, additional clustering of event chunks for event detection and\\ngrammatical feature identiﬁcation in some situations.\\nFor event identiﬁcation, Evita looks at the lexical items tagged by the prepro-\\ncessing step. It uses diﬀerent strategies for identifying events in the three cate-\\ngories: verbs, nouns and adjectives. For identifying events in a verbal chunk, Evita\\nperforms lexical look-up and limited contextual parsing in order to exclude weak\\nstative predicates such as beand generics such as verbs with bare plural subjects.\\nIdentifying events expressed by nouns involves a phase of lexical look-up and dis-\\nambiguation using WordNet, and by mapping events SemCor and TimeBank 1.2 to\\nWordNet synsets. Evita consults 25 subtrees from WordNet where all the synsets\\ndenote events. One of these, the largest, is the tree underneath the sunset that\\ncontains the word event . If the result of this lexical look-up is not conclusive (i.e.,\\nif a nominal occurs as both event and non-event in WordNet), a disambiguation\\nstep is applied, based on rules learned by a Bayesian classiﬁer trained on SemCor.\\nTo identify events from adjectives, Evita uses a conservative approach, where it\\ntags only those adjectives that were annotated as such in TimeBank 1.2, when such\\nadjectives occur as the head of a predicative complement.\\nTo identify grammatical features (e.g., tense, aspect, modality, polarity and non-\\nﬁnite morphology) of events, Evita uses diﬀerent procedures based on the part of\\nspeech of the event denoting expression. But, in general it involves using morphol-\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 29}), Document(page_content='...· 31\\nogy, pattern matching, and applying a large number (e.g., 140 such rules for verbal\\nchunks) simple linguistic rules. However, to identify the event class, it performs lex-\\nical look-up and word sense disambiguation. Clustering is used to identify chunks\\nfrom the preprocessing stage, that contribute information about the same event,\\ne.g., when some modal auxiliaries and use of copular verbs. Clustering is activated\\nby speciﬁc triggers such as the presence of a chunk headed by an auxiliary verb or\\na copular verb.\\nEvaluation of Evita was performed by comparing its performance against Time-\\nBanck 1.2. The reported performance was that Evita had 74.03% precision, 87.31%\\nrecall and an F-measure of 80.12% in event detection. Accuracy (precision?) for\\npolarity, aspect and modality was over 97% in each case.\\n4.3.1.2 Bethard and Martin’s approach (2006). : [Bethard and Martin 2006] use\\nTimeBank-annotated events and identify which words and phrases are events. They\\nconsider event identiﬁcation as a classiﬁcation task that works on word-chunks.\\nThey use the BIO formulation that augments each class label with whether the\\nword is the Beginning, Inside or Outside of a chunk [Ramshaw and Marcus 1995].\\nThey use a number of features, categorized into various classes, for machine learn-\\ning. These include aﬃx features (e.g., three or four characters from the beginning\\nand end of each word), morphological features (e.g., base form of the word, and\\nbase form of any verb associated with the word if the word is a noun or gerund,\\nfor example), word-class features (e.g., POS tags, which noun or verb cluster a\\nword belongs to where the clusters are obtained using co-occurrence statistics in\\nthe manner of [Pradhan et al. 2004]), governing features (e.g., governing light verb,\\ndeterminer type—cardinal or genitive, for example), and temporal features (e.g.,\\na BIO label indicating whether the word is contained inside a TIMEX2 temporal\\nannotation, a governing temporal preposition like since, till, before, etc.). They\\nalso use negation features and Wordnet hypernyms as features. For classiﬁcation,\\nthey use the TinySVM implementation of SVM by [Kudo and Matsumoto 2001].\\nThey perform experiments with TimeBank documents using a 90% stratiﬁed sam-\\npling for training and 10% for testing. They obtained 82% precision and 71% recall,\\nwith an F-measure of 0.759. They did compare their algorithm with an version of\\nEvita they programmed themselves; this system obtained 0.727 F-measure, and\\nthus Bethard and Martin’s approached performed about 4% better. When Bethard\\nand Martin’s system was extended to identifying semantic class of an event, it did\\nnot perform as well, obtaining precision of 67%, recall of 51%, and F-measure of\\n0.317. However, the system was much better at identifying the classes of verbs\\nwith F-measure of 0.707 compared to ﬁnding classes of nouns with an F-measure\\nof 0.337 only.\\n4.3.1.3 Llorens’ et al.’s approach:. TIPSem (Temporal Information Processing\\nbased on Semantic information) is a system that participated in the TemEval-\\n2 Competition [Verhagen et al. 2010] in 2010, which presented several tasks to\\nparticipants, although we are primarily interested in the event extraction task.\\nTIPSem achieved the best F1 score in all the tasks in TempEval-2 for Spanish, and\\nfor English it obtained the best F1 metric in the task of extracting events, which\\nrequired the recognition and classiﬁcation of events as deﬁned by TimeML EVENT\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 30}), Document(page_content='32· ...\\ntag.\\nTIPSem learns Conditional Random Field (CRF) models using features for dif-\\nferent language analysis levels, although the approach focuses on semantic informa-\\ntion, primarily semantic roles and semantic networks. Conditional Random Fields\\npresent a popular and eﬃcient machine learning technique for supervised sequence\\nlabeling [Laﬀerty et al. 2001].\\nThe features used for training the CRF models are similar to one used by others\\nsuch as Bethard and Martin, although details vary. However, they add semantic\\nrole labels to the mix of features. In particular, they identify roles for each gov-\\nerning verb. Semantic role labeling [Gildea and Jurafsky 2002; Moreda et al. 2007;\\nPunyakanok et al. 2004] identiﬁes for each predicate in a sentence, semantic roles\\nand determine their arguments (agent, patient, etc.) and their adjuncts (locative,\\ntemporal, etc.). The previous two features were combined in TIPSem to capture the\\nrelation between them. The authors think this combination introduces additional\\ninformation by distinguishing roles that are dependent on diﬀerent verbs. The\\nimportance of this falls especially on the numbered roles (A0, A1, etc.) meaning\\ndiﬀerent things when depending on diﬀerent verbs.\\nThe test corpus consists of 17K words for English and 10K words for Spanish,\\nprovided by the organizers of TempEval-2. For English, they obtained precision of\\n0.81, recall of 0.86 and F-measure of 0.83 for recognition with event classiﬁcation\\naccuracy of 0.79; for Spanish the numbers were 0.90, 0.86, 0.88 for recognition and\\n0.66 for classiﬁcation accuracy. We provide these numbers although we know that it\\nis diﬃcult to compare one system with another, for example Bethard and Martin’s\\nsystem with TIPSem since the corpora used are diﬀerence.\\n4.3.1.4 TempEval-3: 2012. As in TempEval-2, TempEval-3 [UzZaman et al.\\n2012] participants took part in a task where they had to determine the extent of\\nthe events in a text as deﬁned by the TimeML EVENT tag. In addition, systems\\nmay determine the value of the features CLASS, TENSE, ASPECT, POLARITY,\\nMODALITY and also identify if the event is a main event or not. The main\\nattribute to annotate is CLASS.\\nThe TempEval-3 dataset was mostly automatically generated, using a temporal\\nmerging system. The half-million token text corpus from English Gigaword2 was\\nautomatically annotated using TIPSem, TIPSem-B [Llorens et al. 2010] and TRIOS\\n[UzZaman and Allen 2010]. These systems were re-trained on the TimeBank and\\nAQUAINT corpus, using the TimeML temporal relation set. The outputs of these\\nthree state-of-the-art system were merged using a merging algorithm [UzZaman\\net al. 2012]. The dataset used comprised about 500K tokens of “silver” standard\\ndata and about 100K tokens of “gold” standard data for training, compared to the\\ncorpus of roughly 50K tokens corpus used in TempEval 1 and 2.\\nThere were seven participants and all the participants except one used machine\\nlearning approaches. The top performing system was ATT-1 [Jung and Stent 2013]\\nwith precision 81.44, recall 80;67 and F1 of 81.05 for event recognition, and 71.88 for\\nevent classiﬁcation. Close behind was the ATT-2 system [Jung and Stent 2013] with\\nprecision, recall and F-1 of 81.02, 80.81 and 80.92 for event recognition respectively,\\nand 71.10 for event classiﬁcation. Both systems used MaxEnt classiﬁers with\\nObviously, diﬀerent sets of features impact on the performance of event recog-\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 31}), Document(page_content='...· 33\\nnition and classiﬁcation [Adafre and de Rijke 2005; Angeli et al. 2012; Rigo and\\nLavelli 2011]. In particular, [ ?] also examined performance based on diﬀerent\\nsizes of n-grams in a small scale (n=1,3). Inspired by such work, in building the\\nATT systems, the creators intended to systematically investigate the performance\\nof various models and for each task, they trained twelve models exploring these\\ntwo dimensions, three of which we submitted for TempEval-3, and of these three\\nperformed among the top ten in TempEval-3 Competition.\\nThe ATT-1 models include lexical, syntactic and semantic features, ATT-2 mod-\\nels include only lexical and syntactic features, and ATT-3 models include only\\nlexical features, i.e., words. They experimented with context windows of 0, 1, 3,\\nand 7 words preceding and following the token to be labeled. For each window\\nsize, they trained ATT-1, ATT-2 and ATT-3 models. The ATT-1 models had 18\\nbasic features per token in the context window for up to 15 tokens, so up to 270\\nbasic feaures for each token to be labeled. The ATT-2 models had 16 basic features\\nper token in the context window, so up to 240 basic features for each token to\\nbe labeled. The ATT-3 models had just 1 basic feature per token in the context\\nwindow, so up to 15 basic features for each token to be labeled.\\nFor event extraction and classiﬁcation, and event feature classiﬁcation, they used\\nthe eﬃcient binary MaxEnt classiﬁer for multi-class classiﬁcation, available in the\\nmachine learning toolkit LLAMA [Haﬀner 2006]. They also used LLAMA’s pre-\\nprocessor to build unigram, bigram and trigram extended features from basic fea-\\ntures.\\nFor event and time expression extraction, they trained BIO classiﬁers. It was\\nfound that the absence of semantic features causes only small changes in F1. The\\nabsence of syntactic features causes F1 to drop slightly (less than 2.5% for all but the\\nsmallest window size), with recall decreasing while precision improves somewhat.\\nF1 is also impacted minimally by the absence of semantic features, and about 2-5%\\nby the absence of syntactic features for all but the smallest window size.1\\nA was surprising that that ATT-3 models that use words only performed well,\\nespecially in terms of precision (precision, recall and F2 of 81.95, 75.57 and 78.63\\nfor event recognition, and 69.55 F1 for event classiﬁcation) . It is also surprising\\nthat the words only models with window sizes of 3 and 7 performed as well as the\\nmodels with a window size of 15. These results are promising for “big data text\\nanalytics, where there may not be time to do heavy preprocessing of input text or\\nto train large models.\\n4.3.2 Extracting Events Using Other Representations. We have already dis-\\ncussed several approaches to extraction of events represented by TimeML repre-\\nsentation. Extracting events that use other representation is not very diﬀerent, but\\ndiﬀerent representations have existed and exist, and therefore we brieﬂy present\\nsome such attempts. Some of these predate the time TimeML became popular. For\\nexample, the various Message Understanding Conferences (MUCs, seven were orga-\\nnize by DARPA from 1987 to 1997), asked participants to extract a small number\\nof relations and events. For instance, MUC-7, the last one, called for the extraction\\nof 3 relations (person-employer, maker-product, and organization-location) and 1\\nevent spacecraft launches .\\nFor example, the MUC-7 and ACE events did not attempt to cover all events,\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 32}), Document(page_content='34· ...\\nbut a limited number of pre-speciﬁed event types or classes that participants need\\nto detect during a contest period, based on which the contestants submit papers\\nfor publication. The number and the type of arguments covered are also limited\\nand are pre-speciﬁed before the competitions start.\\n4.3.2.1 The REES System:. [Aone and Ramos-Santacruz 2000] discuss a rela-\\ntion and event extraction system covering areas such as political, ﬁnancial, busi-\\nness, military, and life-related topics. The system consists of tagging modules, a\\nco-reference resolution module, and a temple generation module. They store the\\nevents generated in MUC-7[Chinchor and Marsh 1998] format, which is not very\\nunlike the ACE format.\\nEvents are extracted along with their event participants, e.g., who did what to\\nwhom when and where? For example, for a BUYING event, REES extracts the\\nbuyer, the artifact, the seller, and the time and location of the BUYING event.\\nREES covers 61 types of events. There are 39 types of relations.\\nThe tagging component consists of three modules: NameTagger, NPTagger and\\nEventTagger. Each module relies on the same pattern-based extraction engine,\\nbut uses diﬀerent sets of patterns. The NameTagger recognizes names of peo-\\nple, organizations, places, and artifacts (only vehicles in the implemented system).\\nThe NPTagger then takes the output of the NameTagger and ﬁrst recognizes non-\\nrecursive Base Noun Phrase (BNP) [Ramshaw and Marcus 1995], and then complex\\nNPs for only the four main semantic types of NPs, i.e., Person, Organization, Loca-\\ntion, and Artifact (vehicle, drug and weapon). The EventTagger recognizes events\\napplying its lexicon-driven, syntactically-based generic patterns.\\nREES uses a declarative, lexicon-driven approach. This approach requires a\\nlexicon entry for each event-denoting word, which is generally a verb. The lexicon\\nentry speciﬁes the syntactic and semantic restrictions on the verb’s arguments.\\nAfter the tagging phase, REES sends the output through a rule-based co-reference\\nresolution module that resolves: deﬁnite noun phrases of Organization, Person,\\nand Location types, and singular personal pronouns. REES outputs the extracted\\ninformation in the form of either MUC-style templates or XML.\\nOne of the challenges of event extraction is to be able to recognize and merge\\nthose event descriptions which refer to the same event. The Template Generation\\nmodule uses a set of declarative, customizable rules to merge co- referring events\\ninto a single event.\\nThe system’s recall, precision, and F-Measure scores for the training set (200\\ntexts) and the blind set (208 texts) from about a dozen news sources. On the\\nso-called training set, the system achieved F-measure of 64.75 for event extraction\\nand 75.35 for relation extraction. The blind set F-Measure for 31 types of relations\\n(73.95\\n4.3.2.2 Ahn’s approach. As seen earlier in Subsection 4.2, the way ACE events\\nare speciﬁed, they have a lot of details that need to be extracted. [Ahn 2006]\\nfollows several steps to extract events and uses machine learning algorithms at\\nevery step. The steps are pre-processing of text data, identifying anchors, assigning\\nevent types, extracting arguments identifying attributes of events such as modality,\\npolarity, genericity and tense, and ﬁnally identifyings event co-referents of the same\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 33}), Document(page_content='...· 35\\nindividuated event. In other words, Ahn attempts to cover all the steps sequentially,\\nmaking the simplifying assumption that they are unrelated to each other.\\nA single place in a textual document which may be considered the primary place\\nof reference or discussion about an event is called the event anchor. Ahn treats\\nﬁnding the anchor for an event within a document as a word classiﬁcation task ,\\nusing a two-stage classiﬁcation process. He uses a binary classiﬁer to classify a\\nword as being an event anchor or not. He then classiﬁes those identiﬁed as event\\nanchors into one of the event classes. Ahn used one classiﬁer for binary classiﬁcation\\nand then another classiﬁers to classify only the positive instances.\\nAhn treats identifying event arguments as a pair classiﬁcation task. Each event\\nmention is paired with each of the entity, time and value mentions occurring in\\nthe same sentence to form a single classiﬁcation instance. There were 35 role types\\nin the ACE 2006 task, but no event type allows arguments of all types. Each\\nevent type had its own set of allowable roles. The classiﬁcation experiment run was\\na multi-class classiﬁcation where a separate multi-class classiﬁer was used for each\\nevent type. Ahn trains a separate classiﬁer for each attribute. Genericity, modality,\\nand polarity are each binary classiﬁcation tasks, while tense is a multi-class task.\\nFor event coreference, Ahn follows the approach given in [Florian et al. 2004]. Each\\nevent mention in a document is paired with every other event mention, and a\\nclassiﬁer assigns to each pair of mentions the probability that the paired mentions\\ncorefer. These probabilities are used in a left-to-right entity linking algorithm in\\nwhich each mention is compared with all already-established events (i.e., event\\nmention clusters) to determine whether it should be added to an existing event or\\nstart a new one.\\nAhn experimented with various combinations of a maximum entropy classiﬁer\\nMegaM [Daum´ e III 2004] and a memory-based nearest neighbor classiﬁer called\\nTIMBL [Daelemans et al. 2004], for the various tasks.\\nThe ACE speciﬁcation provided a way to measure the performance of an event\\nextraction system. The evaluation called ACE value is obtained by scoring each of\\nthe component tasks individually and then obtaining a normalized summary value.\\nOverall, using the best learned classiﬁers for the various subtasks, they achieve an\\nACE value score of 22.3%, where the maximum score is 100%. The value is low,\\nbut other systems at the time had comparable performance.\\n4.3.2.3 Naughton 2008:. [Naughton et al. 2008] describe an approach to classify\\nsentences in a document as specifying one or more events from a certain ACE 2006\\nclass. They classify each sentence in a document as containing an instance of\\na certain type or not. Unlike [Ahn 2006], they are not interested in identifying\\narguments or any additional processing. Also, unlike Ahn who classiﬁes each word\\nas possibly being an event anchor for a speciﬁc type of ACE event, Naughton\\net al. perform a classiﬁcation of each sentence in a document as being an on-\\nevent sentence or an oﬀ-event sentence . An on-event sentence is a sentence that\\ncontains one or more instances of the target event type. An oﬀ-event sentence is\\na sentence that does not contain any instances of the target event type. They use\\nseveral approaches to classify a sentence as on-event or oﬀ-event. These include\\nthe following: SVM-based machine learning [Joachims 1998], language modeling\\napproaches using count smoothing, and a manual approach which looks for Wordnet\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 34}), Document(page_content='36· ...\\nsynonyms or hypernyms of certain trigger words in a sentence.\\nNaughton et al. found that 1) use of a large number of features to start but then\\nreduction of these features using information gain, and 2) use of SVM produces the\\nbest results although all versions of SVM (i.e., with all features with no reduction,\\njust the terms without complex features, or a selection of terms and other features)\\nall work very well. A “surprising” ﬁnding was that the “manual” trigger-based\\nclassiﬁcation approach worked almost as well as the SVM based approaches.\\n4.4 Determining Event Coreference\\nWhen an event is mentioned in several places within a document, ﬁnding which\\nreferences are to the same event is called determining event coreference. These are\\nco-referents to the event. Determining when two event mentions in text talk about\\nthe same event or co-refer is a diﬃcult problem. As [Hovy et al. 2013] point out\\nthat the events may be actual occurrences or hypothetical events.\\n4.4.1 Florian’s Approach to Coreference Resolution. [Florian et al. 2004] present\\na statistical language-independent framework for identifying and tracking named,\\nnominal and pronominal references to entities within unrestricted text documents,\\nand chaining them into groups corresponding to each logical entity present in the\\ntext. The model can use arbitrary feature types, integrating a variety of lexical,\\nsyntactic and semantic features. The mention detection model also uses feature\\nstreams derived from diﬀerent named entity classiﬁers.\\nFor mention detection, the approach used is based on a log-linear Maximum En-\\ntropy classiﬁer [Berger et al. 1996] and a linear Robust Risk Minimization classiﬁer\\n[Zhang et al. 2002]. Then they use a MaxEnt model for predicting whether a men-\\ntion should or should not be linked to an existing entity, and to build entity chains.\\nBoth classiﬁers can integrate arbitrary types of information and are converted into\\nsuitable for sequence classiﬁcation for both tasks.\\nFor entity tracking, the process works from left to right. It starts with an initial\\nentity consisting of the ﬁrst mention of a document, and the next mention is pro-\\ncessed by either linking it with one of the existing entities, or starting a new entity.\\nAtomic features used by the entity linking algorithm include string match, context,\\nmention count, distance between the two mentions in words and sentences, editing\\ndistance, properties of pronouns such gender, number and reﬂexiveness. The best\\ncombination of features was able to obtain slightly more than 73% F-1 value using\\nboth RRM and MaxEnt algorithms for mention detection.\\nEntity tracking was evaluated in terms of what is called the ACE value. A gauge\\nof the performance of an EDT system is the ACE value, a measure developed\\nespecially for this purpose. It estimates the normalized weighted cost of detection\\nof speciﬁc-only entities in terms of misses, false alarms and substitution errors.\\nFlorian et al. achieved an ACE value of 73.4 out of 100 for the MaxEnt classiﬁer\\nand 69.7 for the RRM classiﬁer.\\n[Ahn 2006] follows the approach by [Florian et al. 2004] for entity coreference\\ndetermination. He uses a binary classiﬁer to determine if any two event mentions\\nin the document refer to the same event. Thus, he pairs each event with every other\\nevent, and the classiﬁer assigns each pair a probability that they are the same. The\\nprobability is used with entity linking/matching algorithm to determine event co-\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 35}), Document(page_content='...· 37\\nreference. Event co-referencing requires event mentions to be clustered to event\\nclusters. Event mentions in a cluster are the same event. The system described\\nhere obtained an ACE value of between 88-91%, where the maximum ACE value\\nis 100%.\\n[Ahn 2006] uses the following features for event co-reference determination. Let\\nthe candidate be the earlier event mention and the anaphor be the later mention.\\n—The anchors for the candidate and the anaphor, the full or original form, and\\nalso in lowercase, and POS tag.\\n—Type of the candidate event and the anaphor event.\\n—Depth of candidate anchor word in parse tree.\\n—Distance between the candidate and anchor, measured in sentences.\\n—Number, heads, and roles of shared arguments, etc.\\n4.4.2 Bejan and Harabagiu. Supervised approaches to solving event corefer-\\nence use linguistic properties to decide if a pair of event mentions is coreferen-\\ntial [Humphreys et al. 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji\\n2009]. These models depend on labeled training data, and annotating a large\\ncorpus with event coreference information requires substantial manual eﬀort. In\\naddition, since these models make local pairwise decisions, they are unable to cap-\\nture a global event distribution at topic or document collection level. [Bejan and\\nHarabagiu 2010] present how nonparametric Bayesian models can be applied to an\\nopen-domain event coreference task in an unsupervised manner.\\nThe ﬁrst model extends the hierarchical Dirichlet process [Teh et al. 2006] to\\ntake into account additional properties associated with event mentions. The second\\nmodel overcomes some of the limitations of the ﬁrst model, and uses the inﬁnite\\nfactorial hidden Markov model [Gael et al. 2009] coupled to the inﬁnite hidden\\nMarkov model [Beal et al. 2001] in order to consider a potentially inﬁnite number\\nof features associated with observable objects which are event mentions here, per-\\nform an automatic selection of the most salient features, and capture the structural\\ndependencies of observable objects or event mentions at the discourse level. Fur-\\nthermore, both models can work with a potentially inﬁnite number of categorical\\noutcomes or events in this case.\\nTwo event mentions corefer if they have the same event properties and share the\\nsame event participants. To ﬁnd coreferring event mentions, Bejan and Harabagiu\\ndescribe words that may be possible event mentions with lexical features, class fea-\\ntures such as POS and event classes such [Pustejovsky et al. 2003] as occurrence,\\nstate and action, Wordnet features, semantic features obtained by a semantic parse\\n[Bejan and Hathaway 2007] and the predicate argument structures encoded in Prop-\\nBank annotations [Palmer et al. 2005] as well as semantic annotations encoded in\\nthe FrameNet corpus [Baker et al. 1998].\\nThe ﬁrst model represents each event mention by a ﬁnite number of feature types,\\nand is also inspired by the Bayesian model proposed by [Haghighi and Klein 2007].\\nIn this model, a Dirichlet process (DP) [Ferguson 1973] is associated with each\\ndocument, and each mixture component (i.e., event) is shared across documents\\nsince In the process of generating an event mention, an event index z is ﬁrst sampled\\nby using a mech- anism that facilitates sampling from a prior for in- ﬁnite mixture\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 36}), Document(page_content='38· ...\\nmodels called the Chinese restaurant franchise (CRF) representation, as reported\\nin [Teh et al. 2006].\\nThe second model they use is called the iHMM-iFHMM model (inﬁnite hidden\\nMarkov model–inﬁnite factorial hidden Markov model). The iFHMM framework\\nuses the Markov Indian buﬀet process (mIBP) [Gael et al. 2009] in order to represent\\neach object as a sparse subset of a potentially unbounded set of latent features\\n[Ghahramani and Griﬃths 2005; Van Gael et al. 2008], Speciﬁcally, the mIBP\\ndeﬁnes a distribution over an unbounded set of binary Markov chains, where each\\nchain can be associated with a binary latent feature that evolves over time according\\nto Markov dynamics. The iFHMM allows a ﬂexible representation of the latent\\nstructure by letting the number of parallel Markov chains be learned from data, it\\ncannot be used where the number of clustering components is inﬁnite. An iHMM\\nrepresents a nonparametric extension of the hidden Markov model (HMM) [Rabiner\\n1989] that allows performing inference on an inﬁnite number of states. To further\\nincrease the representational power for modeling discrete time series data, they\\ndevelop a nonparametric extension that combines the best of the two models, and\\nlets the two parameters M and K be learned from data Each step in the new\\niHMM-iFHMM generative process is performed in two phases: (i) the latent feature\\nvariables from the iFHMM framework are sampled using the mIBP mechanism; and\\n(ii) the features sampled so far, which become observable during this second phase,\\nare used in an adapted version of the beam sampling algorithm [ ?] to infer the\\nclustering components (i.e., latent events).\\nThey report results in terms of recall (R), precision (P), and F-score (F) by\\nemploying the mention-based B3 metric [Bagga and Baldwin 1998], the entity-based\\nCEAF metric [Luo 2005], and the pairwise F1 (PW) metric. Their experiments for\\nshow that both of these models work well when the feature and cluster numbers\\nare treated as free parameters, and the selection of feature values is performed\\nautomatically.\\n4.4.3 Hovy et al. [Hovy et al. 2013] argue that events represent complex phe-\\nnomena and can therefore co-refer fully, being identical, like other researchers have\\ndiscussed, or co-refer partially , being quasi-identical or only partially identical. Two\\nevent mentions fully co-refer if their activity, event or state representation is identi-\\ncal in terms of all features used (e.g., agent, location or time). Two event mentions\\nare quasi-identical if they partially co-refer, i.e., most features are the same, but\\nthere may be additional details to one or the other.\\nWhen two events fully co-refer, Hovy et al. state they may be lexically identical\\n(i.e., the same senses of the same word, e.g., destroy and destruction ), synonymous\\nwords, one mention is a wider reading of the other (e.g., The attack took place\\nyesterday and The bombing killed four people ), one mention is a paraphrase of the\\nother with possibly some syntactic diﬀerences (e.g., He went to Boston andHe came\\nto Boston ), and one mention deictically refers to the other (e.g., the party and that\\nevent ). Quasi-identity or partial co-reference may arise in two ways: membership\\nidentity orsubevent identity . Membership identity occurs when one mention, say\\nA, is a set of multiple instances of the same type of event, and the other mention,\\nsay B, is one of the individual events in A (e.g., I attended three parties last week.\\nThe ﬁrst one was the best. ). Subevent identity is found when one mention, say A,\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 37}), Document(page_content='...· 39\\nis a stereotypical sequence (or script) of events whereas the other mention, say B, is\\none of the actions or events within the script (e.g., The family ate at the restaurant.\\nThe dad paid the waitress at the end. ).\\nHovy et al.attempt to build a corpus containing event co-reference links with\\nhigh quality annotations, i.e., annotations with high inter-annotator agreement, to\\nbe useful for machine learning. They have created two corpora to assist with a\\nproject on automated deep reading of texts. One corpus is in the domain of violent\\nevents (e..g., bombings, killens and wars), and the other one containing texts about\\nthe lives of famous people. In both of these corpora, they have annotated a limited\\nnumber of articles with full and partial co-references.\\n4.4.4 Delmonte. [Delmonte 2013] claims that performing event co-reference with\\nhigh accuracy requires deep understanding of the text and statistically-based meth-\\nods, both supervised and unsupervised, do not perform well. He claims that this\\nis the case because because it is absolutely necessary to identify arguments of an\\nevent reliably before event co-references can be found. Arguments are diﬃcult to\\nidentify because many are implicit and linguistically unexpressed. Successful even\\nco-reference identiﬁcation needs determination of spatio-temporal anchoring and\\nlocations in time and space are also very often implicit.\\nThe system he builds uses a linguistically based semantic module, which has\\na number of diﬀerent submodules which take care of Spatio-Temporal Reasoning,\\nDiscourse Level Anaphora Resolution, and determining Topic Hierarchy. The coref-\\nerence algorithm works as follows: for each possible referent it check all possible\\ncoreference links, at ﬁrst using only the semantic features, which are: wordform\\nand lemma identity; then semantic similarity measured on the basis of a number of\\nsimilarity criteria which are lexically based. The system searches WordNet synsets\\nand assign a score according to whether the possible referents are directly contained\\nin the same synset or not. A diﬀerent score is assigned if their relation can be in-\\nferred from the hierarchy. Other computational lexical resources they use include\\nFrameNet and Frames hierarchy; SumoMilo and its semantic classiﬁcation.\\nAfter collecting all possible coreferential relations, the system ﬁlters out those\\nlinks that are inconsistent or incompatible. Argument structure and spatiotemporal\\nrelations are computed along with dependence relations; temporal logical relations\\nas computed using an adaptation of Allen’s algorithm. The system also computes\\nsemantic similarity, where high values are preferred. The paper does not give any\\nresults to support the initial hypothesis, although the ideas are interesting.\\n4.4.5 Cybulska and Vossen. [Cybulska and Vossen 2015] use granularity in com-\\nputing event coreference. The intuition is, that an event with a longer duration,\\nthat happens on a bigger area and with multiple particpants (for instance a war\\nbetween Russia and Ukraine ) might be related to but will probably not fully corefer\\nwith a lower level event of shorter duration and with single participants involved\\n(e.g. A Russian soldier has shot dead a Ukrainian naval oﬃcer ).\\nCoreference between mentions of two events is determined by computing compat-\\nibility of contents of event attributes. The attributes used are event trigger, time,\\nlocation, human and non-human participant slots [Cybulska and Vossen 2014a].\\nGranularity size is mentioned in terms of durations of event actions [Gusev et al.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 38}), Document(page_content='40· ...\\n2011] and granularity levels of event participants, time and locations. Granularity\\nis given in terms of partonomic relations or through the part-of relation, between\\nentities and events, using the taxonomy of meronymic relations by [Winston et al.\\n1987]. Granularity levels of the human participant slot are contained within Win-\\nstons et al. Member-Collection relations. The temporal granularity levels make\\npart of Winstons Portion-Mass relationships and locational levels are in line with\\nPlace-Area relations in Winstons taxonomy.\\nCybulska and Vossen experimented with a decision-tree supervised pairwise bi-\\nnary classiﬁer to determine coreference of pairs of event mentions. They also ran\\nexperiments with a linear SVM and a multinomial Naive Bayes classiﬁer but the\\ndecision-tree classiﬁer outperformed both of them.\\nFor the experiments, Cybulska and Vossen use the ECB+ dataset [Cybulska and\\nVossen 2014b]. The ECB+ corpus contains a new corpus component, consisting\\nof 502 texts, describing diﬀerent instances of event types. They provide results in\\nterms of several metrics: recall, precision and F-score, MUC [Vilain et al. 1995],\\nB3 [Bagga and Baldwin 1998], mention-based CEAF [Luo 2005], BLANC [Recasens\\nand Hovy 2011], and CoNLL F1 [Pradhan et al. 2011], and ﬁnd that the introduction\\nof the granularity concept into similarity computation improves results for every\\nmetric.\\n5. BIOMEDICAL EVENT EXTRACTION\\nResearchers are interested in extracting information from the huge amount of\\nbiomedical literature published on a regular basis. Of course, one aspect of in-\\nformation extraction is event extraction, the focus of this paper. In the biomedical\\ncontext, an event extraction system tries to extract details of bimolecular interac-\\ntions among biomedical entities such as proteins and genes, and the processes they\\ntake part in, as described in terms of textual documents. Manually annotated cor-\\npora are used to train machine learning techniques and evaluate event extraction\\ntechniques.\\nThere have been several workshops on biomedical natural language processing.\\nWe focus on the BioNLP Shared Tasks in recent years that had competitions on\\nevent extraction. There have been three BioNLP Shared Task competitions so far:\\n2009, 2011, and 2013. The BioNLP 2009 Shared Task [Kim et al. 2009] was based\\non the GENIA corpus [Kim et al. 2003] which contains PubMed11abstracts of\\narticles on transcription factors in human blood cells. There was a second BioNLP\\nShared Task competition organized in 2011 to measure the advances in approaches\\nand associated results [Kim et al. 2011]. The third BioNLP ST was held in 2013.\\nWe discuss some notable systems from BioNLP ST 2011 and 2013.\\nBefore the BioNLP Shared Tasks, event extraction in the biomedical domain\\nusually classiﬁed each pair of named entities (usually protein names) co-occurring\\nin the text as interacting or not. BioNLP Shared Tasks extended such an approach\\nby adding relations such as direction ,type and nesting . An event deﬁnes the type\\nof interaction, such as phosphorylation , and is usually marked in the text with\\natrigger word (e.g., phosphorylates ) describing the interaction. This word forms\\nthe core of the event description. A directed event has roles that have inherent\\n11http://www.ncbi.nlm.nih.gov/pubmed\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 39}), Document(page_content='...· 41\\ndirectionality such as cause ortheme , the agent or target of the biological process.\\nIn addition, events can act as arguments of other events, creating complex nested\\nstructures. For example, in the sentence Stat3 phosphorylation is regulated by Vav ,\\naphosphorylation -event is the argument of the regulation -event.\\nThe BioNLP Shared Tasks provide task deﬁnitions, benchmark data and eval-\\nuations, and participants compete by developing systems to perform the speciﬁed\\ntasks. The theme of BioNLP-ST 2011 was a generalization of the 2009 contest,\\ngeneralized in three ways: text types, event types, and subject domains. The 2011\\nevent-related tasks were arranged in four tracks: GENIA task (GE) [Kim et al.\\n2011], Epigenetics and Post-translational Modiﬁcations (EPI) [Ohta et al. 2011],\\nInfectious Diseases (ID)[Pyysalo et al. 2011], and the Bacteria Track [Bossy et al.\\n2011; Jourde et al. 2011].\\nOf the four event-related shared tasks in BioNLP 2011, the ﬁrst three were related\\nto event extraction. The Genia task was focused on the domain of transcription\\nfactors in human blood cell. Trascription is a complex but just the ﬁrst step in the\\nprocess in which the instructions contained in the DNA in the nucleus of a cell are\\nused to produce proteins that control most life processes. Transcription factors are\\nproteins that control the transcription process. The EPI task was focused on events\\nrelated to epigenetics, dealing with protein and DNA modiﬁcations, with 14 new\\nevent types, including major protein modiﬁcation types and their reverse reactions.\\nEpigenesis refers to the development of a plant or animal from a seed, spore or egg,\\nthrough a sequence of steps in which cells diﬀerentiate and organs form. The EPI\\ntask was designed toward pathway extraction and curation of domain databases\\n[Wu et al. 2003; Ongenaert et al. 2008]. A biological pathway refers to a sequence\\nof actions among molecules in a cell that leads to a certain product or a change in\\nthe cell. The ID task was focused on extraction of events relevant to biomolecular\\nmechanisms of infectious diseases from full length publications. Tasks other than\\nID focused on abstracts only.\\nIn this paper, we discuss the systems and approaches for only the 2011 GE Task.\\nThis is because several of the winning systems for the GE Task did well in the other\\ntwo relevant tasks as well. The Genia Task is described in Table VII. The table\\nshows for each event type, the primary and secondary arguments to be extracted.\\nFor example, a phosphorylation event is primarily extracted with the protein to be\\nphosphorylated, which is the addition of a phosphate group to a protein or other\\norganic molecule. As secondary information, the speciﬁc site to be phosphorylated\\nmay be extracted. From a computational viewpoint, the event types represent\\ndiﬀerent levels of complexity. When only primary arguments are considered, the\\nﬁrst ﬁve event types in Table VII are classiﬁed as simple events , requiring only\\nunary arguments. The binding and regulation types are more complex. Binding\\nrequires the detection of an arbitrary number of arguments, and Regulation requires\\ndetection of recursive event structure.\\nConsider the sentence In this study we hypothesized that the phosphorylation of\\nTRAF2 inhibits binding to the CD40 cytoplasmic domain. Here there are two\\nprotein (entity) names: TRAF2 and CD40 . The word phosphorylation refers to an\\nevent; this string is a trigger word. Thus, the goal of the GE task was to identify\\na structure like the ones in Tables VIII and IX . In the tables, Tirepresents a\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 40}), Document(page_content='42· ...\\nEvent Type Primary Argument Secondary Argument\\nGene expression Theme (Protein)\\nTranscription Theme (Protein)\\nProtein catabolism Theme (Protein)\\nPhosphorylation Theme (Protein) Site(Entity)\\nLocalization Theme (Protein) AllLoc(entity), ToLoc (Entity)\\nBinding Theme (Protein)+ Site(Entity)+\\nRegulation Theme (Protein/Event), Cause\\n(Protein/Event)Site(Entity), CSite(Entity)\\nPositive regulation Theme (Protein/Event), Cause\\n(Protein/Event)Site(Entity), CSite(Entity)\\nNegative regulation Theme (Protein/Event), Cause\\n(Protein/Event)Site(Entity), CSite(Entity)\\nTable VII. Event types and their arguments in the Genia event task. The type of arguments,\\nprimary and secondary, to be extracted from the text, are also given.\\nTable VIII. Structure of an Event in BioNLP 2011 Contest, corresponding to Task 1 (GE)\\nTable IX. Structure of an Event in BioNLP 2011 Contest, corresponding to Task 2 (EPI)\\ntrigger word, and Eirepresents an event associated with the corresponding trigger\\nword. There are three events, E1is the phosphorylation event, E2is the binding\\nevent and E3is the negative regulation event. For each trigger word, we see the\\nstarting and ending character positions in the entire string. For each event, we see\\nthe participants in it. The second task identiﬁes an additional siteargument.\\nTable X shows the best results for various tasks in the BioNLP 2011 contests.\\n[Kim et al. 2011] note an improvement of 10% over the basic GE task, in 2011\\n(Task GEa), compared to 2009. The results of the GE tasks show that automatic\\nextraction of simple events–those with unary arguments, e.g., gene expression, lo-\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 41}), Document(page_content='...· 43\\nTask Evaluation Results\\nBioNLP ST 2000 46.73 / 58.48/ 51.95\\nMiwa et al. (2010b) 48.62 / 58.96 / 53.29\\nLLL 2005 (LLL) 53.00 / 55.60 / 54.30\\nGE abstracts (GEa) 50.00 / 67.53 / 57.46\\nGE full texts (GEf) 47.84 / 59.76 / 53.14\\nGE PHOSPHORYLATION (GEp) 79.26 / 86.99 / 82.95\\nGE LOCALIZATION (GEl) 37.88 / 77.42 / 50.87\\nEPI full task (EPIf) 52.69 / 53.98 / 53..3\\nEPI core task (EPIc) 68.51 / 69.20 / 68.86\\nEPI PHOSPHORYLATION (EPIp) 86.15 / 74.67 / 80.00\\nID full task (IDf) 48.03 / 65.97 / 55.59\\nID core task (IDc) 50.62 / 66.06 / 57.32\\nTable X. Best results for various sub-tasks in BioNLP ST 2011. Recall/precision/F-score %\\ncalization and phosphorylation—can be achieved at about 70% in F-score, but the\\nextraction of complex events, e.g., binding and regulation is very challenging, with\\nonly 40% performance level. The GE and ID results show that generalization to full\\npapers is possible, with just a small loss in performance. The results of phospho-\\nrylation events in GE and EP are similar (GEp vs. EPIp), which leads [Kim et al.\\n2011] to conclude that removal of the GE domain speciﬁcity does not reduce event\\nextraction performance by much. EPIc results indicate that there are challenges to\\nextracting similar event types that need to be overcome; EPIf results indicate that\\nthere are diﬃcult challenges in extracting additional arguments. The complexity\\nof the ID task is similar to that of the GE task; this shows up in the ﬁnal results,\\nalso indicating that it is possible to generalize to new subject domains and new\\nargument (entity) types.\\nBelow, we provide a brief description of some of the approaches to biomedical\\nevent extraction from the BioNLP 2011 contests.\\n5.1 Technical Methods Used in BioNLP Shared Tasks 2011\\nThe team that won the GE Task was the FAUST system [Riedel et al. 2011], followed\\nby the UMass system [Riedel and McCallum 2011], then the UTurku system [Bj¨ orne\\nand Salakoski 2011]. The performance of these three systems on the various tasks\\nis given in Table XI. In addition, we have the Stanford system in the table because\\nit performed fairly well on the tasks.\\nThe UMass system [Riedel and McCallum 2011] looks at a sentence as having an\\nevent structure, and then projects it onto a labeled graph. See Figure 6 for a target\\nevent structure and the projected graph for the sentence fragment Phosphorylation\\nof TRAF2 inhibits binding to CD40 . The system searches for a structure that\\nconnects the event and its participating entities and imposes certain constraints\\non the structure. Thus, the UMass system treats the search for such a structure\\nas an optimization problem. To formulate this optimization problem, the system\\nrepresents the structure in terms of a set of binary variables, inspired by the work\\nof [Riedel et al. 2009; Bj¨ orne et al. 2009]. These binary variables are based on the\\nprojection of the events to the labeled graph. An example of a binary variable is\\nai,l.rto indicate that between positions iandlin the sentence, there is an edge\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 42}), Document(page_content='44· ...\\nTeam Simple Event Binding Regulation All\\nFAUSTW 68.5/80.3/73.9 44.2/53.7/48.5 38.0/54.9/44.9 49.4/64.8/56.0\\nA 66.2/81.0/72.9 45.5/58.1/51.1 39.4/58.2/47.0 50.0/67.5/57.5\\nF 75.6/78.2/76.9 41.1/44.7/ 42.8 35.0/48.2/40.6 47.9/58.5/52.7\\nUMassW 67.0/ 81.4/ 73.5 43.0/ 56.4/ 48.8 37.5/52.7/43.8 48.5/64.1/55.2\\nA 64.2/80.7/71.5 43.5/60.9/50.8 38.8/55.1/45.5 48.7/65.9/56.1\\nF 75.6/83.1/79.2 41.7/47.6/44.4 34.7/47.5/40.1 47.8/59.8/53.1\\nUTurkuW 68.2/76.5/72.1 42.8/43.6/43.3 38.7/47.6/42.7 49.6/57.7/53.3\\nA 65.0/76.7/70.4 45.2/50.0/47.5 40.4/49.0/44.3 50.1/59.5/54.4\\nF 78.2/75.8/77.0 37.5/31.8/34.4 35.0/44.5/39.2 48.3/53.4/50.7\\nStanfordW 65.8/76.8/70.9 39.9/9.9/44/3 27.6/48.8/35.2 42.4/61.1/50.0\\nA 62.1/77.6/69.3 42.4/54.2/47.6 28.3/50.0/36.1 42.6/62.7/50.7\\nF 75.6/75.0/75.3 34.0/40.2/36.9 26.0/46.1/33.3 41.9/57.4/48.4\\nTable XI. Evaluation results (recall / precision / f-score for Task 1 in Whole data set (W),\\nAbstracts only (A) and Full papers only (F)\\nFig. 6. (a) Sentence with target event structure, (b) Projection to labeled graph. ***Redraw the\\nimage***\\nlabeled rfrom a set of possible edge labels R. Another such binary variable is ti,p,q\\nthat indicates that at position i, there is a binding event with arguments pandq.\\nGiven a number of such variables, it is possible to write an objective function to\\noptimize in order to obtain events and entity bindings. The system decomposes\\nthe biomedical event extraction task into three sub-tasks: (a) event triggers and\\noutgoing edges on arguments, (b) event triggers and incoming edges on arguments,\\nand (c) and protein-protein bindings. The system obtains an objective function\\nfor each of the sub-tasks. It solves the three optimization problems one by one\\nin a loop, till no changes take place, or up to a certain number of iterations. The\\napproach uses optimizing by dual decomposition [Komodakis et al. 2007; Rush et al.\\n2010] since the dual of the original optimization problem is solved.\\nThe Stanford system [McClosky et al. 2011] exploits the observation that event\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 43}), Document(page_content='...· 45\\nstructures bear a close relation to dependency graphs [Jurafsky and Martin 2000,\\nChapter 12]. They cast bimolecular events in terms of these structures which\\nare pseudo-syntactic in nature. They claim that standard parsing tools such as\\nmaximum-spanning tree parsers and parse rerankers can be applied to perform event\\nextraction with minimum domain speciﬁc training. They use an oﬀ-the-shelf de-\\npendency parser, MSTParser [McDonald et al. 2005; McDonald and Pereira 2006],\\nbut extend it with event-speciﬁc features. Their approach requires conversion to\\nand from dependency trees, at the beginning and and at the end. The features in\\nthe MSTParser are quite local (i.e., able to examine a portion of each event at a\\ntime); the decoding necessary can be performed globally, allowing the dependency\\nparser some trade-oﬀs. Event parsing is performed using three modules: 1) anchor\\ndetection to identify and label event anchors, 2) event parsing to form candidate\\nevent structures by linking entries and event anchors, and 3) event reranking to\\nselect the best candidate event structure. First, they parse the sentences with a\\nreranking parser [Charniak and Johnson 2005] with the biomedical parsing model\\nfrom [McClosky 2010], using the set of Stanford dependencies [De Marneﬀe and\\nManning 2008]. After the parsing, they perform anchor detection using a technique\\ninspired by techniques for named entity recogntion to label each token with an\\nevent type or none , using a logistic regression classiﬁer. The classiﬁer uses features\\ninspired by [Bj¨ orne et al. 2009]. They change a parameter to obtain high recall to\\novergenerate event anchors. Multiword event anchors are reduced to their syntactic\\nhead. The event anchors and the included entities become a “reduced” sentence,\\ninput to the event parser. Thus, the event parser gets words that are believed to\\ndirectly take part in the events. This stage uses the MSTParser with additional\\nevent parsing features. The dependency trees are decoded and converted back to\\nevent structures. Finally, for event reranking, the system gets nbest list of event\\nstructures from each decoder in the previous step of event parsing. The reranker\\nuses global features of an event structure to restore and output the highest scor-\\ning structure. The reranking approach is based on parse reranking [Ratnaparkhi\\n1999], but is based on features of event structures instead of syntactic constituency\\nstructure. They use the cvlm estimator [Charniak and Johnson 2005] when learn-\\ning weights for the reranking model. Since the reranker can work with outputs of\\nmultiple decoders, they use it as an ensemble technique as in [Johnson and Ural\\n2010].\\nThe FAUST system [Riedel et al. 2011] shows that using a straightforward model\\ncombination strategy with two competitive systems, the UMass system [Riedel and\\nMcCallum 2011] and the Stanford system [McClosky et al. 2011] just described,\\ncan produce a new system with substantially high accuracy. The new system uses\\nthe framework of stacking [Alpaydin 2010, Chapter 17]. The new system does it\\nby including the predictions of the Stanford system into the UMass system, simply\\nas a feature. Using this simple model of stacking, the FAUST system was able to\\nobtain ﬁrst place in three tasks out of four where it participated.\\nThe Turku Event Extraction System [Bj¨ orne and Salakoski 2011; Bj¨ orne et al.\\n2012] can be easily adapted to diﬀerent event schemes, following the theme of event\\ngeneralization in BioNLP 2011. The system took part in eight tasks in BioNLP\\n2011 and demonstrated the best performance in four of them. The Turku system\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 44}), Document(page_content='46· ...\\ndivides event extraction into three main steps: i) Perform named entity recognition\\nin the sentence, ii) Predict argument relations between entities, and iii) Finally,\\nseparate entity/argument sets into individual events. The Turku system uses a\\ngraph notation with trigger and protein/gene entities as nodes and relations (e.g.,\\ntheme) as edges. In particular, an event in the graph representation is a trigger\\nnode along with its outgoing edges. The steps are shown in Figure 7. The Turku\\nsystem uses Support Vector Machines [Vapnik 1995; Tsochantaridis et al. 2006]\\nat various stages to perform each of the sub-tasks. To use an SVM classiﬁer, one\\nneeds to convert text into features understood by the classiﬁer. The Turku system\\nperforms a number of analyses on the sentences, to obtain features, which are\\nmostly binary. The features are categorized into token features (e.g., Porter-stem\\n[Porter 1980], Penn Treebank part-of-speech tags [Marcus et al. 1993], character bi-\\nand tri-grams, presence of punctuation on numeric characters), sentence features\\n(e.g., the number of named entities in the sentence), dependency chains (up to\\na depth of three, to deﬁne the context of the words), dependency with n-grams\\n(joining a token with two ﬂanking dependencies as well as each dependency with\\ntwo ﬂanking tokens), trigger features (e.g., the trigger word a gene or a protein)\\nand external features (e.g., Wordnet hypernyms, the presence of a word in a list of\\nkey terms). Applicable combinations of these features are then used by the three\\nsteps in event detection: trigger detection, edge detection and unmerging. Trigger\\nwords are detected by classifying each token as negative or as one of the positive\\ntrigger classes using SVMs. Sometimes several triggers overlap, in which case a\\nmerged class (e.g. phosphorylation–regulation ) is used. After trigger prediction,\\ntriggers of merged classes are split into their component classes. Edge detection\\nis used to predict event arguments or triggerless events and relations, all of which\\nare deﬁned as edges in the graph representation. The edge detector deﬁnes one\\nexample per direction for each pair of entities in the sentence, and uses the SVM\\nclassiﬁer to classify the examples as negatives or as belonging to one of the positive\\nclasses. When edges are predicted between these nodes, the result is a merged\\ngraph where overlapping events are merged into a single node and its set of outgoing\\nedges. To produce the ﬁnal events, these merged nodes need to be pulled apart into\\nvalid trigger and argument combinations. Unmerging is also performed using the\\nSVM classiﬁer. Speculation and negation are detected independently, with binary\\nclassiﬁcation of trigger nodes using SVMs. The features used are mostly the same\\nas for trigger detection, with the addition of a list of speculation-related words.\\n6. EXTRACTING EVENTS FROM SOCIALLY GENERATED DOCUMENTS\\nWith the explosive expansion of the Internet during the past twenty years, the\\nvolume of socially generated text has skyrocketed. Socially generated text includes\\nblogs and microblogs. For example, Twitter12, started in 2006, has become a social\\nphenomenon. It allows individuals with accounts to post short messages that are up\\nto 140 characters long. Currently, more than 340 million tweets are sent out every\\nday13. While a majority of posts are conversational or not particularly meaningful,\\n12http://www.twitter.com\\n13http://blog.twitter.com/2012/03/twitter-turns-six.htm\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 45}), Document(page_content='...· 47\\nFig. 7. Turku BioNLP Pipeline\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 46}), Document(page_content='48· ...\\nabout 3.6% of the posts concern topics of mainstream news14. Twitter has been\\ncredited with providing the most current news about many important events before\\ntraditional media, such as the attacks in Mumbai in November 2008. Twitter also\\nplayed a prominent role in the unfolding of the troubles in Iran in 2009 subsequent\\nto a disputed election, and the so-called Twitter Revolutions15in Tunisia and Egypt\\nin 2010-11.\\nMost early work on event extraction of information from documents found on the\\nInternet has focussed on news articles [Chambers and Jurafsky 2011; Doddington\\net al. 2004; Gabrilovich et al. 2004]. However, as noted earlier, social networking\\nsites such as Twitter and Facebook have become important complimentary sources\\nof such information. Individual tweets, like SMS messages, are usually short and\\nself-contained and therefore are not composed of complex discourse structures as\\nis the case with texts containing narratives. However, extracting structured rep-\\nresentation of events from short or informal texts is also challenging because most\\ntweets are about mundane things, without any news value and of interest only to\\nthe immediate social network. Individual tweets are also very terse, without much\\ncontext or content. In addition, since Twitter users can talk about any topic, it is\\nnot clear a priori what event types may be appropriate for extraction.\\nThe architecture of the system called TwiCal for event extraction [Ritter et al.\\n2012] from Twitter messages is given in Figure 8. Given a stream of raw tweets,\\nTwiCal extract events with associated named entities and times of occurrence. First\\nthe tweets are POS tagged using a tagger [Ritter et al. 2012], especially trained with\\nTwitter data. Then named entities are recognized [Ritter et al. 2011] using a rec-\\nognizer trained with Twitter data as well. After this, phrases that mention events\\n(or, event triggers or event phrases or just events) are extracted using supervised\\nlearning. [Ritter et al. 2012] annotated 1,000 tweets with event phrases, following\\nguidelines for annotation of EVENT tags in Timebank [Pustejovsky et al. 2003].\\nThe system recognizes event triggers as a sequence labeling task using Conditional\\nRandom Fields [Laﬀerty et al. 2001]. It uses a contextual dictionary, orthographic\\nfeatures, features based on the Twitter-tuned POS tagger, and dictionaries of event\\nterms gathered from WordNet [Saur´ ı et al. 2005]. Once a large number of events\\nhave been extracted by this CRF learner, TwiCal categorizes these events into types\\nusing an unsupervised approach based on latent variable models, inspired by work\\non modeling selectional preferences [Ritter et al. 2010; S´ eaghdha 2010; Kozareva\\nand Hovy 2010; Roberts and Harabagiu 2011] and unsupervised information ex-\\ntraction [Bejan et al. 2009; Chambers and Jurafsky 2011; Yao et al. 2011]. This\\nautomatic discovery of event types is similar to topic modeling, where one automat-\\nically identiﬁes the extant topics in a corpus of text documents. The automatically\\ndiscovered types (topics) are quickly inspected by a human eﬀort to ﬁlter out in-\\ncoherent ones, and the rest are annotated with informative labels. Examples of\\nevent types discovered along with top event phrases and top entities are given in\\nTable XII. The resulting set of types are applied to categorize millions of extracted\\nevents without the use of any manually annotated examples. For inference, the\\nsystem uses collapsed Gibbs sampling [Griﬃths and Steyvers 2004] and prediction\\n14http://www.pearanalytics.com/blog/tag/twitter/\\n15http://en.wikipedia.org/wiki/Twitter_Revolution\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 47}), Document(page_content='...· 49\\nFig. 8. TwiCal Architecture\\nLabel Top Event phrases Top Entities\\nSports tailgate, scrimmage, tailgating, home-\\ncomingespn, ncaa, tigers, eagles\\nConcert concert, presale, performs, tickets taylor swift, toronto, britney spears,\\nrihanna\\nPerform matinee, musical, priscilla, wicked shrek, les mis, lee evans, broadway\\nTV new season, season ﬁnale, ﬁnished sea-\\nson, episodesjersey shore, true blood, glee, dvr, hbo\\nTable XII. Some of Examples of event types extracted by TwiCal\\nEntity Event phrase Date Type\\nSteve Jobs died 10/6/11 Death\\niPhone announcement 10/4/11 ProductLaunch\\nGOP debate 9/7/11 PoliticalEvent\\nAmanda Knox verdict 10/3/11 Trial\\nTable XIII. Examples of events extracted by TwiCal\\nis performed using a streaming approach to inference [Yao et al. 2009]. To resolve\\ntemporal expressions, TwiCal uses TempEx [Mani and Wilson 2000], which takes\\nas input a reference date, some text and POS tags, and marks temporal expressions\\nwith unambiguous calendar references. Finally, the system measures the strength\\nof association between each named entity and date based on the number of tweets\\nthey co-occur in, in order to determine if the event is signiﬁcant. Examples of events\\nextracted by TwiCal are given in Table XIII. Each event is a 4-tuple including a\\nnamed entity, event phrase, calendar date and event type.\\nThe TwiCal system describe above used topic modeling using latent variables as\\none of the several computational components; it is used to capture events captured\\nusing supervised learning into types or topics. [Weng and Lee 2011] point out some\\ndrawbacks of using such an approach. The main problem is that frequently the re-\\nsult generated by Latent Dirichlet Analysis (LDA) is diﬃcult to interpret because\\nit simply gives a list of words associate with the topic. For example, when [Weng\\nand Lee 2011] attempt to ﬁnd the four most important topics using LDA based on a\\nTwitter collection emanating from Singapore on June 16, 2010, they ﬁnd the topics\\nlisted in Table XIV. Therefore, Weng et al. present another approach to detect\\nevents from a corpus of Twitter messages. Their focus is on detection and therefore,\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 48}), Document(page_content='50· ...\\nnot on extraction of components that describe an event. Event detection is based\\non the assumption that when an event is taking place, some related words show an\\nincrease in usage. In this scheme, an event is represented by a number of keywords\\nshowing a burst in appearance count [Yang et al. 1998; Kleinberg 2003]. Although\\nit is clear that tweets report events, but such reports are usually overwhelmed by\\nhigh ﬂood of meaningless “babbles”. In addition, the algorithms for event detec-\\ntion must be scalable to handle the torrent of Twitter posts. The EDCoW (Event\\nDetection with Clustering of Wavelet-based Signals) system builds signals for in-\\ndividual words by applying wavelet analysis [] on frequency-based raw signals of\\nwords occurring in the Twitter posts. These signals capture only the bursts in\\nthe words’ appearance. The signals are computed eﬃciently by wavelet analysis\\n[Kaiser 2011; Daubechies et al. 1992]. Wavelets are quickly vanishing oscillating\\nfunctions and unlike sine and cosine functions used in Discrete Fourier Transforma-\\ntion (DFT) []which are localized in frequency but extend inﬁnitely in time, wavelets\\nare localized both in time and frequency. Therefore, wavelet transformation is able\\nto provide precise measurements about when and to what extent bursts take place\\nin a signal. [Weng and Lee 2011] claim that this makes it a better choice for event\\ndetection when building signals for individual words. Wavelet transformation con-\\nverts signals from time domain to time-scale domain where scale can be considered\\nthe inverse of frequency. Such signals also take less space for storage. Thus, the\\nﬁrst thing EDCoW does is convert frequencies over time to wavelets, using a slid-\\ning window interval. It removes trivial words by examining signal auto-correlations.\\nThe remaining words are then clustered to form events with a modularity-based\\ngraph partitioning technique, which uses a scalable eigenvalue algorithm. It de-\\ntects events by grouping sets of words with similar patterns of burst. To cluster,\\nsimilarities between words need to be computed. It does so by using cross correla-\\ntion, which is a common measure of similarity between two signals [Orfanidis 1985].\\nCross correlation is a pairwise operation. Cross correlation values among a number\\nof signals can be represented in terms of a correlation matrix M, which happens\\nto be a symmetric sparse matrix of adjacent similarities. With this graph setup,\\nevent detection can be formulated as a graph partitioning problem, i.e., to cut the\\ngraph into subgraphs. Each subgraph corresponds to an event, which contains a\\nset of words with high cross correlation, and also that the cross correlation between\\nwords in diﬀerent subgraphs are low. The quality of such partitioning is measures\\nusing a metric called modularity [Newman 2004; 2006]. The modularity of a graph\\nis deﬁned as the sum of weights of all the edges that fall within subgraphs (after\\npartitioning) subtracted by the expected edge weight sum if the edges were placed\\nat random. The main computation task in this component is ﬁnding the largest\\neigenvalue and corresponding eigenvector, of the sparse symmetric modularity ma-\\ntrix. This is solved using power iteration, which is able to scale up with the increase\\nin the number of words in the tweets [Ipsen and Wills 2006]. EDCoW requires each\\nindividual event to contain at least two words. To diﬀerentiate big events from triv-\\nial ones, EDCoW quantiﬁes the events’ signiﬁcance, which depends on two factors,\\nthe number of words and cross-correlation among the words related to the event.\\nTo make EDCoW work with TwiCal to see if it improves performance, the topic\\ndetection module will have to be replaced. EDCoW associates fewer words to topics\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 49}), Document(page_content='...· 51\\nTopic ID Top Words\\n13 ﬂood, orchard, rain, spain, road, weather, singapor, love, cold\\n48 time, don, feel, sleep, love, tomorrow, happy, home, hate\\n11 time, love, don, feel, wait, watch, singapor, hope, life\\n8 watch, world, cup, match, time, love, don, south, goal\\nTable XIV. Examples of Topics Detected by LDA from Singapore based tweets on June 16, 2010\\nEvent Words Event Description\\ndemocrat, naoto Ruling Democratic Party of Japan elected Naoto Kan as chief\\nss501, juju Korean popular bands Super Junior’s and SS501’s performance on mubank\\n#kor, greece, #gre A match between South Korea and Greece in World Cup 2010l\\nTable XV. Examples of Events Detected by EDCoW in June 2010\\nbecause it ﬁlters words away before associating with a topic. Table XV gives a few\\nevent words obtained by EDCoW and the corresponding event description. Please\\nnote that the event description was created by the authors and not the system.\\n6.1 Summarization\\n[Filatova and Hatzivassiloglou 2004] use event-based features to represent sentences\\nand shows that their approach improves the quality of the ﬁnal summaries compared\\nto a baseline bag-of-words approach.\\n6.2 Question Answering\\nEvent recognition is a core task in question-answering since the majority of web\\nquestions have been found to be relate to events and situations in the world [Saur´ ı\\net al. 2005]. For example, to answer the question How many people were killed in\\nBaghdad in March? , orWho was the Prime MInister of India in when China and\\nIndia fought their only war? , the question-answering system may have to identify\\nevents across a bunch of documents before creating an answer.\\n7. FUTURE DIRECTIONS OF RESEARCH\\nIt also seems like when doctors take notes on a patient’s history or medical record,\\nthe information is not written in order of events or in temporal order all the time.\\nIt will be good to take notes from here and there and put them in an event ordered\\nfashion or temporally ordered manner. Extracting an event based structure of the\\nmedical record would help understand the medical history better.\\nMost systems process sentences in isolation, like most event extraction systems\\nat the current time. Therefore, events crossing sentence boundaries cannot be\\ndetected.\\nREFERENCES\\n1981. Longman dictionary of contemporary english. England: Longman Group Limited .\\nAdafre, S. F. and de Rijke, M. 2005. Feature engineering and post-processing for temporal\\nexpression recognition using conditional random ﬁelds. In Proceedings of the ACL Workshop\\non Feature Engineering for Machine Learning in Natural Language Processing . 9–16.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 50}), Document(page_content='52· ...\\nAguilar, J. ,Beller, C. ,McNamee, P. ,and Van Durme, B. 2014. A comparison of the events\\nand relations across ace, ere, tac-kbp, and framenet annotation standards. 2nd Workshop on\\nEvents: Deﬁnition, Detection, Coreference and Representation, NAACL-HLT , 45.\\nAhn, D. 2006. The stages of event extraction. In Proc. COLING/ACL 2006 Workshop on\\nAnnotating and Reasoning about Time and Events . 1–8.\\nAllen, J. F. 1983. Maintaining knowledge about temporal intervals. Communications of the\\nACM 26, 11, 832–843.\\nAllen, J. F. 1984. Towards a general theory of action and time. Artiﬁcial intelligence 23, 2,\\n123–154.\\nAlpaydin, E. 2010. Introduction to machine learning . The MIT Press.\\nAngeli, G. ,Manning, C. D. ,and Jurafsky, D. 2012. Parsing time: Learning to interpret time\\nexpressions. In Proceedings of the 2012 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies . 446–455.\\nAone, C. and Ramos-Santacruz, M. 2000. Rees: a large-scale relation and event extraction sys-\\ntem. In Proceedings of the sixth conference on Applied natural language processing . Association\\nfor Computational Linguistics, 76–83.\\nBagga, A. and Baldwin, B. 1998. Algorithms for scoring coreference chains. In The ﬁrst inter-\\nnational conference on language resources and evaluation workshop on linguistics coreference .\\nVol. 1. 563–566.\\nBagga, A. and Baldwin, B. 1999. Cross-document event coreference: Annotations, experiments,\\nand observations. In Proceedings of the Workshop on Coreference and its Applications . 1–8.\\nBaker, C. ,Fillmore, C. ,and Lowe, J. 1998. The berkeley framenet project. In Proceedings\\nof the 36th Annual Meeting of the Association for Computational Linguistics and 17th Inter-\\nnational Conference on Computational Linguistics-Volume 1 . Association for Computational\\nLinguistics, 86–90.\\nBaker, M. 1988. Incorporation: A theory of grammatical function changing . University of\\nChicago Press Chicago.\\nBarnes, J. et al. 1984. The complete works of Aristotle: The revised Oxford translation . Vol. 1.\\nBollingen Foundation.\\nBeal, M. J. ,Ghahramani, Z. ,and Rasmussen, C. E. 2001. The inﬁnite hidden markov model.\\nInAdvances in neural information processing systems . 577–584.\\nBejan, C. ,Titsworth, M. ,Hickl, A. ,and Harabagiu, S. 2009. Nonparametric bayesian models\\nfor unsupervised event coreference resolution. Advances in Neural Information Processing\\nSystems 23 .\\nBejan, C. A. and Harabagiu, S. 2010. Unsupervised event coreference resolution with rich\\nlinguistic features. In Proceedings of the 48th Annual Meeting of the Association for Compu-\\ntational Linguistics . 1412–1422.\\nBejan, C. A. and Hathaway, C. 2007. Utd-srl: a pipeline architecture for extracting frame se-\\nmantic structures. In Proceedings of the 4th International Workshop on Semantic Evaluations .\\n460–463.\\nBelvin, R. 1993. The two causative haves are the two possessive haves. MIT working papers in\\nlinguistics 20 , 19–34.\\nBerger, A. L. ,Pietra, V. J. D. ,and Pietra, S. A. D. 1996. A maximum entropy approach to\\nnatural language processing. Computational linguistics 22, 1, 39–71.\\nBethard, S. and Martin, J. 2006. Identiﬁcation of event mentions and their semantic class.\\nInProceedings of the 2006 Conference on Empirical Methods in Natural Language Processing .\\nAssociation for Computational Linguistics, 146–154.\\nBj¨orne, J. ,Ginter, F. ,and Salakoski, T. 2012. University of turku in the bionlp’11 shared\\ntask. BMC bioinformatics 13, Suppl 11, S4.\\nBj¨orne, J. ,Heimonen, J. ,Ginter, F. ,Airola, A. ,Pahikkala, T. ,and Salakoski, T. 2009.\\nExtracting complex biological events with rich graph-based feature sets. In Proceedings of\\nthe Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task .\\nAssociation for Computational Linguistics, 10–18.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 51}), Document(page_content='...· 53\\nBj¨orne, J. and Salakoski, T. 2011. Generalizing biomedical event extraction. In Proceedings of\\nthe BioNLP Shared Task 2011 Workshop . Association for Computational Linguistics, 183–191.\\nBossy, R. ,Jourde, J. ,Bessieres, P. ,van de Guchte, M. ,and N ´edellec, C. 2011. BioNLP\\nShared Task 2011: Bacteria Biotope. In Proceedings of the BioNLP Shared Task 2011 Work-\\nshop. Association for Computational Linguistics, 56–64.\\nBresnan, J. 1982. The mental representation of grammatical relations . Vol. 170. MIT press\\nCambridge, MA.\\nCarlson, L. 1981. Aspect and quantiﬁcation in tense and aspect. ed. by philip tedeschi and annie\\nzaenen. Syntax and Semantics Ann Arbor, Mich. 14 , 31–64.\\nChambers, N. and Jurafsky, D. 2011. Template-based information extraction without the\\ntemplates. In Proceedings of ACL .\\nChambers, N. ,Wang, S. ,and Jurafsky, D. 2007. Classifying temporal relations between events.\\nInProceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration\\nSessions . Association for Computational Linguistics, 173–176.\\nCharniak, E. and Johnson, M. 2005. Coarse-to-ﬁne n-best parsing and maxent discrimina-\\ntive reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational\\nLinguistics . Association for Computational Linguistics, 173–180.\\nCharoenporn, T. ,Sornlertlamvanich, V. ,Mokarat, C. ,and Isahara, H. 2008. Semi-\\nautomatic compilation of asian wordnet. In 14th Annual Meeting of the Association for Natural\\nLanguage Processing . 1041–1044.\\nChen, Z. and Ji, H. 2009. Graph-based event coreference resolution. In Proceedings of the 2009\\nWorkshop on Graph-based Methods for Natural Language Processing . 54–57.\\nChinchor, N. and Marsh, E. 1998. Muc-7 information extraction task deﬁnition. In Proceeding\\nof the seventh message understanding conference (MUC-7), Appendices . 359–367.\\nCollins, M. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis,\\nUniversity of Pennsylvania.\\nCollins, M. 2003. Head-driven statistical models for natural language parsing. Computational\\nlinguistics 29, 4, 589–637.\\nCybulska, A. and Vossen, P. 2010. Event models for historical perspectives: Determining\\nrelations between high and low level events in text, based on the classiﬁcation of time, location\\nand participants. In Proceedings of LREC . 17–23.\\nCybulska, A. and Vossen, P. 2011. Historical event extraction from text. ACL HLT 2011\\nWorkshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities\\n(LaTeCH 2011) , 39.\\nCybulska, A. and Vossen, P. 2014a. Guidelines for ecb+ annotation of events and their corefer-\\nence. Tech. rep., Technical report, Technical Report NWR-2014-1, VU University Amsterdam.\\nCybulska, A. and Vossen, P. 2014b. Using a sledgehammer to crack a nut? lexical diversity and\\nevent coreference resolution. In Proceedings of the Ninth International Conference on Language\\nResources and Evaluation (LREC’14) . 4545–4552.\\nCybulska, A. and Vossen, P. 2015. Translating granularity of event slots into features for event\\ncoreference resolution. In Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT .\\n1–10.\\nDaelemans, W. ,Zavrel, J. ,van der Sloot, K. ,and Van den Bosch, A. 2004. Timbl: Tilburg\\nmemory-based learner. Tilburg University .\\nDang, H. 2004. Investigations into the role of lexical semantics in word sense disambiguation,\\nph.d. dissertation, university of pennsylvania.\\nDang, H. ,Kipper, K. ,Palmer, M. ,and Rosenzweig, J. 1998. Investigating regular sense ex-\\ntensions based on intersective levin classes. In Proceedings of the 17th international conference\\non Computational linguistics-Volume 1 . Association for Computational Linguistics, 293–299.\\nDaubechies, I. et al. 1992. Ten lectures on wavelets . Vol. 61. SIAM.\\nDaum ´e III, H. 2004. Notes on cg and lm-bfgs optimization of logistic regression. Pa-\\nper available at http://pub. hal3. name# daume04cg-bfgs, implementation available at\\nhttp://hal3.name/megam .\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 52}), Document(page_content='54· ...\\nDavidson, D. 1967, 2001. Essays on actions and events . Vol. 1. Oxford University Press, USA.\\nDe Marneffe, M.-C. ,MacCartney, B. ,and Manning, C. D. 2006. Generating typed depen-\\ndency parses from phrase structure parses. In Proceedings of LREC . Vol. 6. 449–454.\\nDe Marneffe, M.-C. and Manning, C. D. 2008. The stanford typed dependencies represen-\\ntation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain\\nParser Evaluation . Association for Computational Linguistics, 1–8.\\nDelmonte, R. 2013. Coping with implicit arguments and events coreference. NAACL HLT 2013 ,\\n1.\\nDoddington, G. ,Mitchell, A. ,Przybocki, M. ,Ramshaw, L. ,Strassel, S. ,and Weischedel,\\nR.2004. The automatic content extraction (ace) program–tasks, data, and evaluation. In\\nProceedings of LREC . Vol. 4. 837–840.\\nDorr, B. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual\\nmachine translation. Machine Translation 12, 4, 271–322.\\nDowty, D. 1979. Word meaning and Montague grammar: The semantics of verbs and times in\\ngenerative semantics and in Montague’s PTQ . Vol. 7. Springer.\\nDowty, D. 1991. Thematic proto-roles and argument selection. Language , 547–619.\\nFellbaum, C. 2010. Wordnet. Theory and Applications of Ontology: Computer Applications ,\\n231–243.\\nFerguson, T. S. 1973. A bayesian analysis of some nonparametric problems. The annals of\\nstatistics , 209–230.\\nFilatova, E. and Hatzivassiloglou, V. 2004. Event-based extractive summarization. In Pro-\\nceedings of ACL Workshop on Summarization .\\nFillmore, C. 1968. The case for case. universals in linguistic theory, ed. by emmon bach & robert\\nt. harms, 1-88.\\nFillmore, C. 1976. Frame semantics and the nature of language. Annals of the New York\\nAcademy of Sciences 280, 1, 20–32.\\nFillmore, C. 1977. The case for case reopened. Syntax and semantics 8, 1977, 59–82.\\nFillmore, C. 2006. Frame semantics. Cognitive linguistics: basic readings , 185–238.\\nFillmore, C. and Baker, C. 2001a. Frame semantics for text understanding. In Proceedings of\\nWordNet and Other Lexical Resources Workshop . Pittsburgh, NAACL, 3–5.\\nFillmore, C. and Baker, C. 2001b. Frame semantics for text understanding. In Proceedings of\\nWordNet and Other Lexical Resources Workshop . Pittsburgh, NAACL, 3–5.\\nFillmore, C. ,Johnson, C. ,and Petruck, M. 2003. Background to framenet. International\\njournal of lexicography 16, 3, 235–250.\\nFlorian, R. ,Hassan, H. ,Ittycheriah, A. ,Jing, H. ,Kambhatla, N. ,Luo, X. ,Nicolov, N. ,\\nand Roukos, S. 2004. A statistical model for multilingual entity detection and tracking. In\\nHLT-NAACL .\\nGabbard, R. ,Kulick, S. ,and Marcus, M. 2006. Fully parsing the Penn Treebank. In Pro-\\nceedings of the Human Language Technology Conference of the NAACL, Main Conference .\\n184–191.\\nGabrilovich, E. ,Dumais, S. ,and Horvitz, E. 2004. Newsjunkie: providing personalized news-\\nfeeds via analysis of information novelty. In Proceedings of the 13th international conference\\non World Wide Web . ACM, 482–490.\\nGael, J. V. ,Teh, Y. W. ,and Ghahramani, Z. 2009. The inﬁnite factorial hidden markov model.\\nInAdvances in Neural Information Processing Systems . 1697–1704.\\nGangemi, A. ,Guarino, N. ,Masolo, C. ,Oltramari, A. ,and Schneider, L. 2002. Sweetening\\nontologies with dolce. Knowledge engineering and knowledge management: Ontologies and the\\nsemantic Web , 223–233.\\nGanitkevitch, J. ,Van Durme, B. ,and Callison-Burch, C. 2013. Ppdb: The paraphrase\\ndatabase. In HLT-NAACL . 758–764.\\nGhahramani, Z. and Griffiths, T. L. 2005. Inﬁnite latent feature models and the indian buﬀet\\nprocess. In Advances in neural information processing systems . 475–482.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 53}), Document(page_content='...· 55\\nGildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational\\nlinguistics 28, 3, 245–288.\\nGriffiths, T. L. and Steyvers, M. 2004. Finding scientiﬁc topics. Proceedings of the National\\nacademy of Sciences of the United States of America 101, Suppl 1, 5228–5235.\\nGrimshaw, J. 1990. Argument structure . the MIT Press.\\nGrimshaw, J. and Jackendoff, R. 1981. Brandeis verb lexicon. Electronic database funded by\\nNational Science Foundation Grant NSF IST-81-20403 awarded to Brandeis University .\\nGrishman, R. ,Macleod, C. ,and Meyers, A. 1994. Comlex syntax: Building a computational\\nlexicon. In Proceedings of the 15th conference on Computational linguistics-Volume 1 . Associ-\\nation for Computational Linguistics, 268–272.\\nGrover, C. ,Tobin, R. ,Alex, B. ,and Byrne, K. 2010. Edinburgh-ltg: Tempeval-2 system\\ndescription. In Proceedings of the 5th International Workshop on Semantic Evaluation . Asso-\\nciation for Computational Linguistics, 333–336.\\nGruber, J. 1965. Studies in lexical relations. Ph.D. thesis, Massachusetts Institute of Technology.\\nGusev, A. ,Chambers, N. ,Khaitan, P. ,Khilnani, D. ,Bethard, S. ,and Jurafsky, D. 2011.\\nUsing query patterns to learn the duration of events. In Proceedings of the ninth international\\nconference on computational semantics . 145–154.\\nHabash, N. and Dorr, B. 2002. Handling translation divergences: Combining statistical and\\nsymbolic techniques in generation-heavy machine translation. Machine Translation: From Re-\\nsearch to Real Users , 84–93.\\nHaffner, P. 2006. Scaling large margin classiﬁers for spoken language understanding. Speech\\nCommunication 48, 3, 239–261.\\nHaghighi, A. and Klein, D. 2007. Unsupervised coreference resolution in a nonparametric\\nbayesian model. In Annual meeting-Association for Computational Linguistics . Vol. 45. 848.\\nHalpern, J. Y. and Pearl, J. 2005. Causes and explanations: A structural-model approach.\\npart i: Causes. The British journal for the philosophy of science 56, 4, 843–887.\\nHamilton, E. ,Cairns, H. ,et al. 1961. Plato, the collected dialogues . Vol. 71. Bollingen.\\nHan, C. ,Lavoie, B. ,Palmer, M. ,Rambow, O. ,Kittredge, R. ,Korelsky, T. ,Kim, N. ,and\\nKim, M. 2000. Handling structural divergences and recovering dropped arguments in a ko-\\nrean/english machine translation system. Envisioning Machine Translation in the Information\\nFuture , 168–176.\\nHeafield, K. ,Pouzyrevsky, I. ,Clark, J. H. ,and Koehn, P. 2013. Scalable modiﬁed kneser-\\nney language model estimation. In ACL (2) . 690–696.\\nHigginbotham, J. 1985. On semantics. Linguistic inquiry 16, 4, 547–593.\\nHobbs, J. R. 1985. Ontological promiscuity. In Proceedings of the 23rd annual meeting on\\nAssociation for Computational Linguistics . Association for Computational Linguistics, 60–69.\\nHoeksema, J. 1983. Plurality and conjunction. Studies in modeltheoretic semantics 1 , 63–83.\\nHornby, A. , Ed. 1980. Oxford Advanced Learner’s Dictionary of Current English . Vol. 1428.\\nCambridge Univ Press.\\nHovy, E. 2003. Using an ontology to simplify data access. Communications of the ACM 46, 1,\\n47–49.\\nHovy, E. ,Marcus, M. ,Palmer, M. ,Ramshaw, L. ,and Weischedel, R. 2006. Ontonotes: the\\n90% solution. In Proceedings of the Human Language Technology Conference of the NAACL,\\nCompanion Volume: Short Papers . Association for Computational Linguistics, 57–60.\\nHovy, E. ,Mitamura, T. ,Verdejo, F. ,Araki, J. ,and Philpot, A. 2013. Events are not simple:\\nIdentity, non-identity, and quasi-identity. In NAACL HLT, Workshop on Events: Deﬁnition,\\nDetection, Coreference and Representation . Vol. 2013. 21.\\nHumphreys, K. ,Gaizauskas, R. ,and Azzam, S. 1997. Event coreference for information ex-\\ntraction. In Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora\\nResolution for Unrestricted Texts . 75–81.\\nIkuta, R. ,Styler IV, W. F. ,Hamang, M. ,OGorman, T. ,and Palmer, M. 2014. Challenges\\nof adding causation to richer event descriptions. ACL 2014 , 12.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 54}), Document(page_content='56· ...\\nIpsen, I. C. and Wills, R. S. 2006. Mathematical properties and analysis of googles pagerank.\\nBol. Soc. Esp. Mat. Apl 34 , 191–196.\\nJackendoff, R. 1985. Semantics and cognition . Vol. 8. The MIT Press.\\nJoachims, T. 1998. Text categorization with support vector machines: Learning with many\\nrelevant features. Machine Learning: ECML-98 , 137–142.\\nJohnson, M. and Ural, A. E. 2010. Reranking the Berkeley and Brown parsers. In Human\\nLanguage Technologies: The 2010 Annual Conference of the North American Chapter of the\\nAssociation for Computational Linguistics . Association for Computational Linguistics, 665–\\n668.\\nJoshi, A. 1985. How much context sensitivity is necessary for characterizing structural descrip-\\ntions: Tree adjoining grammars. Natural language parsing: Psychological, computational and\\ntheoretical perspectives , 206–250.\\nJourde, J. ,Manine, A.-P. ,Veber, P. ,Fort, K. ,Bossy, R. ,Alphonse, E. ,and Bessieres, P.\\n2011. BioNLP Shared Task 2011: Bacteria Gene Interactions and Renaming. In Proceedings of\\nthe BioNLP Shared Task 2011 Workshop . Association for Computational Linguistics, 65–73.\\nJung, H. and Stent, A. 2013. Att1: Temporal annotation using big windows and rich syntactic\\nand semantic features. In Second Joint Conference on Lexical and Computational Semantics\\n(* SEM) . Vol. 2. 20–24.\\nJurafsky, D. and Martin, J. H. 2000. Speech and language processing: An introduction to\\nnatural language processing, computational linguistics, and speech recognition.\\nKaiser, G. 2011. A friendly guide to wavelets . Springer.\\nKatz, J. and Fodor, J. 1963. The structure of a semantic theory. Language 39, 2, 170–210.\\nKenny, A. 1963, 2003. Action, emotion and will . Psychology Press.\\nKim, J. ,Ohta, T. ,Pyysalo, S. ,Kano, Y. ,and Tsujii, J. 2009. Overview of bionlp’09 shared\\ntask on event extraction. In Proceedings of the Workshop on Current Trends in Biomedical\\nNatural Language Processing: Shared Task . Association for Computational Linguistics, 1–9.\\nKim, J. ,Ohta, T. ,Tateisi, Y. ,and Tsujii, J. 2003. Genia corpusa semantically annotated\\ncorpus for bio-textmining. Bioinformatics 19, suppl 1, i180–i182.\\nKim, J. ,Wang, Y. ,Takagi, T. ,and Yonezawa, A. 2011. Overview of Genia event task in\\nBioNLP Shared Task 2011. ACL HLT 2011 , 7.\\nKingsbury, P. and Palmer, M. 2002. From treebank to propbank. In Proceedings of the\\n3rd International Conference on Language Resources and Evaluation (LREC-2002) . Citeseer,\\n1989–1993.\\nKingsbury, P. and Palmer, M. 2003. Propbank: the next level of treebank. In Proceedings of\\nTreebanks and lexical Theories . Vol. 3.\\nKipper, K. ,Dang, H. ,and Palmer, M. 2000. Class-based construction of a verb lexicon. In\\nProceedings of the National Conference on Artiﬁcial Intelligence . Menlo Park, CA; Cambridge,\\nMA; London; AAAI Press; MIT Press; 1999, 691–696.\\nKipper, K. ,Dang, H. ,Schuler, W. ,and Palmer, M. 2000. Building a class-based verb lexicon\\nusing tags. In TAG+ 5 Fifth International Workshop on Tree Adjoining Grammars and Related\\nFormalisms . Citeseer, 147–154.\\nKipper, K. ,Korhonen, A. ,Ryant, N. ,and Palmer, M. 2008. A large-scale classiﬁcation of\\nenglish verbs. Language Resources and Evaluation 42, 1, 21–40.\\nKipper-Schuler, K. 2005. Verbnet: A broad-coverage, comprehensive verb lexicon, ph.d. disser-\\ntation, university of pennsylvania.\\nKleinberg, J. 2003. Bursty and hierarchical structure in streams. Data Mining and Knowledge\\nDiscovery 7, 4, 373–397.\\nKomodakis, N. ,Paragios, N. ,and Tziritas, G. 2007. Mrf optimization via dual decomposition:\\nMessage-passing revisited. In Computer Vision, 2007. ICCV 2007. IEEE 11th International\\nConference on . IEEE, 1–8.\\nKorhonen, A. and Briscoe, T. 2004. Extended lexical-semantic classiﬁcation of english verbs. In\\nProceedings of the HLT-NAACL Workshop on Computational Lexical Semantics . Association\\nfor Computational Linguistics, 38–45.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 55}), Document(page_content='...· 57\\nKozareva, Z. and Hovy, E. 2010. Learning arguments and supertypes of semantic relations\\nusing recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for\\nComputational Linguistics . Association for Computational Linguistics, 1482–1491.\\nKudo, T. and Matsumoto, Y. 2001. Chunking with support vector machines. In Proceedings\\nof the second meeting of the North American Chapter of the Association for Computational\\nLinguistics on Language technologies . 1–8.\\nLafferty, J. ,McCallum, A. ,and Pereira, F. C. 2001. Conditional random ﬁelds: Proba-\\nbilistic models for segmenting and labeling sequence data. In Proceedings of the International\\nConference on Machine Learning .\\nLee, H. ,Recasens, M. ,Chang, A. ,Surdeanu, M. ,and Jurafsky, D. 2012. Joint entity and\\nevent coreference resolution across documents. In Proceedings of the 2012 Joint Conference\\non Empirical Methods in Natural Language Processing and Computational Natural Language\\nLearning . 489–500.\\nLevin, B. , Ed. 1985. Lexical Semantics in Review . Lexicon Project Working Papers 1, Center\\nfor Cognitive Science, MIT, Cambridge, MA.\\nLevin, B. 1993. English verb classes and alternations: A preliminary investigation . Vol. 348.\\nUniversity of Chicago press Chicago, IL.\\nLevin, B. and Hovav, M. 1995. Unaccusativity: At the syntax-lexical semantics interface . Vol. 26.\\nThe MIT Press.\\nLewis, D. 1973. Causation. The journal of philosophy , 556–567.\\nLlorens, H. ,Saquete, E. ,and Navarro, B. 2010. Tipsem (english and spanish): Evaluating\\ncrfs and semantic roles in tempeval-2. In Proceedings of the 5th International Workshop on\\nSemantic Evaluation . Association for Computational Linguistics, 284–291.\\nLloyd, G. 1968. Aristotle: the growth and structure of his thought . Cambridge University Press.\\nLuo, X. 2005. On coreference resolution performance metrics. In Proceedings of the conference on\\nHuman Language Technology and Empirical Methods in Natural Language Processing . 25–32.\\nMacleod, C. ,Grishman, R. ,Meyers, A. ,Barrett, L. ,and Reeves, R. 1998. Nomlex: A\\nlexicon of nominalizations. In Proceedings of the 8th International Congress of the European\\nAssociation for Lexicography . Citeseer, 187–193.\\nMahesh, K. ,Nirenburg, S. ,et al. 1995. A situated ontology for practical nlp. In Proceedings of\\nthe IJCAI-95 Workshop on Basic Ontological Issues in Knowledge Sharing . Vol. 19. Citeseer,\\n21.\\nMani, I. and Wilson, G. 2000. Robust temporal processing of news. In Proceedings of the 38th\\nAnnual Meeting on Association for Computational Linguistics . Association for Computational\\nLinguistics, 69–76.\\nMarcus, M. ,Marcinkiewicz, M. ,and Santorini, B. 1993. Building a large annotated corpus\\nof english: The penn treebank. Computational linguistics 19, 2, 313–330.\\nMcClosky, D. 2010. Any domain parsing: automatic domain adaptation for natural language\\nparsing, ph.d. thesis.\\nMcClosky, D. ,Surdeanu, M. ,and Manning, C. D. 2011. Event extraction as dependency\\nparsing for bionlp 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop . Association\\nfor Computational Linguistics, 41–45.\\nMcDonald, R. and Pereira, F. 2006. Online learning of approximate dependency parsing\\nalgorithms. In Proceedings of EACL . Vol. 6. 81–88.\\nMcDonald, R. ,Pereira, F. ,Ribarov, K. ,and Haji ˇc, J. 2005. Non-projective dependency\\nparsing using spanning tree algorithms. In Proceedings of the conference on Human Language\\nTechnology and Empirical Methods in Natural Language Processing . Association for Computa-\\ntional Linguistics, 523–530.\\nMcNamee, P. ,Dang, H. T. ,Simpson, H. ,Schone, P. ,and Strassel, S. 2010. An evaluation of\\ntechnologies for knowledge base population. In Language Resources and Evaluation Conference\\n(LREC) .\\nMenzies, P. 1999. Intrinsic versus extrinsic conceptions of causation. In Causation and laws of\\nnature . Springer, 313–329.\\nMenzies, P. 2008. Counterfactual theories of causation.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 56}), Document(page_content='58· ...\\nMeyers, A. ,Grishman, R. ,Kosaka, M. ,and Zhao, S. 2001. Covering treebanks with glarf. In\\nProceedings of the ACL 2001 Workshop on Sharing Tools and Resources-Volume 15 . Associa-\\ntion for Computational Linguistics, 51–58.\\nMeyers, A. ,Macleod, C. ,Yangarber, R. ,Grishman, R. ,Barrett, L. ,Reeves, R. ,et al.\\n1998. Using nomlex to produce nominalization patterns for information extraction. In Proceed-\\nings: the Computational Treatment of Nominals, Montreal, Canada,(Coling-ACL98 workshop) .\\nVol. 2.\\nMiller, G. 1995. Wordnet: a lexical database for english. Communications of the ACM 38, 11,\\n39–41.\\nMoens, M. 1987. Tense, aspect and temporal reference, ph.d. dissertation.\\nMoens, M. and Steedman, M. 1988. Temporal ontology and temporal reference. Computational\\nlinguistics 14, 2, 15–28.\\nMoreda, P. ,Navarro, B. ,and Palomar, M. 2007. Corpus-based semantic role approach in\\ninformation retrieval. Data & Knowledge Engineering 61, 3, 467–483.\\nMourelatos, A. 1978. Events, processes, and states. Linguistics and philosophy 2, 3, 415–434.\\nNapoles, C. ,Gormley, M. ,and Van Durme, B. 2012. Annotated gigaword. In Proceedings of\\nthe Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge\\nExtraction . Association for Computational Linguistics, 95–100.\\nNaughton, M. ,Stokes, N. ,and Carthy, J. 2008. Investigating statistical techniques for\\nsentence-level event classiﬁcation. In Proceedings of the 22nd International Conference on\\nComputational Linguistics-Volume 1 . Association for Computational Linguistics, 617–624.\\nNewman, M. E. 2004. Fast algorithm for detecting community structure in networks. Physical\\nreview E 69, 6, 066133.\\nNewman, M. E. 2006. Modularity and community structure in networks. Proceedings of the\\nNational Academy of Sciences 103, 23, 8577–8582.\\nNiles, I. and Pease, A. 2001. Towards a standard upper ontology. In Proceedings of the inter-\\nnational conference on Formal Ontology in Information Systems-Volume 2001 . ACM, 2–9.\\nOhta, T. ,Pyysalo, S. ,and Tsujii, J. 2011. Overview of the epigenetics and post-translational\\nmodiﬁcations (epi) task of bionlp shared task 2011. In Proceedings of the BioNLP Shared Task\\n2011 Workshop . Association for Computational Linguistics, 16–25.\\nOngenaert, M. ,Van Neste, L. ,De Meyer, T. ,Menschaert, G. ,Bekaert, S. ,and\\nVan Criekinge, W. 2008. Pubmeth: a cancer methylation database combining text-mining\\nand expert annotation. Nucleic Acids Research 36, suppl 1, D842–D846.\\nOrfanidis, S. J. 1985. Optimum signal processing: an introduction . Macmillan New York.\\nPalmer, M. ,Babko-Malaya, O. ,and Dang, H. 2004. Diﬀerent sense granularities for diﬀerent\\napplications. In Proceedings of Workshop on Scalable Natural Language Understanding .\\nPalmer, M. ,Dang, H. ,and Fellbaum, C. 2007. Making ﬁne-grained and coarse-grained sense\\ndistinctions, both manually and automatically. Natural Language Engineering 13, 2, 137.\\nPalmer, M. ,Gildea, D. ,and Kingsbury, P. 2005. The proposition bank: An annotated corpus\\nof semantic roles. Computational Linguistics 31, 1, 71–106.\\nParsons, T. 1990. Events in the Semantics of English . MIT Pr.\\nPerlmutter, D. M. 1978. mpersonal passives and the unaccusative hypothesis. In Proc. of the\\n4th Annual Meeting of the Berkeley Linguistics Society . UC Berkeley, 157–189.\\nPetruck, M. 1996. Frame semantics. Handbook of pragmatics , 1–13.\\nPhilpot, A. ,Hovy, E. ,and Pantel, P. 2005. The omega ontology. In Proceedings of the\\nONTOLEX Workshop at the International Conference on Natural Language Processing (IJC-\\nNLP) .\\nPinker, S. 1989. Learnability and cognition: The acquisition of argument structure. The MIT\\nPress.\\nPorter, M. F. 1980. An algorithm for suﬃx stripping. Program: electronic library and informa-\\ntion systems 14, 3, 130–137.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 57}), Document(page_content='...· 59\\nPradhan, S. ,Ramshaw, L. ,Marcus, M. ,Palmer, M. ,Weischedel, R. ,and Xue, N. 2011.\\nConll-2011 shared task: Modeling unrestricted coreference in ontonotes. In Proceedings of the\\nFifteenth Conference on Computational Natural Language Learning: Shared Task . 1–27.\\nPradhan, S. S. ,Ramshaw, L. ,Weischedel, R. ,MacBride, J. ,and Micciulla, L. 2007. Un-\\nrestricted coreference: Identifying entities and events in ontonotes. In Semantic Computing,\\n2007. ICSC 2007. International Conference on . IEEE, 446–453.\\nPradhan, S. S. ,Ward, W. ,Hacioglu, K. ,Martin, J. H. ,and Jurafsky, D. 2004. Shallow\\nsemantic parsing using support vector machines. In HLT-NAACL . 233–240.\\nPrasad, R. ,Dinesh, N. ,Lee, A. ,Miltsakaki, E. ,Robaldo, L. ,Joshi, A. ,and Webber, B.\\n2008. The penn discourse treebank 2.0. In Proceedings of the 6th International Conference on\\nLanguage Resources and Evaluation (LREC 2008) . Citeseer, 2961.\\nPrescher, D. ,Riezler, S. ,and Rooth, M. 2000. Using a probabilistic class-based lexicon for lex-\\nical ambiguity resolution. In Proceedings of the 18th conference on Computational linguistics-\\nVolume 2 . Association for Computational Linguistics, 649–655.\\nPunyakanok, V. ,Roth, D. ,Yih, W.-t. ,Zimak, D. ,and Tu, Y. 2004. Semantic role labeling\\nvia generalized inference over classiﬁers. 130–133.\\nPustejovsky, J. 1991a. The generative lexicon. Computational linguistics 17, 4, 409–441.\\nPustejovsky, J. 1991b. The syntax of event structure. Cognition 41, 1, 47–81.\\nPustejovsky, J. ,Castano, J. ,Ingria, R. ,Sauri, R. ,Gaizauskas, R. ,Setzer, A. ,Katz, G. ,\\nand Radev, D. 2003. Timeml: Robust speciﬁcation of event and temporal expressions in text.\\nNew Directions in Question Answering 2003 , 28–34.\\nPustejovsky, J. ,Hanks, P. ,Sauri, R. ,See, A. ,Gaizauskas, R. ,Setzer, A. ,Radev, D. ,\\nSundheim, B. ,Day, D. ,Ferro, L. ,et al. 2003. The Timebank corpus. In Corpus Linguistics .\\nVol. 2003. 40.\\nPustejovsky, J. ,Meyers, A. ,Palmer, M. ,and Poesio, M. 2005. Merging propbank, nom-\\nbank, timebank, penn discourse treebank and coreference. In Proceedings of the Workshop on\\nFrontiers in Corpus Annotations II: Pie in the Sky . Association for Computational Linguistics,\\n5–12.\\nPyysalo, S. ,Ohta, T. ,Rak, R. ,Sullivan, D. ,Mao, C. ,Wang, C. ,Sobral, B. ,Tsujii, J. ,and\\nAnaniadou, S. 2011. Overview of the infectious diseases (id) task of bionlp shared task 2011.\\nInProceedings of the BioNLP Shared Task 2011 Workshop . Association for Computational\\nLinguistics, 26–35.\\nQuillian, M. R. 1968. Semantic Information Processing, . The MIT Press, Cambridge, MA,\\nChapter Semantic Memory, or CHAPTER.\\nQuine, W. V. 1956. Quantiﬁers and propositional attitudes. the Journal of Philosophy , 177–187.\\nRabiner, L. R. 1989. A tutorial on hidden markov models and selected applications in speech\\nrecognition. Proceedings of the IEEE 77, 2, 257–286.\\nRamshaw, L. A. and Marcus, M. P. 1995. Text chunking using transformation-based learning.\\nThird Workshop on Very Large Corpora (WVLC-3) at the Annual Meeting of the Association\\nfor Computational Linguistics , 82–94.\\nRappaport, M. and Levin, B. 1988. What to do with theta-roles in thematic relations. Syntax\\nand semantics 21 , 7–36.\\nRastogi, P. and Van Durme, B. 2014. Augmenting framenet via ppdb. NAACL 2014, Workshop\\non Events: Deﬁnition, Detection, Coreference and Representation , 1.\\nRatnaparkhi, A. 1999. Learning to parse natural language with maximum entropy models.\\nMachine learning 34, 1-3, 151–175.\\nRecasens, M. and Hovy, E. 2011. Blanc: Implementing the rand index for coreference evaluation.\\nNatural Language Engineering 17, 04, 485–510.\\nRiedel, S. ,Chun, H.-W. ,Takagi, T. ,and Tsujii, J. 2009. A markov logic approach to bio-\\nmolecular event extraction. In Proceedings of the Workshop on Current Trends in Biomedical\\nNatural Language Processing: Shared Task . Association for Computational Linguistics, 41–49.\\nRiedel, S. and McCallum, A. 2011. Robust biomedical event extraction with dual decomposition\\nand minimal domain adaptation. In Proceedings of the BioNLP Shared Task 2011 Workshop .\\nAssociation for Computational Linguistics, 46–50.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 58}), Document(page_content='60· ...\\nRiedel, S. ,McClosky, D. ,Surdeanu, M. ,McCallum, A. ,and Manning, C. D. 2011. Model\\ncombination for event extraction in bionlp 2011. In Proceedings of the BioNLP Shared Task\\n2011 Workshop . Association for Computational Linguistics, 51–55.\\nRigo, S. and Lavelli, A. 2011. Multisex-a multi-language timex sequential extractor. In 2011\\nEighteenth International Symposium on Temporal Representation and Reasoning . 163–170.\\nRitter, A. ,Clark, S. ,Etzioni, O. ,et al. 2011. Named entity recognition in tweets: an exper-\\nimental study. In Proceedings of the Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, 1524–1534.\\nRitter, A. ,Etzioni, O. ,et al. 2010. A latent dirichlet allocation method for selectional pref-\\nerences. In Proceedings of the 48th Annual Meeting of the Association for Computational\\nLinguistics . Association for Computational Linguistics, 424–434.\\nRitter, A. ,Etzioni, O. ,Clark, S. ,et al. 2012. Open domain event extraction from twitter. In\\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and\\ndata mining . ACM, 1104–1112.\\nRitter, E. and Rosen, S. 1996. Strong and weak predicates: Reducing the lexical burden.\\nLinguistic Analysis 26, 1-2, 29–62.\\nRoberts, K. and Harabagiu, S. M. 2011. Unsupervised learning of selectional restrictions and\\ndetection of argument coercions. In Proceedings of the Conference on Empirical Methods in\\nNatural Language Processing . Association for Computational Linguistics, 980–990.\\nRobkop, K. ,Thoongsup, S. ,Charoenporn, T. ,Sornlertlamvanich, V. ,and Isahara, H.\\n2010. Wnms: Connecting the distributed wordnet in the case of asian wordnet. In Princi-\\nples, Construction, and Applications of Multilingual Wordnets. Proceedings of the Fifth Global\\nWordNet Conference (GWC 2010), India. Narosa Publishing .\\nRosen, C. 1984. The interface between semantic roles and initial grammatical relations. Studies\\nin relational grammar 2, 38-77.\\nRosen, S. 1996. Events and verb classiﬁcation. Linguistics 34 , 191–223.\\nRosen, S. 1999. The syntactic representation of linguistic events. Glot International 4, 2, 3–11.\\nRudinger, R. and Van Durme, B. 2014. Is the stanford dependency representation semantic?\\nACL 2014 , 54.\\nRuppenhofer, J. ,Ellsworth, M. ,Petruck, M. ,Johnson, C. ,and Scheffczyk, J. 2006.\\nFramenet ii: Extended theory and practice. International Computer Science Institute .\\nRush, A. M. ,Sontag, D. ,Collins, M. ,and Jaakkola, T. 2010. On dual decomposition and\\nlinear programming relaxations for natural language processing. In Proceedings of the 2010\\nConference on Empirical Methods in Natural Language Processing . Association for Computa-\\ntional Linguistics, 1–11.\\nSager, N. 1981. Natural language information processing . Addison-Wesley Publishing Company,\\nAdvanced Book Program.\\nSameer, S. ,Hovy, E. ,Marcus, M. ,Palmer, M. ,Ramshaw, L. ,and Weischedel, R. 2007.\\nOntonotes: A uniﬁed relational semantic representation. International Journal of Semantic\\nComputing 1, 04, 405–419.\\nSanfilippo, A. 1994. Lkb encoding of lexical knowledge. In Inheritance, defaults and the lexicon .\\nCambridge University Press, 190–222.\\nSaur´ı, R. ,Knippen, R. ,Verhagen, M. ,and Pustejovsky, J. 2005. Evita: a robust event\\nrecognizer for qa systems. In Proceedings of the conference on Human Language Technology and\\nEmpirical Methods in Natural Language Processing . Association for Computational Linguistics,\\n700–707.\\nSauri, R. ,Littman, J. ,Knippen, B. ,Gaizauskas, R. ,Setzer, A. ,and Pustejovsky, J. 2005.\\nTimeml annotation guidelines, http://www.timeml.org/timemldocs/annguide14.pdf.\\nSchabes, Y. 1990. Mathematical and computational aspects of lexicalized grammars, ph.d. dis-\\nsertation, university of pennsylvania.\\nSchuler, K. 2005. Verbnet: A broad-coverage, comprehensive verb lexicon. Ph.D. thesis, Disser-\\ntations available from ProQuest, University of Pennsylvania.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 59}), Document(page_content='...· 61\\nS´eaghdha, D. O. 2010. Latent variable models of selectional preference. In Proceedings of\\nthe 48th Annual Meeting of the Association for Computational Linguistics . Association for\\nComputational Linguistics, 435–444.\\nSetzer, A. 2001. Temporal information in newswire articles: an annotation scheme and corpus\\nstudy. Ph.D. thesis, University of Sheﬃeld Sheﬃeld, UK.\\nSetzer, A. and Gaizauskas, R. J. 2000. Annotating events and temporal information in newswire\\ntexts. In LREC . Vol. 2000. 1287–1294.\\nShi, L. and Mihalcea, R. 2005. Putting pieces together: Combining framenet, verbnet and word-\\nnet for robust semantic parsing. Computational Linguistics and Intelligent Text Processing ,\\n100–111.\\nSinha, M. ,Reddy, M. ,and Bhattacharyya, P. 2006. An approach towards construction and\\napplication of multilingual indo-wordnet. In 3rd Global Wordnet Conference (GWC 06), Jeju\\nIsland, Korea .\\nSmith, C. 1997. The parameter of aspect . Vol. 43. Springer.\\nSornlertlamvanich, V. ,Charoenporn, T. ,Robkop, K. ,Mokarat, C. ,and Isahara, H. 2009.\\nReview on development of asian wordnet.\\nStyler IV, W. F. ,Bethard, S. ,Finan, S. ,Palmer, M. ,Pradhan, S. ,de Groen, P. C. ,\\nErickson, B. ,Miller, T. ,Lin, C. ,Savova, G. ,et al. 2014. Temporal annotation in the\\nclinical domain. Transactions of the Association for Computational Linguistics 2 , 143–154.\\nSwier, R. and Stevenson, S. 2004. Unsupervised semantic role labelling. In Proceedings of the\\n2004 Conference on Empirical Methods in Natural Language Processing . 95–102.\\nSwift, M. 2005. Towards automatic verb acquisition from verbnet for spoken dialog processing.\\nInProceedings of the Interdisciplinary Workshop on the Identiﬁcation and Representation of\\nVerb Features and Verb Classes . 115–120.\\nTalmy, L. 1985. Lexicalization patterns: Semantic structure in lexical forms. Language typology\\nand syntactic description 3 , 57–149.\\nTeh, Y. W. ,Jordan, M. I. ,Beal, M. J. ,and Blei, D. M. 2006. Hierarchical dirichlet processes.\\nJournal of the american statistical association 101, 476.\\nTenny, C. 1994. Aspectual roles and the syntax-semantics interface . Vol. 52. Springer Verlag.\\nTer Meulen, A. 1983. The representation of time in natural language. Studies in modeltheoretic\\nsemantics, Dordrecht, Foris Publications , 177–191.\\nTer Meulen, A. 1997. Representing time in natural language: The dynamic interpretation of\\ntense and aspect . The MIT Press.\\nTsochantaridis, I. ,Joachims, T. ,Hofmann, T. ,Altun, Y. ,and Singer, Y. 2006. Large margin\\nmethods for structured and interdependent output variables. Journal of Machine Learning\\nResearch 6, 2, 1453.\\nUzZaman, N. and Allen, J. F. 2010. Trips and trios system for tempeval-2: Extracting tem-\\nporal information from text. In Proceedings of the 5th International Workshop on Semantic\\nEvaluation . 276–283.\\nUzZaman, N. ,Llorens, H. ,Allen, J. ,Derczynski, L. ,Verhagen, M. ,and Pustejovsky, J.\\n2012. Tempeval-3: Evaluating events, time expressions, and temporal relations. arXiv preprint\\narXiv:1206.5333 .\\nVan Gael, J. ,Saatci, Y. ,Teh, Y. W. ,and Ghahramani, Z. 2008. Beam sampling for the\\ninﬁnite hidden markov model. In Proceedings of the 25th international conference on Machine\\nlearning . 1088–1095.\\nvan Voorst, J. 1988. Event structure . Vol. 59. John Benjamins Publishing Co.\\nVapnik, V. 1995. The nature of statistical learning theory. Data mining and knowledge discov-\\nery 6 , 1–47.\\nVendler, Z. 1967. Linguistics in philosophy . Cornell University Press Ithaca.\\nVerhagen, M. ,Sauri, R. ,Caselli, T. ,and Pustejovsky, J. 2010. Semeval-2010 task 13:\\nTempeval-2. In Proceedings of the 5th international workshop on semantic evaluation . Associ-\\nation for Computational Linguistics, 57–62.\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 60}), Document(page_content='62· ...\\nVerkuyl, H. 1996. A theory of aspectuality: The interaction between temporal and atemporal\\nstructure . Vol. 64. Cambridge University Press.\\nVilain, M. ,Burger, J. ,Aberdeen, J. ,Connolly, D. ,and Hirschman, L. 1995. A model-\\ntheoretic coreference scoring scheme. In Proceedings of the 6th conference on Message under-\\nstanding . 45–52.\\nVlach, F. 1981. The semantics of the progressive in tense and aspect. ed. by philip tedeschi and\\nannie zaenen.\\nVossen, P. 1998a. EuroWordNet: a multilingual database with lexical semantic networks . Kluwer\\nAcademic.\\nVossen, P. 1998b. Introduction to eurowordnet. Computers and the Humanities 32, 2, 73–89.\\nVossen, P. 2004. Eurowordnet: a multilingual database of autonomous and language-speciﬁc\\nwordnets connected via an inter-lingualindex. International Journal of Lexicography 17, 2,\\n161–173.\\nWeischedel, R. 2011. OntoNotes Release 4.0 . Linguistic Data Consortium, University of Pennsyl-\\nvania, http://www.ldc.upenn.edu/Catalog/docs/LDC2011T03/OntoNotes-Release-4.0.pdf .\\nWeng, J. and Lee, B.-S. 2011. Event detection in twitter. In ICWSM .\\nWilks, Y. 1975. A preferential, pattern-seeking, semantics for natural language inference. Arti-\\nﬁcial Intelligence 6, 1, 53–74.\\nWilliams, E. 1981. Argument structure and morphology. The linguistic review 1, 1, 81–114.\\nWinston, M. E. ,Chaffin, R. ,and Herrmann, D. 1987. A taxonomy of part-whole relations.\\nCognitive science 11, 4, 417–444.\\nWu, C. H. ,Yeh, L.-S. L. ,Huang, H. ,Arminski, L. ,Castro-Alvear, J. ,Chen, Y. ,Hu, Z. ,\\nKourtesis, P. ,Ledley, R. S. ,Suzek, B. E. ,et al. 2003. The protein information resource.\\nNucleic acids research 31, 1, 345–347.\\nYang, Y. ,Pierce, T. ,and Carbonell, J. 1998. A study of retrospective and on-line event\\ndetection. In Proceedings of the 21st annual international ACM SIGIR conference on Research\\nand development in information retrieval . 28–36.\\nYao, L. ,Haghighi, A. ,Riedel, S. ,and McCallum, A. 2011. Structured relation discovery\\nusing generative models. In Proceedings of the Conference on Empirical Methods in Natural\\nLanguage Processing . 1456–1466.\\nYao, L. ,Mimno, D. ,and McCallum, A. 2009. Eﬃcient methods for topic model inference\\non streaming document collections. In Proceedings of the 15th ACM SIGKDD international\\nconference on Knowledge discovery and data mining . 937–946.\\nZhang, T. ,Damerau, F. ,and Johnson, D. 2002. Text chunking based on a generalization of\\nwinnow. The Journal of Machine Learning Research 2 , 615–637.\\n...\\nACM Transactions, Vol. V, No. N, January 2016.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.04012.pdf', 'page': 61}), Document(page_content='1\\nSpatial Concept Acquisition for a Mobile Robot\\nthat Integrates Self-Localization and Unsupervised\\nWord Discovery from Spoken Sentences\\nAkira Taniguchi, Tadahiro Taniguchi, Member, IEEE, and Tetsunari Inamura, Member, IEEE,\\nAbstract —In this paper, we propose a novel unsupervised\\nlearning method for the lexical acquisition of words related\\nto places visited by robots, from human continuous speech\\nsignals. We address the problem of learning novel words by\\na robot that has no prior knowledge of these words except\\nfor a primitive acoustic model. Further, we propose a method\\nthat allows a robot to effectively use the learned words and\\ntheir meanings for self-localization tasks. The proposed method\\nis nonparametric Bayesian spatial concept acquisition method\\n(SpCoA) that integrates the generative model for self-localization\\nand the unsupervised word segmentation in uttered sentences via\\nlatent variables related to the spatial concept. We implemented\\nthe proposed method SpCoA on SIGVerse, which is a simulation\\nenvironment, and TurtleBot 2, which is a mobile robot in a real\\nenvironment. Further, we conducted experiments for evaluating\\nthe performance of SpCoA. The experimental results showed\\nthat SpCoA enabled the robot to acquire the names of places\\nfrom speech sentences. They also revealed that the robot could\\neffectively utilize the acquired spatial concepts and reduce the\\nuncertainty in self-localization.\\nIndex Terms —Learning place names, lexical acquisition, self-\\nlocalization, spatial concept\\nI. I NTRODUCTION\\nAUTONOMOUS robots, such as service robots, operating\\nin the human living environment with humans have to\\nbe able to perform various tasks and language communication.\\nTo this end, robots are required to acquire novel concepts\\nand vocabulary on the basis of the information obtained from\\ntheir sensors, e.g., laser sensors, microphones, and cameras,\\nand recognize a variety of objects, places, and situations in\\nan ambient environment. Above all, we consider it important\\nfor the robot to learn the names that humans associate with\\nplaces in the environment and the spatial areas corresponding\\nto these names; i.e., the robot has to be able to understand\\nwords related to places. Therefore, it is important to deal with\\nconsiderable uncertainty, such as the robot’s movement errors,\\nsensor noise, and speech recognition errors.\\nSeveral studies on language acquisition by robots have\\nassumed that robots have no prior lexical knowledge. These\\nstudies differ from speech recognition studies based on a large\\nvocabulary and natural language processing studies based on\\nlexical, syntactic, and semantic knowledge [1], [2]. Studies on\\nAkira Taniguchi and Tadahiro Taniguchi are with Ritsumeikan\\nUniversity, 1-1-1 Noji Higashi, Kusatsu, Shiga 525-8577, Japan (e-\\nmail:a.taniguchi@em.ci.ritsumei.ac.jp; taniguchi@em.ci.ritsumei.ac.jp).\\nTetsunari Inamura is with National Institute of Informatics/The Graduate\\nUniversity for Advanced Studies, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-\\n8430, Japan (e-mail:inamura@nii.ac.jp).\\nTaniguchi\\nLaboratory\\n Teachings of multiple\\nTaniguchi\\nLaboratory\\n“Where is this place?”\\nTaniguchi\\nLaboratory\\nLaborator y\\n“This is \\nthe front of TV.”\\n/dis iz afroqtabutibe/ ?Teaching“Here is\\nthe front of TV .”\\n“This place is\\n white shelf .”\\nLearning\\nUtilization of spatial concepts\\n??\\n!Place of learning target\\nBefore the modification of localization\\nAfter the modification of localization/afroqtabutibe/\\n/waitosherfu/\\n/bigbuqkkais/(c)(a) (b)\\n(d)\\n(e)\\nFig. 1. Schematic representation of the target task: (a) Learning targets are\\nthree places near the objects. (b) When an utterer and a robot came in front\\nof the TV , the utterer spoke “Here is the front of TV . ” The same holds true\\nfor the other places. (c) The robot performs word discovery from the uttered\\nsentences and learns the related spatial concepts. Words are related to each\\nplace. (d) The robot is in front of TV actually. However, the hypothesis of the\\nself-position of the robot is uncertain. Then, the robot asks a neighbor what\\nthe current place is. (e) By utilizing spatial concepts and an uttered sentence,\\nthe robot can narrow down the hypothesis of self-position.\\nlanguage acquisition by robots also constitute a constructive\\napproach to the human developmental process and the emer-\\ngence of symbols.\\nThe objectives of this study were to build a robot that learns\\nwords related to places and efﬁciently utilizes this learned\\nvocabulary in self-localization. Lexical acquisition related to\\nplaces is expected to enable a robot to improve its spatialarXiv:1602.01208v3  [cs.AI]  7 May 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 0}), Document(page_content='2\\ncognition. A schematic representation depicting the target task\\nof this study is shown in Fig. 1. This study assumes that\\na robot does not have any vocabularies in advance but can\\nrecognize syllables or phonemes. The robot then performs\\nself-localization while moving around in the environment, as\\nshown in Fig. 1 (a). An utterer speaks a sentence including\\nthe name of the place to the robot, as shown in Fig. 1 (b). For\\nthe purposes of this study, we need to consider the problems\\nof self-localization and lexical acquisition simultaneously.\\nWhen a robot learns novel words from utterances, it is\\ndifﬁcult to determine segmentation boundaries and the identity\\nof different phoneme sequences from the speech recognition\\nresults, which can lead to errors. First, let us consider the case\\nof the lexical acquisition of an isolated word. For example, if a\\nrobot obtains the speech recognition results “aporu” ,“epou” ,\\nand “aqpuru” (incorrect phoneme recognition of apple ), it\\nis difﬁcult for the robot to determine whether they denote the\\nsame referent without prior knowledge. Second, let us consider\\na case of the lexical acquisition of the utterance of a sentence.\\nFor example, a robot obtains a speech recognition result, such\\nas“thisizanaporu. ” The robot has to necessarily segment a\\nsentence into individual words, e.g., “this” ,“iz”,“an” , and\\n“aporu” . In addition, it is necessary for the robot to recognize\\nwords referring to the same referent, e.g., the fruit apple ,\\nfrom among the many segmented results that contain errors. In\\ncase of Fig. 1 (c), there is some possibility of learning names\\nincluding phoneme errors, e.g., “afroqtabutibe, ” because the\\nrobot does not have any lexical knowledge.\\nOn the other hand, when a robot performs online probabilis-\\ntic self-localization, we assume that the robot uses sensor data\\nand control data, e.g., values obtained using a range sensor\\nand odometry. If the position of the robot on the global map\\nis unclear, the difﬁculties associated with the identiﬁcation\\nof the self-position by only using local sensor information\\nbecome problematic. In the case of global localization using\\nlocal information, e.g., a range sensor, the problem that\\nthe hypothesis of self-position is present in multiple remote\\nlocations, frequently occurs, as shown in Fig. 1 (d).\\nIn order to solve the abovementioned problems, in this\\nstudy, we adopted the following approach. An utterance is\\nrecognized as not a single phoneme sequence but a set of\\ncandidates of multiple phonemes. We attempt to suppress the\\nvariability in the speech recognition results by performing\\nword discovery taking into account the multiple candidates\\nof speech recognition. In addition, the names of places are\\nlearned by associating with words and positions. The lexical\\nacquisition is complemented by using certain particular spatial\\ninformation; i.e., this information is obtained by hearing utter-\\nances including the same word in the same place many times.\\nFurthermore, in this study, we attempt to address the problem\\nof the uncertainty of self-localization by improving the self-\\nposition errors by using a recognized utterance including the\\nname of the current place and the acquired spatial concepts,\\nas shown in Fig. 1 (e).\\nIn this paper, we propose nonparametric Bayesian spatial\\nconcept acquisition method (SpCoA) on basis of unsupervised\\nword segmentation and a nonparametric Bayesian generative\\nmodel that integrates self-localization and a clustering in bothwords and places. The main contributions of this paper are as\\nfollows:\\n•We have proposed a learning method for spatial concepts\\nthat can perform the lexical acquisition related to places,\\ni.e., the names of places, from a continuous speech signal\\nin an unsupervised manner.\\n•We have achieved relatively accurate lexical acquisition\\nthat reduced the variability and errors in phonemes by\\nperforming word discovery using the multiple candidates\\nof the speech recognition results, i.e., by using a lattice\\nformat.\\n•In addition to the general self-localization method of\\nmobile robots, we showed that self-localization by the\\nproposed method can reduce the uncertainty of self-\\nposition by utilizing the learned spatial concepts and an\\nuttered sentence about the current position.\\nThe remainder of this paper is organized as follows: In\\nSection II, previous studies on language acquisition and lexical\\nacquisition relevant to our study are described. In Section III,\\nthe proposed method SpCoA is presented. In Sections IV and\\nV, we discuss the effectiveness of SpCoA in the simulation\\nand in the real environment. Section VI concludes this paper.\\nII. R ELATED WORKS\\nA. Lexical acquisition\\nMost studies on lexical acquisition typically focus on lexi-\\ncons about objects [1], [3]–[11]. Many of these studies have\\nnot be able to address the lexical acquisition of words other\\nthan those related to objects, e.g., words about places.\\nRoy et al. proposed a computational model that enables\\na robot to learn the names of objects from an object im-\\nage and spontaneous infant-directed speech [1]. Their results\\nshowed that the model performed speech segmentation, word\\ndiscovery, and visual categorization. Iwahashi et al. reported\\nthat a robot properly understands the situation and acquires\\nthe relationship of object behaviors and sentences [3]–[5]. Qu\\n& Chai focused on the conjunction between speech and eye\\ngaze and the use of domain knowledge in lexical acquisition\\n[7], [8]. They proposed an unsupervised learning method that\\nautomatically acquires novel words for an interactive system.\\nQu & Chai’s method based on the IBM translation model [12]\\nestimates the word-entity association probability.\\nNakamura et al. proposed a method to learn object concepts\\nand word meanings from multimodal information and verbal\\ninformation [10]. The method proposed in [10] is a categoriza-\\ntion method based on multimodal latent Dirichlet allocation\\n(MLDA) that enables the acquisition of object concepts from\\nmultimodal information, such as visual, auditory, and haptic\\ninformation [13]. Araki et al. addressed the development of\\na method combining unsupervised word segmentation from\\nuttered sentences by a nested Pitman-Yor language model\\n(NPYLM) [14] and the learning of object concepts by MLDA\\n[11]. However, the disadvantage of using NPYLM was that\\nphoneme sequences with errors did not result in appropriate\\nword segmentation.\\nThese studies did not address the lexical acquisition of\\nthe space and place that can also tolerate the uncertainty of', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 1}), Document(page_content='3\\nphoneme recognition. However, for the introduction of robots\\ninto the human living environment, robots need to acquire a\\nlexicon related to not only objects but also places. Our study\\nfocuses on the lexical acquisition related to places. Robots can\\nadaptively learn the names of places in various human living\\nenvironments by using SpCoA. We consider that the acquired\\nnames of places can be useful for various tasks, e.g., tasks\\nwith a movement of robots by the speech instruction.\\nB. Simultaneous learning of places and vocabulary\\nThe following studies have addressed lexical acquisition\\nrelated to places. However, these studies could not utilize the\\nlearned language knowledge in other estimations such as the\\nself-localization of a robot.\\nTaguchi et al. proposed a method for the unsupervised\\nlearning of phoneme sequences and relationships between\\nwords and objects from various user utterances without any\\nprior linguistic knowledge other than an acoustic model of\\nphonemes [2], [15]. Further, they proposed a method for the\\nsimultaneous categorization of self-position coordinates and\\nlexical learning [16]. These experimental results showed that\\nit was possible to learn the name of a place from utterances\\nin some cases and to output words corresponding to places in\\na location that was not used for learning.\\nMilford et al. proposed RatSLAM inspired by the biological\\nknowledge of a pose cell of the hippocampus of rodents [17].\\nMilford et al. proposed a method that enables a robot to\\nacquire spatial concepts by using RatSLAM [18]. Further,\\nLingodroids, mobile robots that learn a language through\\nrobot-to-robot communication, have been studied [19]–[21].\\nHere, a robot communicated the name of a place to other\\nrobots at various locations. Experimental results showed that\\ntwo robots acquired the lexicon of places that they had in\\ncommon. In [21], the researchers showed that it was possible\\nto learn temporal concepts in a manner analogous to the\\nacquisition of spatial concepts. These studies reported that the\\nrobots created their own vocabulary. However, these studies\\ndid not consider the acquisition of a lexicon by human-to-\\nrobot speech interactions.\\nWelke et al. proposed a method that acquires spatial rep-\\nresentation by the integration of the representation of the\\ncontinuous state space on the sensorimotor level and the\\ndiscrete symbolic entities used in high-level reasoning [22].\\nThis method estimates the probable spatial domain and word\\nfrom the given objects by using the spatial lexical knowledge\\nextracted from Google Corpus and the position information\\nof the object. Their study is different from ours because their\\nstudy did not consider lexicon learning from human speech.\\nIn the case of global localization, the hypothesis of self-\\nposition often remains in multiple remote places. In this\\ncase, there is some possibility of performing an incorrect\\nestimation and increasing the estimation error. This problem\\nexists during teaching tasks and self-localization after the\\nlexical acquisition. The abovementioned studies could not deal\\nwith this problem. In this paper, we have proposed a method\\nthat enables a robot to perform more accurate self-localization\\nby reducing the estimation error of the teaching time by usinga smoothing method in the teaching task and by utilizing words\\nacquired through the lexical acquisition. The strengths of this\\nstudy are that learning of spatial concept and self-localization\\nrepresented as one generative model and robots are able to\\nutilize acquired lexicon to self-localization autonomously.\\nIII. S PATIAL CONCEPT ACQUISITION\\nWe propose nonparametric Bayesian spatial concept ac-\\nquisition method (SpCoA) that integrates a nonparametric\\nmorphological analyzer for the lattice [23], i.e., latticelm1, a\\nspatial clustering method, and Monte Carlo localization (MCL)\\n[24].\\nA. Generative model\\nIn our study, we deﬁne a position as a speciﬁc coordinate or\\na local point in the environment, and the position distribution\\nas the spatial area of the environment. Further, we deﬁne\\naspatial concept as the names of places and the position\\ndistributions corresponding to these names.\\nThe model that was developed for spatial concept acquisi-\\ntion is a probabilistic generative model that integrates a self-\\nlocalization with the simultaneous clustering of places and\\nwords. Fig. 2 shows the graphical model for spatial concept\\nacquisition. Table I shows each variable of the graphical\\nmodel. The number of words in a sentence at time tis denoted\\nasBt. The generative model of the proposed method is deﬁned\\nas equation (1-10).\\nπ∼GEM(γ) (1)\\nCt∼Mult(π) (2)\\nW∼Dir(β0) (3)\\nOt,b∼Mult(WCt) (4)\\nφl∼GEM(α) (5)\\nit∼p(it|xt,µ,Σ,φl,Ct) (6)\\nΣ∼ IW (Σ|V0,ν0) (7)\\nµ∼ N (µ|m0,(Σ/κ0)) (8)\\nxt∼p(xt|xt−1,ut) (9)\\nzt∼p(zt|xt) (10)\\nThen, the probability distribution for equation (6) can be\\ndeﬁned as follows:\\np(it|xt,µ,Σ,φl,Ct)\\n=N(xt|µit,Σit)Mult(it|φCt)∑\\nit=jN(xt|µj,Σj)Mult(j|φCt).(11)\\nThe prior distribution conﬁgured by using the stick breaking\\nprocess (SBP) [25] is denoted as GEM(·), the multinomial\\ndistribution as Mult(·), the Dirichlet distribution as Dir(·), the\\ninverse–Wishart distribution as IW(·), and the multivariate\\nGaussian (normal) distribution as N(·). The motion model\\nand the sensor model of self-localization are denoted as p(xt|\\nxt−1,ut)andp(zt|xt)in equations (9) and (10), respectively.\\n1latticelm is the name of the tool that [23] is implemented and is treated\\nas the name of the method in this study. http://www.phontron.com/latticelm/', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 2}), Document(page_content='4\\n1−tx xttz\\ntu\\ntCbtO,W\\nΣµti\\nγlφ\\nαKLL /g1828/g3047\\n0β0 0,κm\\n0 0,νV1−tz\\n1−tu1+tz\\n1+tu\\nπ1+tx\\nFig. 2. Graphical model of the proposed method SpCoA\\nThis model can learn an appropriate number of spatial\\nconcepts, depending on the data, by using a nonparametric\\nBayesian approach. We use the SBP, which is one of the\\nmethods based on the Dirichlet process. In particular, this\\nmodel can consider a theoretically inﬁnite number of spatial\\nconceptsL→∞ and position distributions K→∞ . SBP\\ncomputations are difﬁcult because they generate an inﬁnite\\nnumber of parameters. In this study, we approximate a number\\nof parameters by setting sufﬁciently large values, i.e., a weak-\\nlimit approximation [26].\\nIt is possible to correlate a name with multiple places, e.g.,\\n“staircase” is in two different places, and a place with multiple\\nnames, e.g., “toilet” and“restroom” refer to the same place.\\nSpatial concepts are represented by a word distribution of the\\nnames of the place Wland several position distributions ( µk,\\nΣk) indicated by a multinomial distribution φl. In other words,\\nthis model is capable of relating the mixture of Gaussian\\ndistributions to a multinomial distribution of the names of\\nplaces. It should be noted that the arrows connecting itto the\\nsurrounding nodes of the proposed graphical model differ from\\nthose of ordinal Gaussian mixture model (GMM). We assume\\nthat words obtained by the robot do not change its position,\\nbut that the position of the robot affects the distribution of\\nwords. Therefore, the proposed generative process assumes\\nthat the index of position distribution it, i.e., the category of\\nthe place, is generated from the position of the robot xt. This\\nchange can be naturally introduced without any troubles by\\nintroducing equation (11).\\nB. Overview of the proposed method SpCoA\\nWe assume that a robot performs self-localization by using\\ncontrol data and sensor data at all times. The procedure for\\nthe learning of spatial concepts is as follows:\\n1) An utterer teaches a robot the names of places, as shown\\nin Fig. 1 (b). Every time the robot arrives at a place\\nthat was a designated learning target, the utterer says a\\nsentence, including the name of the current place.\\n2) The robot performs speech recognition from the uttered\\nspeech signal data. Thus, the speech recognition systemTABLE I\\nEACH ELEMENT OF THE GRAPHICAL MODEL\\nxt Self-position of a robot\\nut Control data\\nzt Sensor data\\nCt Index of spatial concepts\\nOt,b Segmented word in a uttered sentence\\nWMultinomial distribution as the word probability\\nof the names of places\\nµ,ΣGaussian distribution as a position distribution\\n(mean vector, covariance matrix)\\nit Index of a position distribution\\nφlMultinomial distribution\\nof indexitof Gaussian distribution\\nπMultinomial distribution\\nof indexCtof spatial concepts\\nα Hyperparameter of multinomial distributions φl\\nγ Hyperparameter of multinomial distribution π\\nβ0 Hyperparameter of Dirichlet prior distribution\\nm0,κ0,\\nV0,ν0Hyperparameters of\\nGaussian–inverse–Wishart prior distribution\\nincludes a word dictionary of only Japanese syllables.\\nThe speech recognition results are obtained in a lattice\\nformat.\\n3) Word segmentation is performed by using the lattices of\\nthe speech recognition results.\\n4) The robot learns spatial concepts from words obtained\\nby word segmentation and robot positions obtained by\\nself-localization for all teaching times. The details of the\\nlearning are given in III-C.\\nThe procedure for self-localization utilizing spatial concepts\\nis as follows:\\n1) The words of the learned spatial concepts are registered\\nto the word dictionary of the speech recognition system.\\n2) When a robot obtains a speech signal, speech recognition\\nis performed. Then, a word sequence as the 1-best\\nspeech recognition result is obtained.\\n3) The robot modiﬁes the self-localization from words\\nobtained by speech recognition and the position like-\\nlihood obtained by spatial concepts. The details of self-\\nlocalization are provided in III-D.\\nThe proposed method can learn words related to places\\nfrom the utterances of sentences. We use an unsupervised\\nword segmentation method latticelm that can directly segment\\nwords from the lattices of the speech recognition results of\\nthe uttered sentences [23]. The lattice can represent to a\\ncompact the set of more promising hypotheses of a speech\\nrecognition result, such as N-best, in a directed graph format.\\nUnsupervised word segmentation using the lattices of syllable\\nrecognition is expected to be able to reduce the variability and\\nerrors in phonemes as compared to NPYLM [14], i.e., word\\nsegmentation using the 1-best speech recognition results.\\nThe self-localization method adopts MCL [24], a method\\nthat is generally used as the localization of mobile robots\\nfor simultaneous localization and mapping (SLAM) [27]. We', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 3}), Document(page_content='5\\nassume that a robot generates an environment map by using\\nMCL-based SLAM such as FastSLAM [28], [29] in advance,\\nand then, performs localization by using the generated map.\\nThen, the environment map of both an occupancy grid map\\nand a landmark map is acceptable.\\nC. Learning of spatial concept\\nSpatial concepts are learned from multiple teaching data,\\ncontrol data, and sensor data. The teaching data are a set\\nof uttered sentences for all teaching times. Segmented words\\nof an uttered sentence are converted into a bag-of-words\\n(BoW) representation as a vector of the occurrence counts\\nof wordsOt,B. The set of the teaching times is denoted\\nasTo={t1,t2,...,t N}, and the number of teaching data\\nitems is denoted as N. The model parameters are denoted\\nasΘ ={W,µ,Σ,φl,π}. The initial values of the model\\nparameters can be set arbitrarily in accordance with a con-\\ndition. Further, the sampling values of the model parameters\\nfrom the following joint posterior distribution are obtained by\\nperforming Gibbs sampling.\\np(x0:T,iTo,CTo,Θ|OTo,B,u1:T,z1:T,h) (12)\\nwhere the hyperparameters of the model are denoted as\\nh={α,γ,β 0,m0,κ0,V0,ν0}. The algorithm of the learning\\nof spatial concepts is shown in Algorithm 1.\\nThe conditional posterior distribution of each element used\\nfor performing Gibbs sampling can be expressed as follows:\\nAn indexitof the position distribution is sampled for each\\ndatat∈Tofrom a posterior distribution as follows:\\nit∼p(it=k|xt,µ,Σ,φl,Ct)\\n∝N(xt|µk=it,Σk=it)Mult(it=k|φl=Ct).(13)\\nAn indexCtof the spatial concepts is sampled for each data\\nitemt∈Tofrom a posterior distribution as follows:\\nCt∼p(Ct=l|xt,it,Ot,B,µ,Σ,φl,π)\\n∝Mult(Ot,B|Wl=Ct)Mult(it=k|φl=Ct)\\nMult(Ct=l|π) (14)\\nwhereOt,Bdenotes a vector of the occurrence counts of words\\nin the sentence at time t. A posterior distribution representing\\nword probabilities of the name of place Wis calculated as\\nfollows:\\np(W|CTo,OTo,B)\\n∝∏\\nl∈L[∏\\nl=Ct\\nt∈Top(Ot,B|Wl=Ct)]\\np(Wl) (15)\\nwhere variables with the subscript Todenote the set of all\\nteaching times. A word probability of the name of place Wl\\nis sampled for each l∈Las follows:\\nWl∼Mult( Ol|Wl)Dir(Wl|β0)∝Dir(Wl|βnl)(16)\\nwhereβnlrepresents the posterior parameter and Oldenotes\\nthe BoW representation of all sentences of Ct=lint∈To.A posterior distribution representing the position distribution\\nµ,Σis calculated as follows:\\np(µ,Σ|iTo,xTo,CTo,φl)\\n∝∏\\nk∈K[∏\\nk=it\\nt∈Top(xt|µk=it,Σk=it)]\\np(µk,Σk). (17)\\nA position distribution µk,Σkis sampled for each k∈Kas\\nfollows:\\nµk,Σk∼N(xk|µk,Σk)NIW (µk,Σk|m0,κ0,V0,ν0)\\n∝NIW (µk,Σk|mnk,κnk,Vnk,νnk)(18)\\nwhereNIW (·)denotes the Gaussian–inverse–Wishart distri-\\nbution;mnk,κnk,Vnk, andνnkrepresent the posterior param-\\neters; and xkindicates the set of the teaching positions of\\nit=kint∈To. A topic probability distribution πof spatial\\nconcepts is sampled as follows:\\nπ∼Mult(CTo|π)Dir(π|γ)∝Dir(π|CTo,γ). (19)\\nA posterior distribution representing the mixed weights φlof\\nthe position distributions is calculated as follows:\\np(φl|xTo,iTo,CTo,µ,Σ)\\n∝∏\\nl∈L[∏\\nl=Ct\\nt∈Top(it|φl=Ct)]\\np(φl). (20)\\nA mixed weight φlof the position distributions is sampled for\\neachl∈Las follows:\\nφl∼Mult( il|φl)Dir(φl|α)∝Dir(φl|il,α) (21)\\nwhere ildenotes a vector counting all the indices of the\\nGaussian distribution of Ct=lint∈To.\\nSelf-positions x0:Tare sampled by using a Monte Carlo\\nﬁxed-lag smoother [30] in the learning phase. The smoother\\ncan estimate self-position x0:tand notp(x0:t|u1:t,z1:t), i.e., a\\nsequential estimation from the given data u1:t,z1:tuntil timet,\\nbut it can estimate p(x0:t|u1:T,z1:T), i.e., an estimation from\\nthe given data u1:T,z1:Tuntil timeTlater thant(t<T ). In\\ngeneral, the smoothing method can provide a more accurate\\nestimation than the MCL of online estimation. In contrast, if\\nthe self-position of a robot xtis sampled like direct assignment\\nsampling for each time t, the sampling of xtis divided in the\\ncase with the teaching time t∈Toand another time t /∈To\\nas follows:\\nxt∼\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3p(xt|xt−1,xt+1,ut,ut+1,zt)\\n∝p(xt+1|xt,ut+1)p(zt|xt)p(xt|xt−1,ut)\\n(t /∈To),\\np(xt|xt−1,xt+1,ut,ut+1,zt,it,µ,Σ,φl,Ct)\\n∝p(xt+1|xt,ut+1)p(zt|xt)p(xt|xt−1,ut)\\np(it|xt,µ,Σ,φl,Ct)\\n(t∈To).\\n(22)', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 4}), Document(page_content='6\\nAlgorithm 1 Learning of spatial concepts\\n1:L=∅,To=∅\\n2:// Localization and speech recognition\\n3:fort= 0 toTdo\\n4:x0:t∼Monte Carlo smoother(x0:t−1,u1:t,z1:t)[30]\\n5: ifthe speech signal is observed then\\n6:lattice t= speech recognition( speechsignal )\\n7: addlattice ttoL// Registering the lattice\\n8: addttoTo// Registering the teaching time\\n9: end if\\n10:end for\\n11:// Word segmentation using lattices\\n12:OTo,B∼latticelm(L)[23]\\n13:// Gibbs sampling\\n14:Initialize parameters iTo,CTo,Θ ={W,µ,Σ,φl,π}\\n15:forj= 1 toiterationnumber do\\n16:iTo∼p(iTo|xTo,µ,Σ,φl,CTo)(13)\\n17:CTo∼p(CTo|xTo,iTo,OTo,B,µ,Σ,φl,π)(14)\\n18: W∼p(W|CTo,OTo,B)(16)\\n19: µ,Σ∼p(µ,Σ|iTo,xTo,CTo,φl)(18)\\n20:π∼p(π|CTo)(19)\\n21:φl∼p(φl|xTo,iTo,CTo,µ,Σ)(21)\\n22: fort= 0 toTdo\\n23:xt∼\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3p(xt|xt−1,xt+1,ut,ut+1,zt)\\n(t /∈To)\\np(xt|xt−1,xt+1,ut,ut+1,zt)\\np(it|xt,µ,Σ,φl,Ct)\\n(t∈To)(22)\\n24: end for\\n25:end for\\n26:return Θ\\nD. Self-localization of after learning spatial concepts\\nA robot that acquires spatial concepts can leverage spatial\\nconcepts to self-localization. The estimated model parameters\\nΘ ={W,µ,Σ,φl,π}and a speech recognition sentence Ot,B\\nat timetare given to the condition part of the probability\\nformula of MCL as follows:\\np(x0:t|z1:t,u1:t,O1:t,B,Θ)\\n∝p(zt|xt)p(Ot,B|xt,Θ)p(xt|xt−1,ut)\\np(x0:t−1|z1:t−1,u1:t−1,O1:t−1,B,Θ).(23)\\nWhen the robot hears the name of a place spoken by the\\nutterer, in addition to the likelihood of the sensor model of\\nMCL, the likelihood of xtwith respect to a speech recognition\\nsentence is calculated as follows:\\np(Ot,B|xt,Θ)\\n∝∑\\nCt[\\np(Ot,B|WCt)∑\\nit[\\np(xt|µit,Σit)p(it|φCt)]\\np(Ct|π)]\\n.\\n(24)\\nThe algorithm of self-localization utilizing spatial concepts\\nis shown in Algorithm 2. The set of particles is denoted as Xt,\\nthe temporary set that stores the pairs of the particle x[m]\\ntand\\nthe weightw[m]\\nt, i.e.,⟨x[m]\\nt,w[m]\\nt⟩, is denoted as ¯Xt. The num-\\nber of particles is M. The function sample motion modelAlgorithm 2 Self-localization utilizing spatial concepts\\n1:procedure Localization (Xt−1,ut,zt,Ot,B,Θ)\\n2: ¯Xt=Xt=∅\\n3: form= 1 toMdo\\n4:x[m]\\nt= sample motion model(ut,x[m]\\nt−1)(9)\\n5:w[m]\\nt= sensor model(zt,x[m]\\nt)(10)\\n6: ifthe speech signal is observed then\\n7: w[m]\\nt=w[m]\\nt×p(Ot,B|xt,Θ)\\n8: end if\\n9: add⟨x[m]\\nt,w[m]\\nt⟩to¯Xt\\n10: end for\\n11: form= 1 toMdo\\n12: drawiwith probability∝w[i]\\nt\\n13: addx[i]\\nttoXt\\n14: end for\\n15: returnXt\\n16:end procedure\\nis a function that moves each particle from its previous state\\nxt−1to its current state xtby using control data. The function\\nsensor model calculates the likelihood of each particle x[m]\\nt\\nusing sensor data zt. These functions are normally used in\\nMCL. For further details, please refer to [27]. In this case, a\\nspeech recognition sentence Ot,Bis obtained by the speech\\nrecognition system using a word dictionary containing all the\\nlearned words.\\nIV. E XPERIMENT I\\nIn this experiment, we validate the evidence of the proposed\\nmethod (SpCoA) in an environment simulated on the simulator\\nplatform SIGVerse2[31], which enables the simulation of\\nsocial interactions. The speech recognition is performed using\\nthe Japanese continuous speech recognition system Julius3\\n[32], [33]. The set of 43 Japanese phonemes deﬁned by\\nAcoustical Society of Japan (ASJ)’s speech database com-\\nmittee is adopted by Julius [32]. The representation of these\\nphonemes is also adopted in this study. The Julius system\\nuses a word dictionary containing 115 Japanese syllables. The\\nmicrophone attached on the robot is SHURE’s PG27-USB.\\nFurther, an unsupervised morphological analyzer, a latticelm\\n0.4, is implemented [23].\\nIn the experiment, we compare the following three types\\nof word segmentation methods. A set of syllable sequences is\\ngiven to the graphical model of SpCoA by each method. This\\nset is used for the learning of spatial concepts as recognized\\nuttered sentences OTo,B.\\n(A) latticelm (proposed method)\\nSyllable recognition results in the lattice format are\\nsegmented by using latticelm.\\n(B) 1-best NPYLM\\nSyllable recognition results of the 1-best method are\\nsegmented by using latticelm. In this case, latticelm\\n[23] is almost equivalent to NPYLM [14].\\n2SIGServer-2.2.2, SIGViewer-2.2.0, http://www.sigverse.com/wiki/\\n3Julius dictation-kit-v4.3.1-linux, GMM-HMM decoding, http://julius.\\nsourceforge.jp/', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 5}), Document(page_content='7\\n/geNkaN//gomibako//kiqchiN/\\n/daidokoro/ /terebimae/\\n/teeburunoatari/ /teeburunoatari//hoNdana/ /sofaamae/\\nFig. 3. Environment to be used for learning and localization on SIGVerse:\\nThis is a pseudo-room in the simulated real world. There is a robot in the\\ncenter of the room. The size of the room is 500 cm ×1,000 cm, and the\\nsize of the robot is 50 cm ×50 cm.\\nTABLE II\\nVARIOUS PHRASES OF EACH JAPANESE SENTENCE :“**” IS USED AS A\\nPLACEHOLDER FOR THE NAME OF EACH PLACE . EXAMPLES OF THESE\\nPHRASES ARE “** is here. ” ,“This place is **. ” ,“This place’s name is **. ” ,\\nAND “Came to **. ” INENGLISH .\\n** da yo ** wa kochira desu\\n** desu kochira ga ** ni nari masu\\nkoko ga ** kono basho ga ** da yo\\nkoko wa ** desu kono basho no namae wa **\\n** ni ki mashi ta koko no namae wa ** da yo\\n(C) Bag-of-syllables (BoS)\\nSyllable recognition results of the 1-best method are\\nsegmented by each syllable. In other words, this\\nmethod is used for segmenting not words but ele-\\nments of the recognized syllable sequences directly\\nin the BoS (bag-of-letters) representation.\\nThe remainder of this section is organized as follows: In\\nSection IV-A, the conditions and results of learning spatial\\nconcepts are described. The experiments performed using the\\nlearned spatial concepts are described in Section IV-B to IV-E.\\nIn Section IV-B, we evaluate the accuracy of the phoneme\\nrecognition and word segmentation for uttered sentences. In\\nSection IV-C, we evaluate the clustering accuracy of the\\nestimation results of index Ctof spatial concepts for each\\nteaching utterance. In Section IV-D, we evaluate the accuracy\\nof the acquisition of names of places. In Section IV-E, we\\nshow that spatial concepts can be utilized for effective self-\\nlocalization.\\nA. Learning of spatial concepts\\n1) Conditions: We conduct this experiment of spatial con-\\ncept acquisition in the environment prepared on SIGVerse. The\\nexperimental environment is shown in Fig. 3. A mobile robot\\ncan move by performing forward, backward, right rotation,\\nor left rotation movements on a two-dimensional plane. In\\nthis experiment, the robot can use an approximately correct\\nmap of the considered environment. The robot has a range\\nk=0 k=7 k=17k=1 k=2 k=6\\nk=4k=3Fig. 4. Learning result of the position distribution: A point group of each color\\nto represent each position distribution is drawn on an map of the considered\\nenvironment. The colors of the point groups are determined randomly. Each\\nballoon shows the index number for each position distribution.\\nFig. 5. Learning result of the multinomial distributions of the names of places\\nW(top); multinomial distributions of the index of the position distribution\\nφl(bottom): All the words obtained during the experiment are shown.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 6}), Document(page_content='8\\nsensor in front and performs self-localization on the basis of\\nan occupancy grid map. The initial particles are deﬁned by\\nthe true initial position of the robot. The number of particles\\nisM= 1000 .\\nThe lag value of the Monte Carlo ﬁxed-lag smoothing is\\nﬁxed at 100. The other parameters of this experiment are as\\nfollows:L= 50 ,K= 50 ,α= 1.5,γ= 8,β0= 0.5,\\nm0= [0,0]T,κ0= 0.001,V0= diag(1000 ,1000) , and\\nν0= 2. The number of iterations used for Gibbs sampling\\nis 100. This experiment does not include the direct assign-\\nment sampling of xtin equation (22), i.e., lines 22–24 of\\nAlgorithm 1 are omitted, because we consider that the self-\\nposition can be obtained with sufﬁciently good accuracy by\\nusing the Monte Carlo smoothing. Eight places are selected\\nas the learning targets, and eight types of place names are\\nconsidered. Each uttered place name is shown in Fig. 3. These\\nutterances include the same name in different places, i.e.,\\n“teeburunoatari” (which means near the table in English),\\nand different names in the same place, i.e., “kiqchiN” and\\n“daidokoro” (which mean a kitchen in English). The other\\nteaching names are “geNkaN” (which means an entrance or\\na doorway in English); “terebimae” (which means the front\\nof the TV in English); “gomibako” (which means a trash\\nbox in English); “hoNdana” (which means a bookshelf in\\nEnglish); and “sofaamae” (which means the front of the sofa\\nin English). The teaching utterances, including the 10 types\\nof phrases, are spoken for a total of 90 times. The phrases in\\neach uttered sentence are listed in Table II.\\n2) Results: The learning results of spatial concepts obtained\\nby using the proposed method are presented here. Fig. 4\\nshows the position distributions learned in the experimental\\nenvironment. Fig. 5 (top) shows the word distributions of\\nthe names of places for each spatial concept, and Fig. 5\\n(bottom) shows the multinomial distributions of the indices of\\nthe position distributions. Consequently, the proposed method\\ncan learn the names of places corresponding to each place of\\nthe learning target. In the spatial concept of index Ct= 1, the\\nhighest probability of words was “sofamae” , and the highest\\nprobability of the indices of the position distribution was\\nk= 0; therefore, the name of a place “sofamae” was learned\\nto correspond to the position distribution of k= 0. In the\\nspatial concept of index Ct= 5,“kiqchi” and“daidokoro”\\nwere learned to correspond to the position distribution of\\nk= 1 . Therefore, this result shows that multiple names\\ncan be learned for the same place. In the spatial concept\\nof indexCt= 0,“te” and “durunoatari” (one word in a\\nnormal situation) were learned to correspond to the position\\ndistributions of k= 3 andk= 4. Therefore, this result shows\\nthat the same name can be learned for multiple places.\\nB. Phoneme recognition accuracy of uttered sentences\\n1) Conditions: We compared the performance of three\\ntypes of word segmentation methods for all the considered\\nuttered sentences. It was difﬁcult to weigh the ambiguous\\nsyllable recognition and the unsupervised word segmentation\\nseparately. Therefore, this experiment considered the positions\\nof a delimiter as a single letter. We calculated the matchingTABLE III\\nCOMPARISON OF THE PHONEME ACCURACY RATES OF UTTERED\\nSENTENCES FOR DIFFERENT WORD SEGMENTATION METHODS\\nlatticelem\\n(used in SpCoA) 1-best NPYLM BoS\\nPAR 0.82 0.71 0.67\\nrate of a phoneme string of a recognition result of each uttered\\nsentence and the correct phoneme string of the teaching data\\nthat was suitably segmented into Japanese morphemes using\\nMeCab4, which is an off-the-shelf Japanese morphological\\nanalyzer that is widely used for natural language processing.\\nThe matching rate of the phoneme string was calculated by\\nusing the phoneme accuracy rate (PAR) as follows:\\nPAR = 1−S+D+I\\nN. (25)\\nThe numerator of equation (25) is calculated by using the\\nLevenshtein distance between the correct phoneme string and\\nthe recognition phoneme string. Sdenotes the number of\\nsubstitutions; D, the number of deletions; and I, the number\\nof insertions. Nrepresents the number of phonemes of the\\ncorrect phoneme string.\\n2) Results: Table III shows the results of PAR. Table IV\\npresents examples of the word segmentation results of the\\nthree considered methods. We found that the unsupervised\\nmorphological analyzer capable of using lattices improved\\nthe accuracy of phoneme recognition and word segmentation.\\nConsequently, this result suggests that this word segmentation\\nmethod considers the multiple hypothesis of speech recogni-\\ntion as a whole and reduces uncertainty such as variability\\nin recognition by using the syllable recognition results in the\\nlattice format.\\nC. Estimation accuracy of spatial concepts\\n1) Conditions: We compared the matching rate with the\\nestimation results of index Ctof the spatial concepts of each\\nteaching utterance and the classiﬁcation results of the correct\\nanswer given by humans. The evaluation of this experiment\\nused the adjusted Rand index (ARI) [34]. ARI is a measure\\nof the degree of similarity between two clustering results.\\nFurther, we compared the proposed method with a method\\nof word clustering without location information for the in-\\nvestigation of the effect of lexical acquisition using location\\ninformation. In particular, a method of word clustering with-\\nout location information used the Dirichlet process mixture\\n(DPM) of the unigram model of an SBP representation. The\\nparameters corresponding to those of the proposed method\\nwere the same as the parameters of the proposed method and\\nwere estimated using Gibbs sampling.\\n2) Results: Fig. 6 shows the results of the average of the\\nARI values of 10 trials of learning by Gibbs sampling. Here,\\nwe found that the proposed method showed the best score.\\nThese results and the results reported in Section IV-B suggest\\nthat learning by uttered sentences obtained by better phoneme\\n4MeCab, http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 7}), Document(page_content='9\\nTABLE IV\\nEXAMPLES OF WORD SEGMENTATION RESULTS OF UTTERED SENTENCES .“|”DENOTES A WORD SEGMENT POINT .\\nCorrect word sequence geNkaN |wa |kochira |desu gomibako |ni|ki|mashi |ta kono |basho |no|namae |wa |hoNdana\\nlatticelm geNkaN |wakochiradesu komibako |o|ni|kimashita konobashuno |namae |wa |foNdana\\n1-best NPYLM ki|nika |N|wa |kochira |de|su go|mibako |niki |na|shita kono |bo|shu |no|namae |wa |fo|N|da|na\\nBoS ki|ni|ka|N|wa |ko|chi |ra|de|su go|mi|ba|ko|ni|ki|na|shi |ta ko|no|bo|shu |no|na|ma |e|wa |fo|N|da|na\\nSpCoA SpCoA\\n(1-best NPYLM)SpCoA\\n(BoS)DPM\\n(latticelm)DPM\\n(1-best NPYLM)DPM\\n(BoS)0.00.20.40.60.81.0ARI\\nFig. 6. Comparison of the accuracy rates of the estimation results of spatial\\nconcepts\\nrecognition and better word segmentation produces a good\\nresult for the acquisition of spatial concepts. Furthermore,\\nin a comparison of two clustering methods, we found that\\nSpCoA was considerably better than DPM, a word clustering\\nmethod without location information, irrespective of the word\\nsegmentation method used. The experimental results showed\\nthat it is possible to improve the estimation accuracy of spatial\\nconcepts and vocabulary by performing word clustering that\\nconsidered location information.\\nD. Accuracy of acquired phoneme sequences representing the\\nnames of places\\n1) Conditions: We evaluated whether the names of places\\nwere properly learned for the considered teaching places. This\\nexperiment assumes a request for the best phoneme sequence\\nOt,bestrepresenting the self-position xtfor a robot. The robot\\nmoves close to each teaching place. The probability of a\\nwordOt,best when the self-position xtof the robot is given,\\np(Ot,best|xt), can be obtained by using equation (24). The\\nword having the best probability was selected. We compared\\nthe PAR with the correct phoneme sequence and a selected\\nname of the place. Because “kiqchiN” and“daidokoro” were\\ntaught for the same place, the word whose PAR was the higher\\nscore was adopted.\\n2) Results: Fig. 7 shows the results of PAR for the word\\nconsidered the name of a place. SpCoA (latticelm), the pro-\\nposed method using the results of unsupervised word seg-\\nmentation on the basis of the speech recognition results in\\nthe lattice format, showed the best PAR score. In the 1-best\\nand BoS methods, a part syllable sequence of the name of a\\nplace was more minutely segmented as shown in Table IV.\\nTherefore, the robot could not learn the name of the teaching\\nplace as a coherent phoneme sequence. In contrast, the robot\\ncould learn the names of teaching places more accurately by\\nusing the proposed method.\\nSpCoA SpCoA\\n(1-best NPYLM)SpCoA\\n(BoS)0.00.20.40.60.81.0PARFig. 7. PAR scores for the word considered the name of a place\\nE. Self-localization that utilizes acquired spatial concepts\\n1) Conditions: In this experiment, we validate that the\\nrobot can make efﬁcient use of the acquired spatial concepts.\\nWe compare the estimation accuracy of localization for the\\nproposed method (SpCoA MCL) and the conventional MCL.\\nWhen a robot comes to the learning target, the utterer speaks\\nout the sentence containing the name of the place once again\\nfor the robot. The moving trajectory of the robot and the\\nuttered positions are the same in all the trials. In particular,\\nthe uttered sentence is “kokowa ** dayo” . When learning\\na task, this phrase is not used. The number of particles is\\nM= 1000 , and the initial particles are uniformly distributed\\nin the considered environment. The robot performs a control\\noperation for each time step.\\nThe estimation error in the localization is evaluated as\\nfollows: While running localization, we record the estimation\\nerror (equation (26)) on the xyplane of the ﬂoor for each\\ntime step.\\net=√\\n(¯xt−x∗\\nt)2+ (¯yt−y∗\\nt)2 (26)\\nwhere x∗\\nt,y∗\\ntdenote the true position coordinates of the robot\\nas obtained from the simulator, and ¯xt=∑M\\ni=1w(i)\\ntx(i)\\nt,\\n¯yt=∑M\\ni=1w(i)\\nty(i)\\ntrepresent the weighted mean values\\nof localization coordinates. The normalized weight w(i)\\ntis\\nobtained from the sensor model in MCL as a likelihood.\\nIn the utterance time, this likelihood is multiplied by the\\nvalue calculated using equation (24). x(i)\\nt,y(i)\\ntdenote the x-\\ncoordinate and the y-coordinate of index iof each particle\\nat timet. After running the localization, we calculated the\\naverage ofet.\\nFurther, we compared the estimation accuracy rate (EAR)\\nof the global localization. In each trial, we calculated the\\nproportion of time step in which the estimation error was less\\nthan 50 cm.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 8}), Document(page_content='10\\nSpCoA MCLSpCoA MCL\\n(1-best NPYLM)SpCoA MCL\\n(BoS)MCL\\nMean 57.20 100.92 125.87 223.38\\nS.E. 9.89 40.23 45.49 88.86\\nEAR 0.81 0.70 0.65 0.5602004006008001000 Estimation error [cm]\\nFig. 8. Results of estimation errors and EARs of self-localization\\nFig. 9. Autonomous mobile robot TurtleBot 2: The robot is based on Yujin\\nRobot Kobuki and Microsoft Kinect for its use as a range sensor.\\n2) Results: Fig. 8 shows the results of the estimation error\\nand the EAR for 10 trials of each method. All trials of SpCoA\\nMCL (latticelm) and almost all trials of the method using\\n1-best NPYLM and BoS showed relatively small estimation\\nerrors. Results of the second trial of 1-best NPYLM and the\\nﬁfth trial of BoS showed higher estimation errors. In these\\ntrials, many particles converged to other places instead of the\\nplace where the robot was, based on utterance information.\\nNevertheless, compared with those of the conventional MCL,\\nthe results obtained using spatial concepts showed an obvious\\nimprovement in the estimation accuracy. Consequently, spatial\\nconcepts acquired by using the proposed method proved to be\\nvery helpful in improving the localization accuracy.\\nV. E XPERIMENT II\\nIn this experiment, the effectiveness of the proposed method\\nwas tested by using an autonomous mobile robot TurtleBot 25\\nin a real environment. Fig. 9 shows TurtleBot 2 used in the\\nexperiments. Mapping and self-localization are performed by\\nthe robot operating system (ROS). The speech recognition\\nsystem, the microphone, and the unsupervised morphological\\nanalyzer were the same as those described in Section IV.\\nA. Learning of spatial concepts in the real environment\\n1) Conditions: We conducted an experiment of the spatial\\nconcept acquisition in a real environment of an entire ﬂoor of a\\nbuilding. In this experiment, self-localization was performed\\nusing a map generated by SLAM. The initial particles are\\ndeﬁned by the true initial position of the robot. The generated\\nmap in the real environment and the names of teaching places\\nare shown in Fig. 10. The number of teaching places was\\n5TurtleBot 2, http://turtlebot.com/\\n/kyouiNbeya//toire/\\n/tsubokeN//kitanokeN/\\n/nishikawakeN//raqkukeN/ /kameikuupaakeN/\\n/gomibako//puriNtaabeya/\\n/watarirouka/\\n/kaigishitsu//shinodaseyakeN/\\n/hagiwarakeN/ /gomibako/\\n/gomibako/\\n/watarirouka//kaidaNmae/ /kaidaNmae//souhatsukeN//taniguchikeN/Fig. 10. Teaching places and the names of places shown on the generated\\nmap. The teaching places included places having two names each and multiple\\nplaces having the same names.\\n19, and the number of teaching names was 16. The teaching\\nutterances were performed for a total of 100 times.\\n2) Results: Fig. 11 shows the position distributions learned\\non the map. Table V shows the ﬁve best elements of the\\nmultinomial distributions of the name of place WCtand\\nthe multinomial distributions of the indices of the position\\ndistribution φCtfor each index of spatial concept Ct. Thus,\\nwe found that the proposed method can learn the names of\\nplaces corresponding to the considered teaching places in the\\nreal environment. For example, in the spatial concept of index\\nCt= 10 ,“torire” was learned to correspond to a position\\ndistribution of k= 42 . Similarly, “kidanokeN” corresponded\\ntok= 8 inCt= 29 , and “kaigihitsu” was corresponded\\ntok= 60 inCt= 32 . In the spatial concept of index\\nCt= 27 , a part of the syllable sequences was minutely\\nsegmented as “sohatsuke” ,“N”, and “tani” ,“guchi” . In this\\ncase, the robot was taught two types of names. These words\\nwere learned to correspond to the same position distribution\\nofk= 55 . InCt= 8,“gomibako” showed a high probability,\\nand it corresponded to three distributions of the position of\\nk= 0,36,59. The position distribution of k= 13 had the\\nfourth highest probability in the spatial concept Ct= 8 .\\nTherefore, “raqkukeN, ” which had the ﬁfth highest probability\\nin the spatial concept Ct= 8 (and was expected to relate to\\nthe spatial concept Ct= 74 ), can be estimated as the word\\ndrawn from spatial concept Ct= 8. However, in practice, this\\nsituation did not cause any severe problems because the spatial\\nconcept of the index Ct= 74 had the highest probabilities for\\nthe word “rapukeN” and the position distribution k= 13 than\\nCt= 8. In the probabilistic model, the relative probability\\nand the integrative information are important. When the robot\\nlistened to an utterance related to “raqkukeN, ” it could make\\nuse of the spatial concept of index Ct= 74 for self-\\nlocalization with a high probability, and appropriately updated\\nits estimated self-location. We expected that the spatial concept\\nof indexCt= 2was learned as two separate spatial concepts.\\nHowever, “watarirooka” and“kaidaNmae” were learned as\\nthe same spatial concept. Therefore, the multinomial distri-\\nbutionφ2showed a higher probability for the indices of the\\nposition distribution corresponding to the teaching places of\\nboth “watarirooka” and“kaidaNmae” .\\nThe proposed method adopts a nonparametric Bayesian\\nmethod in which it is possible to form spatial concepts\\nthat allow many-to-many correspondences between names and', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 9}), Document(page_content='11\\nk=0k=1\\nk=7k=8 k=10\\nk=11k=12k=13\\nk=14\\nk=23k=29\\nk=35 k=36k=42\\nk=50k=55\\nk=58\\nk=60k=69k=59\\nFig. 11. Learning result of each position distribution: A point group of each color denoting each position distribution was drawn on the map. The colors of\\nthe point groups were determined randomly. Further, each index number is denoted as it=k.\\nTABLE V\\nLEARNING RESULT OF HIGH -PROBABILITY WORDS AND INDICES OF THE POSITION DISTRIBUTION FOR EACH SPATIAL CONCEPT\\nWCt φCt\\nIndexCt Word (Probability) Indexit (Probability)\\nwatarirooka (0.165 ) 23 (0.175 )\\nkaidaNmae (0.151 ) 58 (0.174 )\\n2 desu (0.117) 50 (0.142 )\\nnikimashita (0.069) 12 (0.141 )\\newa (0.068) 11 (0.039 )\\nnokeN (0.189 ) 29 (0.342 )\\ntsu (0.185 ) 64 (0.008)\\n3 a (0.046) 49 (0.008)\\nnayo (0.045) 82 (0.008)\\nde (0.045) 94 (0.008)\\nshinozaseya (0.178 ) 10 (0.339 )\\nkeN (0.174 ) 37 (0.008)\\n6 desu (0.075) 5 (0.008)\\nkoko (0.042) 94 (0.008)\\nwa (0.041) 29 (0.008)\\ndesu (0.195) 36 (0.174 )\\ngomibako (0.180 ) 59 (0.136 )\\n8 koko (0.102) 0 (0.136 )\\na (0.081) 13 (0.135)\\nrapukeN (0.043) 9 (0.006)\\ntorire (0.154 ) 42 (0.340 )\\nwa (0.118) 83 (0.008)\\n10 kokoga (0.080) 9 (0.008)\\nnikimashita (0.045) 46 (0.008)\\nbyayo (0.043) 11 (0.008)\\ntani (0.098 ) 55 (0.507 )\\nsohatsuke (0.098 ) 84 (0.006)\\n27 N (0.098 ) 21 (0.006)\\nguchi (0.096 ) 42 (0.006)\\ndesu (0.078) 37 (0.006)\\nkidanokeN (0.209 ) 8 (0.336 )\\ndesu (0.091) 60 (0.009)\\n29 dayo (0.090) 70 (0.008)\\na (0.050) 17 (0.008)\\nkonobashoga (0.050) 2 (0.008)WCt φCt\\nIndexCt Word (Probability) Indexit (Probability)\\nkaigihitsu (0.181 ) 60 (0.301 )\\nnikimashita (0.113) 0 (0.065)\\n32 gomirako (0.079) 7 (0.064)\\na (0.078) 36 (0.007)\\ndayo (0.076) 33 (0.007)\\nN (0.113 ) 1 (0.197 )\\nkameikukache (0.112 ) 36 (0.075)\\n34 ewa (0.109) 42 (0.075)\\nkonobashunonama (0.108) 29 (0.009)\\nninarimasu (0.043) 61 (0.008)\\nrakeN (0.159 ) 35 (0.296 )\\nhagiwa (0.157 ) 30 (0.009)\\n44 desu (0.080) 48 (0.008)\\nwakochira (0.046) 32 (0.008)\\newa (0.045) 68 (0.008)\\nburiN (0.132 ) 7 (0.321 )\\nbea (0.130 ) 0 (0.067)\\n47 pa (0.107 ) 22 (0.008)\\newa (0.083) 5 (0.008)\\ndayo (0.078) 96 (0.008)\\nwakeN (0.133 ) 14 (0.332 )\\nnishikya (0.132 ) 90 (0.008)\\n66 desu (0.103) 25 (0.008)\\na (0.071) 69 (0.008)\\nnishi (0.040 ) 87 (0.008)\\nrapukeN (0.145 ) 13 (0.173 )\\nnikimashita (0.081) 56 (0.011)\\n74 nayo (0.080) 2 (0.010)\\nnokeN (0.017) 91 (0.010)\\nwakeN (0.016) 58 (0.010)\\nbea (0.153 ) 69 (0.343 )\\nN (0.151 ) 34 (0.008)\\n75 kyoi (0.149 ) 15 (0.008)\\ndayo (0.064) 75 (0.008)\\ndesu (0.062) 27 (0.008)\\nplaces. In contrast, this can create ambiguity that classiﬁes\\noriginally different spatial concepts into one spatial concept\\nas a side effect. There is a possibility that the ambiguity of\\nconcepts such as Ct= 2 will have a negative effect on self-\\nlocalization, even though the self-localization performance was\\n(overall) clearly increased by employing the proposed method.\\nThe solution of this problem will be considered in future work.\\nIn terms of the PAR of uttered sentences, the evaluation\\nvalue from the evaluation method used in Section IV-B is\\n0.83; this value is comparable to the result in Section IV-B.However, in terms of the PAR of the name of the place,\\nthe evaluation value from the evaluation method used in\\nSection IV-D is 0.35, which is lower than that in Section\\nIV-D. We consider that the increase in uncertainty in the real\\nenvironment and the increase in the number of teaching words\\nreduced the performance. We expect that this problem could\\nbe improved using further experience related to places, e.g., if\\nthe number of utterances per place is increased, and additional\\nsensory information is provided.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 10}), Document(page_content='12\\nB. Modiﬁcation of localization by the acquired spatial con-\\ncepts\\n1) Conditions: In this experiment, we veriﬁed the modi-\\nﬁcation results of self-localization by using spatial concepts\\nin global self-localization. This experiment used the learning\\nresults of spatial concepts presented in Section V-A. The\\nexperimental procedures are shown below. The initial particles\\nwere uniformly distributed on the entire ﬂoor. The robot\\nbegins to move from a little distance away to the target place.\\nWhen the robot reached the target place, the utterer spoke the\\nsentence containing the name of the place for the robot. Upon\\nobtaining the speech information, the robot modiﬁes the self-\\nlocalization on the basis of the acquired spatial concepts. The\\nnumber of particles was the same as that mentioned in Section\\nV-A.\\n2) Results: Fig. 12 shows the results of the self-localization\\nbefore (the top part of the ﬁgure) and after (the bottom part\\nof the ﬁgure) the utterance for three places. The particle states\\nare denoted by red arrows. The moving trajectory of the robot\\nis indicated by a green dotted arrow. Figs. 12 (a), (b), and\\n(c) show the results for the names of places “toire” ,“souhat-\\nsukeN” , and “gomibako” . Further, three spatial concepts, i.e.,\\nthose atk= 0,36,59, were learned as “gomibako” . In this\\nexperiment, the utterer uttered to the robot when the robot\\ncame close to the place of k= 36 . In all the examples shown\\nin the top part of the ﬁgure, the particles were dispersed in\\nseveral places. In contrast, the number of particles near the\\ntrue position of the robot showed an almost accurate increase\\nin all the examples shown in the bottom part of the ﬁgure.\\nThus, we can conclude that the proposed method can modify\\nself-localization by using spatial concepts.\\nVI. C ONCLUSION AND FUTURE WORK\\nIn this paper, we discussed the spatial concept acquisition,\\nlexical acquisition related to places, and self-localization us-\\ning acquired spatial concepts. We proposed nonparametric\\nBayesian spatial concept acquisition method SpCoA that in-\\ntegrates latticelm [23], a spatial clustering method, and MCL.\\nWe conducted experiments for evaluating the performance\\nof SpCoA in a simulation and a real environment. SpCoA\\nshowed good results in all the experiments. In experiments of\\nthe learning of spatial concepts, the robot could form spatial\\nconcepts for the places of the learning targets from human\\ncontinuous speech signals in both the room of the simulation\\nenvironment and the entire ﬂoor of the real environment.\\nFurther, the unsupervised word segmentation method latticelm\\ncould reduce the variability and errors in the recognition of\\nphonemes in all the utterances. SpCoA achieved more accurate\\nlexical acquisition by performing word segmentation using\\nthe lattices of the speech recognition results. In the self-\\nlocalization experiments, the robot could effectively utilize\\nthe acquired spatial concepts for recognizing self-position and\\nreducing the estimation errors in self-localization.\\nAs a method that further improves the performance of the\\nlexical acquisition, a mutual learning method was proposed by\\nNakamura et al. on the basis of the integration of the learning\\nof object concepts with a language model [35], [36]. Followinga similar approach, Heymann et al. proposed a method that\\nalternately and repeatedly updates phoneme recognition results\\nand the language model by using unsupervised word segmenta-\\ntion [37]. As a result, they achieved robust lexical acquisition.\\nIn our study, we can expect to improve the accuracy of lexical\\nacquisition for spatial concepts by estimating both the spatial\\nconcepts and the language model.\\nFurthermore, as a future work, we consider it necessary\\nfor robots to learn spatial concepts online and to recognize\\nwhether the uttered word indicates the current place or desti-\\nnation. Furthermore, developing a method that simultaneously\\nacquires spatial concepts and builds a map is one of our\\nfuture objectives. We believe that the spatial concepts will\\nhave a positive effect on the mapping. We also intend to\\nexamine a method that associates the image and the landscape\\nwith spatial concepts and a method that estimates both spatial\\nconcepts and object concepts.\\nREFERENCES\\n[1] D. Roy and A. Pentland, “Learning words from sights and sounds: A\\ncomputational model,” Cognitive science , vol. 26, no. 1, pp. 113–146,\\n2002.\\n[2] R. Taguchi, N. Iwahashi, T. Nose, K. Funakoshi, and M. Nakano,\\n“Learning lexicons from spoken utterances based on statistical model\\nselection,” in Annual Conference of the International Speech Communi-\\ncation Association (INTERSPEECH) , 2009, pp. 2731–2734.\\n[3] N. Iwahashi, “Language acquisition through a human–robot interface\\nby combining speech, visual, and behavioral information,” Information\\nSciences , vol. 156, no. 1, pp. 109–121, 2003.\\n[4] ——, “Robots that learn language: A developmental approach to situated\\nhuman-robot conversations,” in Human Robot Interaction . InTech,\\n2007, pp. 95–118.\\n[5] N. Iwahashi, R. Taguchi, K. Sugiura, K. Funakoshi, and M. Nakano,\\n“Robots that learn to converse: Developmental approach to situated\\nlanguage processing,” in Proceedings of International Symposium on\\nSpeech and Language Processing , 2009, pp. 532–537.\\n[6] P. Gorniak and D. Roy, “Probabilistic grounding of situated speech using\\nplan recognition and reference resolution,” in Proceedings of the 7th\\ninternational conference on Multimodal interfaces . ACM, 2005, pp.\\n138–143.\\n[7] S. Qu and J. Y . Chai, “Incorporating temporal and semantic information\\nwith eye gaze for automatic word acquisition in multimodal conversa-\\ntional systems,” in Proceedings of the Conference on Empirical Methods\\nin Natural Language Processing , 2008, pp. 244–253.\\n[8] ——, “Context-based word acquisition for situated dialogue in a virtual\\nworld,” Journal of Artiﬁcial Intelligence Research , vol. 37, no. 1, pp.\\n247–278, 2010.\\n[9] J. H ¨ornstein, L. Gustavsson, J. Santos-Victor, and F. Lacerda, “Multi-\\nmodal language acquisition based on motor learning and interaction,”\\ninFrom Motor Learning to Interaction Learning in Robots . Springer,\\n2010, pp. 467–489.\\n[10] T. Nakamura, T. Araki, T. Nagai, and N. Iwahashi, “Grounding of word\\nmeanings in latent Dirichlet allocation-based multimodal concepts,”\\nAdvanced Robotics , vol. 25, no. 17, pp. 2189–2206, 2011.\\n[11] T. Araki, T. Nakamura, T. Nagai, S. Nagasaka, T. Taniguchi, and\\nN. Iwahashi, “Online learning of concepts and words using multimodal\\nLDA and hierarchical Pitman-Yor Language Model,” in IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems (IROS) .\\nIEEE, 2012, pp. 1623–1630.\\n[12] P. F. Brown, V . J. D. Pietra, S. A. D. Pietra, and R. L. Mercer, “The\\nmathematics of statistical machine translation: Parameter estimation,”\\nComputational linguistics , vol. 19, no. 2, pp. 263–311, 1993.\\n[13] T. Nakamura, T. Nagai, and N. Iwahashi, “Multimodal categorization by\\nhierarchical Dirichlet process,” in IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS) . IEEE, 2011, pp. 1520–1525.\\n[14] D. Mochihashi, T. Yamada, and N. Ueda, “Bayesian unsupervised\\nword segmentation with nested Pitman-Yor language modeling,” in\\nProceedings of the Joint Conference of the 47th Annual Meeting of the\\nACL and the 4th International Joint Conference on Natural Language\\nProcessing of the AFNLP (ACL-IJCNLP) , 2009, pp. 100–108.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 11}), Document(page_content='13\\nBefore\\nAfter\\n(a) The name of the place was “toire. ”\\nBefore\\nAfter (b) The name of the place was “souhatsukeN. ”\\nBefore\\nAfter (c) The name of the place was “gomibako. ”\\nFig. 12. States of particles: before the teaching utterance (top); after the teaching utterance (bottom). The uttered sentence is “kokowa ** dayo, ” (which\\nmeans “Here is **. ” )“**” is the name of the place.\\n[15] R. Taguchi, N. Iwahashi, K. Funakoshi, M. Nakano, T. Nose, and\\nT. Nitta, “Learning physically grounded lexicons from spoken utter-\\nances,” Human Machine Interaction–Getting Closer , pp. 69–84, 2012.\\n[16] R. Taguchi, Y . Yamada, K. Hattori, T. Umezaki, M. Hoguro, N. Iwa-\\nhashi, K. Funakoshi, and M. Nakano, “Learning place-names from\\nspoken utterances and localization results by mobile robot,” in Annual\\nConference of the International Speech Communication Association\\n(INTERSPEECH) , 2011, pp. 1325–1328.\\n[17] M. Milford, G. Wyeth, and D. Prasser, “RatSLAM: a hippocampal model\\nfor simultaneous localization and mapping,” in IEEE International\\nConference on Robotics and Automation (ICRA) , 2004, pp. 403–408.\\n[18] M. Milford, R. Schulz, D. Prasser, G. Wyeth, and J. Wiles, “Learning\\nspatial concepts from RatSLAM representations,” Robotics and Au-\\ntonomous Systems , vol. 55, no. 5, pp. 403–410, 2007.\\n[19] R. Schulz, G. Wyeth, and J. Wiles, “Lingodroids: socially grounding\\nplace names in privately grounded cognitive maps,” Adaptive Behavior ,\\nvol. 19, no. 6, pp. 409–424, 2011.\\n[20] S. Heath, D. Ball, R. Schulz, and J. Wiles, “Communication between\\nLingodroids with different cognitive capabilities,” in IEEE International\\nConference on Robotics and Automation (ICRA) . IEEE, 2013, pp. 490–\\n495.\\n[21] R. Schulz, G. Wyeth, and J. Wiles, “Are we there yet? grounding tem-\\nporal concepts in shared journeys,” IEEE Transactions on Autonomous\\nMental Development , vol. 3, no. 2, pp. 163–175, 2011.\\n[22] K. Welke, P. Kaiser, A. Kozlov, N. Adermann, T. Asfour, M. Lewis,\\nand M. Steedman, “Grounded spatial symbols for task planning based\\non experience,” in 13th International Conference on Humanoid Robots\\n(Humanoids). IEEE/RAS , 2013.\\n[23] G. Neubig, M. Mimura, and T. Kawahara, “Bayesian learning of a\\nlanguage model from continuous speech,” IEICE TRANSACTIONS on\\nInformation and Systems , vol. 95, no. 2, pp. 614–625, 2012.\\n[24] F. Dellaert, D. Fox, W. Burgard, and S. Thrun, “Monte carlo localization\\nfor mobile robots,” in IEEE International Conference on Robotics and\\nAutomation (ICRA) , vol. 2. IEEE, 1999, pp. 1322–1328.\\n[25] J. Sethuraman, “A constructive deﬁnition of Dirichlet priors,” Statistica\\nSinica , vol. 4, pp. 639–650, 1994.\\n[26] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, “A sticky\\nHDP-HMM with application to speaker diarization,” The Annals of\\nApplied Statistics , pp. 1020–1056, 2011.\\n[27] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics . MIT Press,\\n2005.\\n[28] M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit, “FastSLAM: A\\nfactored solution to the simultaneous localization and mapping prob-\\nlem,” in In Proceedings of the AAAI National Conference on Artiﬁcial\\nIntelligence . American Association for Artiﬁcial Intelligence, 2002,\\npp. 593–598.\\n[29] D. Hahnel, W. Burgard, D. Fox, and S. Thrun, “An efﬁcient FastSLAM\\nalgorithm for generating maps of large-scale cyclic environments from\\nraw laser range measurements,” in IEEE/RSJ International Conference\\non Intelligent Robots and Systems (IROS) , 2003, pp. 206–211.\\n[30] G. Kitagawa, “Computational aspects of sequential Monte Carlo ﬁlter\\nand smoother,” Annals of the Institute of Statistical Mathematics , vol. 66,\\nno. 3, pp. 443–471, 2014.[31] T. Inamura, T. Shibata, H. Sena, T. Hashimoto, N. Kawai, T. Miyashita,\\nY . Sakurai, M. Shimizu, M. Otake, K. Hosoda, et al. , “Simulator\\nplatform that enables social interaction simulation –SIGVerse: SocioIn-\\ntelliGenesis simulator–,” in IEEE/SICE International Symposium on\\nSystem Integration , 2010, pp. 212–217.\\n[32] T. Kawahara, T. Kobayashi, K. Takeda, N. Minematsu, K. Itou, M. Ya-\\nmamoto, A. Yamada, T. Utsuro, and K. Shikano, “Sharable software\\nrepository for Japanese large vocabulary continuous speech recognition,”\\ninFifth International Conference on Spoken Language Processing , 1998.\\n[33] A. Lee, T. Kawahara, and K. Shikano, “Julius—an open source real-\\ntime large vocabulary recognition engine,” in European Conference on\\nSpeech Communication and Technology (EUROSPEECH) , 2001.\\n[34] L. Hubert and P. Arabie, “Comparing partitions,” Journal of classiﬁca-\\ntion, vol. 2, no. 1, pp. 193–218, 1985.\\n[35] T. Nakamura, T. Araki, T. Nagai, S. Nagasaka, T. Taniguchi, and N. Iwa-\\nhashi, “Multimodal concept and word learning using phoneme sequences\\nwith errors,” in IEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS) , 2013, pp. 157–162.\\n[36] T. Nakamura, T. Nagai, K. Funakoshi, S. Nagasaka, T. Taniguchi, and\\nN. Iwahashi, “Mutual learning of an object concept and language model\\nbased on MLDA and NPYLM,” in IEEE/RSJ International Conference\\non Intelligent Robots and Systems (IROS) , 2014, pp. 600–607.\\n[37] J. Heymann, O. Walter, R. Haeb-Umbach, and B. Raj, “Iterative\\nBayesian Word Segmentation for Unsupervised V ocabulary Discovery\\nfrom Phoneme Lattices,” in 39th International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP) , 2014.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01208.pdf', 'page': 12}), Document(page_content='Evaluating the Performance of a Speech\\nRecognition based System\\nVinod Kumar Pandey, Sunil Kumar Kopparapu\\nTCS Innovation Labs - Mumbai,\\nTata Consultancy Services, Pokharan Road 2\\nThane 400 601, Maharastra, India.\\n{vinod.pande, sunilkumar.kopparapu }@tcs.com\\nAbstract. Speech based solutions have taken center stage with growth\\nin the services industry where there is a need to cater to a very large\\nnumber of people from all strata of the society. While natural language\\nspeech interfaces are the talk in the research community, yet in practice,\\nmenu based speech solutions thrive. Typically in a menu based speech\\nsolution the user is required to respond by speaking from a closed set\\nof words when prompted by the system. A sequence of human speech\\nresponse to the IVR prompts results in the completion of a transaction.\\nA transaction is deemed successful if the speech solution can correctly\\nrecognize all the spoken utterances of the user whenever prompted by the\\nsystem. The usual mechanism to evaluate the performance of a speech\\nsolution is to do an extensive test of the system by putting it to actual\\npeople use and then evaluating the performance by analyzing the logs for\\nsuccessful transactions. This kind of evaluation could lead to dissatisﬁed\\ntest users especially if the performance of the system were to result in\\na poor transaction completion rate. To negate this the Wizard of Oz\\napproach is adopted during evaluation of a speech system. Overall this\\nkind of evaluations is an expensive proposition both in terms of time and\\ncost. In this paper, we propose a method to evaluate the performance\\nof a speech solution without actually putting it to people use . We ﬁrst\\ndescribe the methodology and then show experimentally that this can\\nbe used to identify the performance bottlenecks of the speech solution\\neven before the system is actually used thus saving evaluation time and\\nexpenses.\\nKeywords: Speech solution evaluation, Speech recognition, Pre-launch recog-\\nnition performance measure.\\n1 Introduction\\nThere are several commercial menu based ASR systems available around the\\nworld for a signiﬁcant number of languages and interestingly speech solution\\nbased on these ASR are being used with good success in the Western part of\\nthe globe [5], [3], [6], [2]. Typically, a menu based ASR system restricts user toarXiv:1601.02543v1  [cs.CL]  11 Jan 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 0}), Document(page_content='2\\nspeak from a pre-deﬁned closed set of words for enabling a transaction. Before\\ncommercial deployment of a speech solution it is imperative to have a quan-\\ntitative measure of the performance of the speech solution which is primarily\\nbased on the speech recognition accuracy of the speech engine used. Generally,\\nthe recognition performance of any speech recognition based solution is quanti-\\ntatively evaluated by putting it to actual use by the people who are the intended\\nusers and then analyzing the logs to identify successful and unsuccessful trans-\\nactions. This evaluation is then used to identifying any further improvement in\\nthe speech recognition based solution to better the overall transaction comple-\\ntion rates. This process of evaluation is both time consuming and expensive. For\\nevaluation one needs to identify a set of users and also identify the set of actual\\nusage situations and perform the test. It is also important that the set of users\\nare able to use the system with ease meaning that even in the test conditions\\nthe performance of the system, should be good, while this can not usually be\\nguaranteed this aspect of keeping the user experience good makes it necessary to\\nemploy a wizard of Oz (WoZ) approach. Typically this requires a human agent\\nin the loop during actual speech transaction where the human agent corrects any\\nmis-recognition by actually listening to the conversation between the human user\\nand the machine without the user knowing that there is a human agent in the\\nloop. The use of WoZ is another expense in the testing a speech solution. All this\\nmakes testing a speech solution an expensive and time consuming procedure.\\nIn this paper, we describe a method to evaluate the performance of a speech\\nsolution without actual people using the system as is usually done. We then show\\nhow this method was adopted to evaluate a speech recognition based solution as\\na case study. This is the main contribution of the paper. The rest of the paper\\nis organized as follows. The method for evaluation without testing is described\\nin Section 2. In Section 3 we present a case study and conclude in Section 4.\\n2 Evaluation without Testing\\nFig. 1 shows the schematic of a typical menu based speech solution having 3\\nnodes. At each node there are a set of words that the user is expected to speak\\nand the system is supposed to recognize. In this particular schematic, at the\\nentry node the user can speak any of the nwords, namely W1orW2or···or\\nWn;nis usually called the perplexity of the node in the speech literature. The\\nlarger the nthe more the perplexity and higher the confusion and hence lower\\nthe recognition accuracies. In most commercial speech solutions the perplexity\\nis kept very low, typically a couple of words. Once the word at the entry node\\nhas been recognized (say word Wkhas been recognized), the system moves on\\nto the second node where the active list of words to be recognized could be\\none of Wk1,Wk2,Wk3, ...Wkpif the perplexity at the kthnode is p. This is\\ncarried on to the third node. A transaction is termed successful if and only if\\nthe recognition at each of the three nodes is correct. For example, typically in a\\nbanking speech solution the entry node could expect someone to speak among', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 1}), Document(page_content='3\\nFig. 1. Schematic of a typical menu based ASR system ( Wnis spoken word).\\n/credit card /1, /savings account /, /current account /, /loan product /, /demat /,\\nand / mutual fund transfer / which has a perplexity of 6. Once a person speaks,\\nsay, / savings account / and is recognized correctly by the system, at the second\\nnode it could be / account balance / or / cheque / or / last5transactions / (perplex-\\nity 3) and at the third node (say, on recognition of / cheque /) it could be / new\\ncheque book request /, /cheque status /, and / stop cheque request / (perplexity 3).\\nNote 1. Though we will not dwell on this, it is important to note that an error\\nin recognition at the entry node is more expensive than a recognition error at a\\nlower node.\\nBased on the call ﬂow, and the domain the system can have several nodes for\\ncompletion of a transaction. Typical menu based speech solutions strive for a\\n3 - 5 level nodes to make it usable. In any speech based solution (see Fig. 2)\\nﬁrst the spoken utterance is hypothesized into a sequence of phonemes using\\nthe acoustic models. Since the phoneme recognition accuracy is low, instead of\\nchoosing one phoneme it identiﬁes l-best (typically l= 3) matching phonemes.\\nThis phone lattice is then matched with all the expected words (language model)\\nat that node to ﬁnd the best match. For a node with perplexity nthe constructed\\nphoneme lattice of the spoken utterance is compared with the phoneme sequence\\nrepresentation of all the nwords (through the lexicon which is one of he key\\n1We will use / / to indicate the spoken word. For example / W1/ represents the spoken\\nequivalent of the written word W1.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 2}), Document(page_content='4\\n-?\\n??Acoustic Models\\nText OutputLexicon\\nLanguage Model SpeechSpeech\\nRecognition\\nFig. 2. A typical speech recognition system. In a menu based system the language\\nmodel is typically the set of words that need to be recognized at a given node.\\ncomponents of a speech recognition system). The hypothesized phone lattice is\\ndeclared one of the nwords depending on the closeness of the phoneme lattice\\nto the phoneme representation of the nwords.\\nWe hypothesize that we can identify the performance of a menu based speech\\nsystem by identifying the possible confusion among all the words that are active\\nat a given node.\\nNote 2. If active words at a given node are phonetically similar it becomes dif-\\nﬁcult for the speech recognition system to distinguish them which in turn leads\\nto recognition errors.\\nWe used Levenshtein distance [1], [4] a well known measure to analyze and\\nidentify the confusion among the active words at a given node. This analysis\\ngives a list of all set of words that have a high degree of confusability among\\nthem; this understanding can be then used to (a) restructure the set of active\\nwords at that node and/or (b) train the words that can be confused by using\\na larger corpus of speech data. This allows the speech recognition engine to be\\nequipped to be able to distinguish the confusing words better. Actual use of\\nthis analysis was carried out for a speech solution developed for Indian Railway\\nInquiry System to identify bottlenecks in the system before its actual launch.\\n3 Case Study\\nA schematic of a speech based Railway Information system, developed for Hindi\\nlanguage is shown in Fig. 3. The system enables user to get information on ﬁve', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 3}), Document(page_content='5\\nFig. 3. Call ﬂow of Indian Railway Inquiry System ( Wnis spoken word)\\ndiﬀerent services, namely, (a) Arrival of a given train at a given station, (b)\\nDeparture of a given train at a given station, (c) Ticket availability on\\na given date in a given train between two stations, and class, (d) Fare in a\\ngiven class in a given train between two stations, and (e) PNR status . At the\\nﬁrst recognition node (node-1), there are one or more active words correspond-\\ning to each of these services. For example, for selecting the service Fare , the\\nuser can speak among / kiraya jankari /, /kiraya /, /fare/. Similarly, for selecting\\nservice Ticket availability , user can speak / upalabdhata jankari / or / ticket\\navailability / or / upalabdhata /.\\nNote 3. Generally the perplexity at a node is greater than on equal to the num-\\nber of words that need to be recognized at that node.\\nIn this manner each of the services could have multiple words or phrases that\\ncan mean the same thing and the speaker could utter any of these words to refer\\nto that service. The sum of all the possible diﬀerent ways in which a service can\\nbe called ( di) summed over all the 5 services gives the perplexity ( N) at that\\nnode, namely,\\nN=5∑\\ni=0di (1)\\nThe speech recognition engine matches the phoneme lattice of the spoken ut-\\nterance with all the Nwords which are active. The active word (one among\\ntheNwords) with highest likelihood score is the recognized word. In order to\\navoid low likelihood recognitions a threshold is set so that even the best likeli-\\nhood wordis returned only if the likelihood score is greater than the predeﬁned\\nthreshold. Completion of a service requires recognitions at several nodes with\\ndiﬀerent perplexity at each node. Clearly depending on the type of service that\\nthe user is wanting to use; the user has to go through diﬀerent number of recog-\\nnition nodes. For example, to complete the Arrival service it is required to', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 4}), Document(page_content='6\\npass through 3 recognition nodes namely (a) selection of a service, (b) selection\\nof a train name and (c) selection of the railway station. While the perplexity\\n(the words that are active) at the service selection node is ﬁxed the perplexity\\nat the station selection node could depend on the selection of the train name at\\nan earlier node. For example, if the selected train stops at 23 stations, then the\\nperplexity at the station selection node will be ≥23.\\nFor confusability analysis at each of the node, we have used the Levenshtein\\ndistance [4] or the edit distance as is well known in computer science literature.\\nWe found that the utterances / Sahi/ and / Galat / have 100% recognition. These\\nwords Sahi is represented by the string of phonemes in the lexicon as S AA HH\\nIand the word Galat is represented as the phoneme sequence G L AX tT in the\\nlexicon. We identiﬁed the edit distance between these two words Sahi and Galat\\nand used that distance measure as the threshold that is able to diﬀerentiate any\\ntwo words (say T). So if the distance between any two active words at a given\\nrecognition node is lower than the threshold T, then there is a greater chance\\nthat those two active words could get confused (one word could be recognized as\\nthe other which is within a distance of T). There are ways in which this possible\\nmisrecognition words could be avoided. The easiest way is to make sure that\\nthese two words together are not active at a given recognition node.\\nTable 1. List of Active Words at node 1\\nW. No. Active Word Phonetic\\nW1 kiraya jankari K I R AA Y AA J AA tN K AA R I\\nW2 kiraya K I R AA Y AA\\nW3 fare F AY R\\nW4 aagaman jankari AA G AX M AX tN J AA tN K AA R I\\nW5 aagaman AA G AX M AX tN\\nW6 arrival departure AX R AA I V AX L dD I P AA R CH AX R\\nW7 upalabdhata jankari U P AX L AX B tDH AX tT AA J AA tN K AA R I\\nW8 ticket availability tT I K EY tT AX V AY L AX B I L I tT Y\\nW9 upalabdhata U P AX L AX B tD AX tTH AA\\nW10 arrival AX R AA I V AX L\\nW11 prasthan P R AX S tTH AA tN\\nW12 departure dD I P AA R CH AX R\\nW13 pnrjankari P I EY tN AA R J AA tN K AA R I\\nW14 pnr P I AX tN AA R\\nTable 1 shows the list of active word at the node 1 when the speech applica-\\ntion was initially designed and Table 2 shows the edit distance between all the\\nactive words at the node service given in Fig. 3. The distance between words Sahi\\nand Galat was found to be 5 .7 which was set at the threshold, namely T= 5.7.\\nThis threshold value was used to identify confusing active words. Clearly, as seen', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 5}), Document(page_content='7\\nin the Table the distance between word pairs fare,pnrand pnr,prasthan is 5.2\\nand 5 .8 respectively, which is very close to the threshold value of 5 .7. This can\\ncause a high possibility that / fare/ may get recognized as pnrand vice-versa.\\nOne can derive from the analysis of the active words that fare andpnrcan\\nTable 2. Distance Measurement for Active Words at Node 1 of the Railway Inquiry\\nSystem\\nW1W2W3W4W5W6W7W8W9W10W11W12W13W14\\nW108.414.28.514.115.3112017.113 13 13.2 6.2 11.2\\nW28.407.213.88.514.717.815.7117.2 7.8 7.7 11.2 6.2\\nW314.27.2014.27.214.818.215.811.28.2 8.2 7.8 14.25.8\\nW48.513.814.208.415.99.619.714.313 13 14.7 8.2 11.2\\nW514.18.57.28.4013.216.715.79.77.2 6.7 8.2 14.1 7.1\\nW615.314.714.815.913.2018.718.915.59.4 13.7 7 17.3 11.8\\nW71117.818.29.716.718.702011.217.1 15.6 17.8 10.2 13.8\\nW82015.715.819.715.718.920014.515.8 17.5 16.5 18.6 15.7\\nW917.11111.214.39.715.511.214.5010 9.2 11.1 16.3 9.7\\nW10137.28.213.17.29.417.115.8100 8.5 8 13 7.8\\nW1113.17.88.213.16.713.715.717.59.28.5 0 8.4 11.75.2\\nW1213.27.77.814.78.2717.816.511.18.1 8.4 0 12.1 6.8\\nW136.211.214.28.214.117.310.218.616.313.1 11.7 12.1 0 9.8\\nW1411.26.25.811.27.111.813.815.79.67.85.2 6.8 9.8 0\\nnot coexist as active words at the same node. The result of the analysis was to\\nremove the active words fareandpnrat that node.\\nWhen the speech system was actually tested by giving speech samples, 17\\nout of 20 instances of / pnr/ was was recognized as fareand vice-versa. Similarly\\n19 out of 20 instances / pnr/ was misrecognized as prasthan and vice versa This\\nconfusion is expected as can be seen from the edit distance analysis of the active\\nwords in the Table 2. This modiﬁed active word list (removal of fareandpnr)\\nincreased the recognition accuracy at the service node (Fig. 3) by as much as\\n90%.\\nA similar analysis was carried out at other recognition nodes and the active\\nword list was suitably modiﬁed to avoid possible confusion between active word\\npair. This analysis and modiﬁcation of the list of active words at a node resulted\\nin a signiﬁcant improvement in the transaction completion rate. We will present\\nmore experimental results in the ﬁnal paper.\\n4 Conclusion\\nIn this paper we proposed a methodology to identify words that could lead to\\nconfusion at any given node of a speech recognition based system. We used', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 6}), Document(page_content='8\\nedit distance as the metric to identifying the possible confusion between the\\nactive words. We showed that this metric can be used eﬀectively to enhance the\\nperformance of a speech solution without actually putting it to people test. There\\nis a signiﬁcant saving in terms of being able to identify recognition bottlenecks in\\na menu based speech solution through this analysis because it does not require\\nactual people testing the system. This methodology was adopted to restructuring\\nthe set of active words at each node for better speech recognition in an actual\\nmenu based speech recognition system that caters to masses.\\nReferences\\n1. Gusﬁeld, D.: Algorithms on Strings, Trees, and Sequences: Computer Science and\\nComputational Biology. Cambridge University Press (2001)\\n2. Kim, C., Stern, R.: Feature extraction for robust speech recognition based on max-\\nimizing the sharpness of the power distribution and on power ﬂooring. IEEE In-\\nternational Conference on Acoustics Speech and Signal Processing pp. 4574–4577\\n(2010)\\n3. Lua, X., Matsudaa, S., Unokib, M., Nakamuraa, S.: Temporal contrast normaliza-\\ntion and edge-preserved smoothing of temporal modulation structures of speech for\\nrobust speech recognition. Speech Communication 52, 1–11 (2010)\\n4. Navarro, G.: A guided tour to approximate string matching. ACM Computing Sur-\\nveys 33, 31–88 (2001)\\n5. Sun, Y., Gemmeke, J., Cranen, B., Bosch, L., Boves, L.: Using a dbn to integrate\\nsparse classiﬁcation and gmm-based asr. Proceedings of Interspeech 2010 (2010)\\n6. Zhao, Y., Juang, B.: A comparative study of noise estimation algorithms for vts-\\nbased robust speech recognition. Proceedings of Interspeech 2010 (2010)', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.02543.pdf', 'page': 7}), Document(page_content='Many Languages, One Parser\\nWaleed Ammar♦George Mulcaire♥Miguel Ballesteros♠♦Chris Dyer♦Noah A. Smith♥\\n♦School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA\\n♥Computer Science & Engineering, University of Washington, Seattle, WA, USA\\n♠NLP Group, Pompeu Fabra University, Barcelona, Spain\\nwammar@cs.cmu.edu, gmulc@uw.edu, miguel.ballesteros@upf.edu\\ncdyer@cs.cmu.edu, nasmith@cs.washington.edu\\nAbstract\\nWe train one multilingual model for depen-\\ndency parsing and use it to parse sentences\\nin several languages. The parsing model\\nuses (i) multilingual word clusters and em-\\nbeddings; (ii) token-level language informa-\\ntion; and (iii) language-speciﬁc features (ﬁne-\\ngrained POS tags). This input representation\\nenables the parser not only to parse effec-\\ntively in multiple languages, but also to gener-\\nalize across languages based on linguistic uni-\\nversals and typological similarities, making it\\nmore effective to learn from limited annota-\\ntions. Our parser’s performance compares fa-\\nvorably to strong baselines in a range of data\\nscenarios, including when the target language\\nhas a large treebank, a small treebank, or no\\ntreebank for training.\\n1 Introduction\\nDeveloping tools for processing many languages\\nhas long been an important goal in NLP (Rösner,\\n1988; Heid and Raab, 1989),1but it was only when\\nstatistical methods became standard that massively\\nmultilingual NLP became economical. The main-\\nstream approach for multilingual NLP is to design\\nlanguage-speciﬁc models. For each language of in-\\nterest, the resources necessary for training the model\\nare obtained (or created), and separate parameters\\nare ﬁt for each language separately. This approach\\nis simple and grants the ﬂexibility of customizing\\n1As of 2007, the total number of native speakers of the\\nhundred most popular languages only accounts for 85% of the\\nworld’s population (Wikipedia, 2016).the model and features to the needs of each lan-\\nguage, but it is suboptimal for theoretical and prac-\\ntical reasons. Theoretically, the study of linguistic\\ntypology tells us that many languages share mor-\\nphological, phonological, and syntactic phenomena\\n(Bender, 2011); therefore, the mainstream approach\\nmisses an opportunity to exploit relevant supervi-\\nsion from typologically related languages. Practi-\\ncally, it is inconvenient to deploy or distribute NLP\\ntools that are customized for many different lan-\\nguages because, for each language of interest, we\\nneed to conﬁgure, train, tune, monitor, and occasion-\\nally update the model. Furthermore, code-switching\\nor code-mixing (mixing more than one language in\\nthe same discourse), which is pervasive in some gen-\\nres, in particular social media, presents a challenge\\nfor monolingually-trained NLP models (Barman et\\nal., 2014).2\\nIn parsing, the availability of homogeneous syn-\\ntactic dependency annotations in many languages\\n(McDonald et al., 2013; Nivre et al., 2015b; Agi ´c\\net al., 2015; Nivre et al., 2015a) has created an\\nopportunity to develop a parser that is capable of\\nparsing sentences in multiple languages, address-\\ning these theoretical and practical concerns.3A\\nmultilingual parser can potentially replace an array\\nof language-speciﬁc monolingually-trained parsers\\n2While our parser can be used to parse input with code-\\nswitching, we have not evaluated this capability due to the lack\\nof appropriate data.\\n3Although multilingual dependency treebanks have been\\navailable for a decade via the 2006 and 2007 CoNLL shared\\ntasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the tree-\\nbank of each language was annotated independently and with\\nits own annotation conventions.arXiv:1602.01595v4  [cs.CL]  26 Jul 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 0}), Document(page_content='(for languages with a large treebank). The same\\napproach has been used in low-resource scenarios\\n(with no treebank or a small treebank in the target\\nlanguage), where indirect supervision from auxiliary\\nlanguages improves the parsing quality (Cohen et\\nal., 2011; McDonald et al., 2011; Zhang and Barzi-\\nlay, 2015; Duong et al., 2015a; Duong et al., 2015b;\\nGuo et al., 2016), but these models may sacriﬁce ac-\\ncuracy on source languages with a large treebank. In\\nthis paper, we describe a model that works well for\\nboth low-resource and high-resource scenarios.\\nWe propose a parsing architecture that takes as in-\\nput sentences in several languages,4optionally pre-\\ndicting the part-of-speech (POS) tags and input lan-\\nguage. The parser is trained on the union of avail-\\nable universal dependency annotations in different\\nlanguages. Our approach integrates and critically\\nrelies on several recent developments related to de-\\npendency parsing: universal POS tagsets (Petrov et\\nal., 2012), cross-lingual word clusters (Täckström et\\nal., 2012), selective sharing (Naseem et al., 2012),\\nuniversal dependency annotations (McDonald et al.,\\n2013; Nivre et al., 2015b; Agi ´c et al., 2015; Nivre\\net al., 2015a), advances in neural network architec-\\ntures (Chen and Manning, 2014; Dyer et al., 2015),\\nand multilingual word embeddings (Gardner et al.,\\n2015; Guo et al., 2016; Ammar et al., 2016). We\\nshow that our parser compares favorably to strong\\nbaselines trained on the same treebanks in three data\\nscenarios: when the target language has a large tree-\\nbank (Table 3), a small treebank (Table 7), or no\\ntreebank (Table 8). Our parser is publicly available.5\\n2 Overview\\nOur goal is to train a dependency parser for a set\\nof target l anguagesLt, given universal dependency\\nannotations in a set of s ource l anguagesLs. Ide-\\nally, we would like to have training data in all tar-\\nget languages (i.e., Lt⊆Ls), but we are also inter-\\nested in the case where the sets of source and target\\nlanguages are disjoint (i.e., Lt∩Ls=∅). When\\nall languages in Lthave a large treebank, the main-\\nstream approach has been to train one monolingual\\nparser per target language and route sentences of a\\n4We discuss data requirements in the next section.\\n5https://github.com/clab/\\nlanguage-universal-parsergiven language to the corresponding parser at test\\ntime. In contrast, our approach is to train one pars-\\ning model with the union of treebanks in Ls, then\\nuse this single trained model to parse text in any lan-\\nguage inLt, hence the name “Ma ny Languages, O ne\\nParser” (M ALOP A). M ALOP Astrikes a balance be-\\ntween: (1) enabling cross-lingual model transfer via\\nlanguage-invariant input representations; i.e., coarse\\nPOS tags, multilingual word embeddings and mul-\\ntilingual word clusters, and (2) tweaking the be-\\nhavior of the parser depending on the current input\\nlanguage via language-speciﬁc representations; i.e.,\\nﬁne-grained POS tags and language embeddings.\\nIn addition to universal dependency annotations\\nin source languages (see Table 1), we use the follow-\\ning data resources for each language in L=Lt∪Ls:\\n•universal POS annotations for training a POS tag-\\nger,6\\n•a bilingual dictionary with another language in L\\nfor adding cross-lingual lexical information,7\\n•language typology information,8\\n•language-speciﬁc POS annotations,9and\\n•a monolingual corpus.10\\nNovel contributions of this paper include: (i) us-\\ning one parser instead of an array of monolingually-\\ntrained parsers without sacriﬁcing accuracy on lan-\\nguages with a large treebank, (ii) an effective neural\\nnetwork architecture for using language embeddings\\nto improve multilingual parsing, and (iii) a study\\nof how automatic language identiﬁcation affects the\\nperformance of a multilingual dependency parser.\\nWhile not the primary focus of this paper, we also\\nshow that a variant of our parser outperforms pre-\\nvious work on multi-source cross-lingual parsing in\\n6See §3.6 for details.\\n7Our best results make use of this resource. We require that\\nall languages in Lare (transitively) connected. The bilingual\\ndictionaries we used are based on unsupervised word align-\\nments of parallel corpora, as described in Guo et al. (2016).\\nSee §3.3 for details.\\n8See §3.4 for details.\\n9Our best results make use of this resource. See §3.5 for\\ndetails.\\n10This is only used for training word embeddings with ‘mul-\\ntiCCA,’ ‘multiCluster’ and ‘translation-invariance’ methods in\\nTable 6. We do not use this resource when we compare to pre-\\nvious work.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 1}), Document(page_content='German (de) English (en) Spanish (es) French (fr) Italian (it) Portuguese (pt) Swedish (sv)\\nUDT 2train 14118 (264906) 39832 (950028) 14138 (375180) 14511 (351233) 6389 (149145) 9600 (239012) 4447 (66631)\\ndev. 801 (12215) 1703 (40117) 1579 (40950) 1620 (38328) 399 (9541) 1211 (29873) 493 (9312)\\ntest 1001 (16339) 2416 (56684) 300 (8295) 300 (6950) 400 (9187) 1205 (29438) 1219 (20376)\\nUD 1.2train 14118 (269626) 12543 (204586) 14187 (382436) 14552 (355811) 11699 (249307) 8800 (201845) 4303 (66645)\\ndev. 799 (12512) 2002 (25148) 1552 (41975) 1596 (39869) 489 (11656) 271 (4833) 504 (9797)\\ntest 977 (16537) 2077 (25096) 274 (8128) 298 (7210) 489 (11719) 288 (5867) 1219 (20377)\\ntags - 50 - - 36 866 134\\nTable 1: Number of sentences (tokens) in each treebank split in Universal Dependency Treebanks (UDT)\\nversion 2.0 and Universal Dependencies version (UD) 1.2 for the languages we experiment with. The last\\nrow gives the number of unique language-speciﬁc ﬁne-grained POS tags used in a treebank.\\nlow resource scenarios, where languages in Lthave\\na small treebank (see Table 7) or where Lt∩Ls=∅\\n(see Table 8). In the small treebank setup with 3,000\\ntoken annotations, we show that our parser consis-\\ntently outperforms a strong monolingual baseline\\nwith 5.7 absolute LAS (labeled attachment score)\\npoints per language, on average.\\n3 Parsing Model\\nRecent advances suggest that recurrent neural net-\\nworks, especially long short-term memory (LSTM)\\narchitectures, are capable of learning useful repre-\\nsentations for modeling problems of sequential na-\\nture (Graves et al., 2013; Sutskever et al., 2014).\\nIn this section, we describe our language-universal\\nparser, which extends the stack LSTM (S-LSTM)\\nparser of Dyer et al. (2015).\\n3.1 Transition-based Parsing with S-LSTMs\\nThis section brieﬂy reviews Dyer et al.’s S-LSTM\\nparser, which we modify in the following sections.\\nThe core parser can be understood as the sequential\\nmanipulation of three data structures:\\n•a buffer (from which we read the token sequence),\\n•a stack (which contains partially-built parse trees),\\nand\\n•a list of actions previously taken by the parser.\\nThe parser uses the arc-standard transition system\\n(Nivre, 2004).11At each timestep t, a transition ac-\\ntion is applied that alters these data structures ac-\\ncording to Table 2.\\n11In a preprocessing step, we transform nonprojective trees\\nin the training treebanks to pseudo-projective trees using the\\n“baseline” scheme in (Nivre and Nilsson, 2005). We evaluate\\nagainst the original nonprojective test set.Along with the discrete transitions of the arc-\\nstandard system, the parser computes vector repre-\\nsentations for the buffer, stack and list of actions at\\ntime steptdenoted bt,st, and at, respectively.12\\nThe parser state at time tis given by:\\npt= max{0,W[st;bt;at] +Wbias} (1)\\nwhere the matrix Wand the vector Wbiasare\\nlearned parameters. The matrix Wis multiplied by\\nthe vector [st;bt;at]created by the concatenation of\\nst,bt,at. The parser state ptis then used to deﬁne\\na categorical distribution over possible next actions\\nz:13\\np(z|pt) =exp(\\ng⊤\\nzpt+qz)\\n∑\\nz′exp(\\ng⊤\\nz′pt+qz′) (2)\\nwhere gzandqzare parameters associated with ac-\\ntionz. The selected action is then used to update\\nthe buffer, stack and list of actions, and to compute\\nbt+1,st+1andat+1accordingly.\\nThe model is trained to maximize the log-\\nlikelihood of correct actions. At test time, the parser\\ngreedily chooses the most probable action in every\\ntime step until a complete parse tree is produced.\\nThe following sections describe our extensions of\\nthe core parser. More details about the core parser\\ncan be found in Dyer et al. (2015).\\n3.2 Token Representations\\nThe vector representations of input tokens feed into\\nthe stack-LSTM modules of the buffer and the stack.\\n12A stack-LSTM module is used to compute the vector rep-\\nresentation for each data structure, as detailed in Dyer et al.\\n(2015).\\n13The total number of actions is 1+2×the number of unique\\ndependency labels in the treebank used for training, but we only\\nconsider actions which meet the arc-standard preconditions in\\nFig. 2.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 2}), Document(page_content='Stack tBuffer tAction Dependency Stack t+1 Buffer t+1\\nu,v,S B REDUCE -RIGHT (r)ur→v u,S B\\nu,v,S B REDUCE -LEFT (r)ur←v v,S B\\nSu,B SHIFT — u,S B\\nTable 2: Parser transitions indicating the action applied to the stack and buffer at time tand the resulting\\nstack and buffer at time t+ 1.\\nFor monolingual parsing, we represent each token\\nby concatenating the following vectors:\\n•a ﬁxed, pretrained embedding of the word type,\\n•a learned embedding of the word type,\\n•a learned embedding of the Brown cluster,\\n•a learned embedding of the ﬁne-grained POS tag,\\n•a learned embedding of the coarse POS tag.\\nFor multilingual parsing with M ALOP A, we start\\nwith a simple delexicalized model where the token\\nrepresentation only consists of learned embeddings\\nof coarse POS tags, which are shared across all lan-\\nguages to enable model transfer. In the following\\nsubsections, we enhance the token representation in\\nMALOP Ato include lexical embeddings, language\\nembeddings, and ﬁne-grained POS embeddings.\\n3.3 Lexical Embeddings\\nPrevious work has shown that sacriﬁcing lexical fea-\\ntures amounts to a substantial decrease in the perfor-\\nmance of a dependency parser (Cohen et al., 2011;\\nTäckström et al., 2012; Tiedemann, 2015; Guo et al.,\\n2015). Therefore, we extend the token representa-\\ntion in M ALOP Aby concatenating learned embed-\\ndings of multilingual word clusters, and pretrained\\nmultilingual embeddings of word types.\\nMultilingual Brown clusters. Before training the\\nparser, we estimate Brown clusters of English words\\nand project them via word alignments to words in\\nother languages. This is similar to the ‘projected\\nclusters’ method in Täckström et al. (2012). To go\\nfrom Brown clusters to embeddings, we ignore the\\nhierarchy within Brown clusters and assign a unique\\nparameter vector to each cluster.\\nMultilingual word embeddings. We also use\\nGuo et al.’s (2016) ‘robust projection’ method to pre-\\ntrain multilingual word embeddings. The ﬁrst stepin ‘robust projection’ is to learn embeddings for En-\\nglish words using the skip-gram model (Mikolov et\\nal., 2013). Then, we compute an embedding of non-\\nEnglish words as the weighted average of English\\nword embeddings, using word alignment probabili-\\nties as weights. The last step computes an embed-\\nding of non-English words which are not aligned to\\nany English words by averaging the embeddings of\\nall words within an edit distance of 1 in the same\\nlanguage. We experiment with two other methods—\\n‘multiCCA’ and ‘multiCluster,’ both proposed by\\nAmmar et al. (2016)—for pretraining multilingual\\nword embeddings in §4.1. ‘MultiCCA’ uses a lin-\\near operator to project pretrained monolingual em-\\nbeddings in each language (except English) to the\\nvector space of pretrained English word embed-\\ndings, while ‘multiCluster’ uses the same embed-\\nding for translationally-equivalent words in different\\nlanguages. The results in Table 6 illustrate that the\\nthree methods perform similarly on this task.\\n3.4 Language Embeddings\\nWhile many languages, especially ones that belong\\nto the same family, exhibit some similar syntac-\\ntic phenomena (e.g., all languages have subjects,\\nverbs, and objects), substantial syntactic differences\\nabound. Some of these differences are easy to char-\\nacterize (e.g., subject-verb-object vs. verb-subject-\\nobject, prepositions vs. postpositions, adjective-\\nnoun vs. noun-adjective), while others are sub-\\ntle (e.g., number and positions of negation mor-\\nphemes). It is not at all clear how to translate de-\\nscriptive facts about a language’s syntax into fea-\\ntures for a parser.\\nConsequently, training a language-universal\\nparser on treebanks in multiple source languages\\nrequires caution. While exposing the parser to a\\ndiverse set of syntactic patterns across many lan-\\nguages has the potential to improve its performance', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 3}), Document(page_content='in each, dependency annotations in one language\\nwill, in some ways, contradict those in typologically\\ndifferent languages.\\nFor instance, consider a context where the next\\nword on the buffer is a noun, and the top word on\\nthe stack is an adjective, followed by a noun. Tree-\\nbanks of languages where postpositive adjectives\\nare typical (e.g., French) will often teach the parser\\nto predict REDUCE -LEFT , while those of languages\\nwhere prepositive adjectives are more typical (e.g.,\\nEnglish) will teach the parser to predict SHIFT .\\nInspired by Naseem et al. (2012), we address this\\nproblem by informing the parser about the input lan-\\nguage it is currently parsing. Let lbe the input vector\\nrepresentation of a particular language. We consider\\nthree deﬁnitions for l:14\\n•one-hot encoding of the language ID,\\n•one-hot encoding of individual word-order prop-\\nerties,15and\\n•averaged one-hot encoding of WALS typological\\nproperties (including word-order properties).16\\nIt is worth noting that the ﬁrst deﬁnition (language\\nID) turns out to work best in our experiments.\\nWe use a hidden layer with tanh nonlinearity to\\ncompute the language embedding l′as:\\nl′= tanh( Ll+Lbias)\\nwhere the matrix Land the vector Lbiasare addi-\\ntional model parameters. We modify the parsing ar-\\nchitecture as follows:\\n•include l′in the token representation (which feeds\\ninto the stack-LSTM modules of the buffer and\\nthe stack as described in §3.1),\\n14The ﬁles which contain these deﬁnitions are\\navailable at https://github.com/clab/\\nlanguage-universal-parser/tree/master/\\ntypological_properties .\\n15The World Atlas of Language Structures (WALS; Dryer\\nand Haspelmath, 2013) is an online portal documenting typo-\\nlogical properties of 2,679 languages (as of July 2015). We\\nuse the same set of WALS features used by Zhang and Barzilay\\n(2015), namely 82A (order of subject and verb), 83A (order of\\nobject and verb), 85A (order of adposition and noun phrase),\\n86A (order of genitive and noun), and 87A (order of adjective\\nand noun).\\n16Some WALS features are not annotated for all languages.\\nTherefore, we use the average value of all languages in the same\\ngenus. We rescale all values to be in the range [−1,1].•include l′in the action vector representation\\n(which feeds into the stack-LSTM module that\\nrepresents previous actions as described in §3.1),\\nand\\n•redeﬁne the parser state at time taspt=\\nmax{0,W[st;bt;at;l′] +Wbias}.\\nIntuitively, the ﬁrst two modiﬁcations allow the\\ninput language to inﬂuence the vector representation\\nof the stack, the buffer and the list of actions. The\\nthird modiﬁcation allows the input language to in-\\nﬂuence the parser state which in turn is used to pre-\\ndict the next action. In preliminary experiments, we\\nfound that adding the language embeddings at the\\ntoken and action level is important. We also experi-\\nmented with computing more complex functions of\\n(st,bt,at,l′) to deﬁne the parser state, but they did\\nnot help.\\n3.5 Fine-grained POS Tag Embeddings\\nTiedemann (2015) shows that omitting ﬁne-grained\\nPOS tags signiﬁcantly hurts the performance of a de-\\npendency parser. However, those ﬁne-grained POS\\ntagsets are deﬁned monolingually and are only avail-\\nable for a subset of the languages with universal de-\\npendency treebanks.\\nWe extend the token representation to include\\na ﬁne-grained POS embedding (in addition to the\\ncoarse POS embedding). We stochastically dropout\\nthe ﬁne-grained POS embedding for each token with\\n50% probability (Srivastava et al., 2014) so that the\\nparser can make use of ﬁne-grained POS tags when\\navailable but stay reliable when the ﬁne-grained\\nPOS tags are missing.\\n3.6 Predicting POS Tags\\nThe model discussed thus far conditions on the POS\\ntags of words in the input sentence. However, gold\\nPOS tags may not be available in real applications\\n(e.g., parsing the web). Here, we describe two mod-\\niﬁcations to (i) model both POS tagging and depen-\\ndency parsing, and (ii) increase the robustness of the\\nparser to incorrect POS predictions.\\nTagging model. Letx1,...,x n,y1,...,y n,\\nz1,...,z 2nbe the sequence of words, POS tags,\\nand parsing actions, respectively, for a sentence of\\nlengthn. We deﬁne the joint distribution of a POS', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 4}), Document(page_content='tag sequence and parsing actions given a sequence\\nof words as follows:\\np(y1,...,y n,z1,...,z 2n|x1,...,x n) =\\nn∏\\ni=1p(yi|x1,...,x n)\\n×2n∏\\nj=1p(zj|x1,...,x n,y1,...,y n,z1,...,z j−1)\\nwherep(zj|...)is deﬁned in Eq. 2, and p(yi|\\nx1,...,x n)uses a bidirectional LSTM (Graves et\\nal., 2013). Huang et al. (2015) show that the perfor-\\nmance of a bidirectional LSTM POS tagger is on par\\nwith a conditional random ﬁeld tagger.\\nWe use slightly different token representations for\\ntagging and parsing in the same model. For tag-\\nging, we construct the token representation by con-\\ncatenating the embeddings of the word type (pre-\\ntrained), the Brown cluster and the input language.\\nThis token representation feeds into the bidirectional\\nLSTM, followed by a softmax layer (at each posi-\\ntion) which deﬁnes a categorical distribution over\\npossible POS tags. For parsing , we construct the to-\\nken representation by further concatenating the em-\\nbeddings of predicted POS tags. This token repre-\\nsentation feeds into the stack-LSTM modules of the\\nbuffer and stack components of the transition-based\\nparser. This multi-task learning setup enables us to\\npredict both POS tags and dependency trees in the\\nsame model. We note that pretrained word embed-\\ndings, cluster embeddings and language embeddings\\nare shared for tagging and parsing.\\nBlock dropout. We use an independently devel-\\noped variant of word dropout (Iyyer et al., 2015),\\nwhich we call block dropout . The token representa-\\ntion used for parsing includes the embedding of pre-\\ndicted POS tags, which may be incorrect. We intro-\\nduce another modiﬁcation which makes the parser\\nmore robust to incorrect POS tag predictions, by\\nstochastically zeroing out the entire embedding of\\nthe POS tag. While training the parser, we replace\\nthe POS embedding vector ewith another vector (of\\nthe same dimensionality) stochastically computed\\nas:e′= (1−b)/µ×e, whereb∈ {0,1}is a\\nBernoulli-distributed random variable with parame-\\nterµwhich is initialized to 1.0 (i.e., always dropout,settingb= 1,e′= 0), and is dynamically updated\\nto match the error rate of the POS tagger on the de-\\nvelopment set. At test time, we never dropout the\\npredicted POS embedding, i.e., e′=e. Intuitively,\\nthis method extends the dropout method (Srivastava\\net al., 2014) to address structured noise in the input\\nlayer.\\n4 Experiments\\nIn this section, we evaluate the M ALOP Aapproach\\nin three data scenarios: when the target language has\\na large treebank (Table 3), a small treebank (Table 7)\\nor no treebank (Table 8).\\nData. For experiments where the target language\\nhas a large treebank, we use the standard data splits\\nfor German (de), English (en), Spanish (es), French\\n(fr), Italian (it), Portuguese (pt) and Swedish (sv) in\\nthe latest release (version 1.2) of Universal Depen-\\ndencies (Nivre et al., 2015a), and experiment with\\nboth gold and predicted POS tags. For experiments\\nwhere the target language has no treebank, we use\\nthe standard splits for these languages in the older\\nuniversal dependency treebanks v2.0 (McDonald et\\nal., 2013) and use gold POS tags, following the base-\\nlines (Zhang and Barzilay, 2015; Guo et al., 2016).\\nTable 1 gives the number of sentences and words\\nannotated for each language in both versions. In a\\npreprocessing step, we lowercase all tokens and re-\\nmove multi-word annotations and language-speciﬁc\\ndependency relations. We use the same multilingual\\nBrown clusters and multilingual embeddings of Guo\\net al. (2016), kindly provided by the authors.\\nOptimization. We follow Dyer et al. (2015) in\\nparameter initialization and optimization.17How-\\never, when training the parser on multiple languages\\n17We use stochastic gradient updates with an initial learn-\\ning rate ofη0= 0.1in epoch #0, update the learning rate\\nin following epochs as ηt=η0/(1 + 0.1t). We clip the ℓ2\\nnorm of the gradient to avoid “exploding” gradients. Unla-\\nbeled attachment score (UAS) on the development set deter-\\nmines early stopping. Parameters are initialized with uniform\\nsamples in ±√\\n6/(r+c)whererandcare the sizes of the\\nprevious and following layer in the nueral network (Glorot and\\nBengio, 2010). The standard deviations of the labeled attach-\\nment score (LAS) due to random initialization in individual tar-\\nget languages are 0.36 (de), 0.40 (en), 0.37 (es), 0.46 (fr), 0.47\\n(it), 0.41 (pt) and 0.24 (sv). The standard deviation of the aver-\\nage LAS scores across languages is 0.17.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 5}), Document(page_content='LAS target language average\\nde en es fr it pt sv\\nmonolingual 79.3 85.9 83.7 81.7 88.7 85.7 83.5 84.0\\nMALOP A 70.4 69.3 72.4 71.1 78.0 74.1 65.4 71.5\\n+lexical 76.7 82.0 82.7 81.2 87.6 82.1 81.2 81.9\\n+language ID 78.6 84.2 83.4 82.4 89.1 84.2 82.6 83.5\\n+ﬁne-grained POS 78.9 85.4 84.3 82.4 89.0 86.2 84.5 84.3\\nTable 3: Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and\\nMALOP Ain the fully supervised scenario where Lt=Ls. Note that we use the universal dependencies\\nverson 1.2 which only includes annotations for ∼13,000 English sentences, which explains the relatively\\nlow scores in English. When we instead use the universal dependency treebanks version 2.0 which includes\\nannotations for∼40,000 English sentences (originally from the English Penn Treebank), we achieve UAS\\nscore 93.0 and LAS score 91.5.\\nin M ALOP A, instead of updating the parameters\\nwith the gradient of individual sentences, we use\\nmini-batch updates which include one sentence sam-\\npled uniformly (without replacement) from each\\nlanguage’s treebank, until all sentences in the small-\\nest treebank are used (which concludes an epoch).\\nWe repeat the same process in following epochs.\\nWe found this to help prevent one source language\\nwith a larger treebank (e.g., German) from dominat-\\ning parameter updates at the expense of other source\\nlanguages with a smaller treebank (e.g., Swedish).\\n4.1 Target Languages with a Treebank\\n(Lt=Ls)\\nHere, we evaluate our M ALOP Aparser when the\\ntarget language has a treebank.\\nBaseline. For each target language, the strong\\nbaseline we use is a monolingually-trained S-LSTM\\nparser with a token representation which concate-\\nnates: pretrained word embeddings (50 dimen-\\nsions),18learned word embeddings (50 dimensions),\\ncoarse (universal) POS tag embeddings (12 dimen-\\nsions), ﬁne-grained (language-speciﬁc, when avail-\\nable) POS tag embeddings (12 dimensions), and em-\\nbeddings of Brown clusters (12 dimensions), and\\nuses a two-layer S-LSTM for each of the stack, the\\nbuffer and the list of actions. We independently train\\none baseline parser for each target language, and\\nshare no model parameters. This baseline, denoted\\n18These embeddings are treated as ﬁxed inputs to the parser,\\nand are not optimized towards the parsing objective. We use the\\nsame embeddings used in Guo et al. (2016).‘monolingual’ in Tables 3 and 7, achieves UAS score\\n93.0 and LAS score 91.5 when trained on the En-\\nglish Penn Treebank, which is comparable to Dyer\\net al. (2015).\\nMALOP A.We train M ALOP Aon the concante-\\nnation of training sections of all seven languages. To\\nbalance the development set, we only concatenate\\nthe ﬁrst 300 sentences of each language’s develop-\\nment section.\\nToken representations. The ﬁrst M AL-\\nOPAparser we evaluate uses only coarse POS\\nembeddings to construct the token representation.19\\nAs shown in Table 3, this parser consistently\\nunderperforms the monolingual baselines, with a\\ngap of 12.5 LAS points on average.\\nAugmenting the token representation with lexical\\nembeddings to the token representation (both mul-\\ntilingual word clusters and pretrained multilingual\\nword embeddings, as described in §3.3) substan-\\ntially improves the performance of M ALOP A, re-\\ncovering 83% of the gap in average performance.\\nWe experimented with three ways to include\\nlanguage information in the token representation,\\nnamely: ‘language ID’, ‘word order’ and ‘full ty-\\npology’ (see §3.4 for details), and found all three\\nto improve the performance of M ALOP Agiving\\nLAS scores 83.5, 83.2 and 82.5, respectively. It is\\nnoteworthy that the model beneﬁts more from lan-\\n19We use the same number of dimensions for the coarse POS\\nembeddings as in the monolingual baselines. The same applies\\nto all other types of embeddings used in M ALOP A.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 6}), Document(page_content='Recall % left right root short long nsubj* dobj conj *comp case *mod\\nmonolingual 89.9 95.2 86.4 92.9 81.1 77.3 75.5 66.0 45.6 93.3 77.0\\nMALOP A 85.4 93.3 80.2 91.2 73.3 57.3 62.7 64.2 34.0 90.7 69.6\\n+lexical 89.9 93.8 84.5 92.6 78.6 73.3 73.4 66.9 35.3 91.6 75.3\\n+language ID 89.1 94.7 86.6 93.2 81.4 74.7 73.0 71.2 48.2 92.8 76.3\\n+ﬁne-grained POS 89.5 95.7 87.8 93.6 82.0 74.7 74.9 69.7 46.0 93.3 76.3\\nTable 4: Recall of some classes of dependency attachments/relations in German.\\nLAS target language average\\nlanguage ID coarse POS de en es fr it pt sv\\ngold gold 78.6 84.2 83.4 82.4 89.1 84.2 82.6 83.5\\npredicted gold 78.5 80.2 83.4 82.1 88.9 83.9 82.5 82.7\\ngold predicted 71.2 79.9 80.5 78.5 85.0 78.4 75.5 78.4\\npredicted predicted 70.8 74.1 80.5 78.2 84.7 77.1 75.5 77.2\\nTable 5: Effect of automatically predicting language ID and POS tags with M ALOP Aon LAS scores.\\nguage ID than from typological properties. Using\\n‘language ID,’ we recover another 12% of the origi-\\nnal gap.\\nFinally, the best conﬁguration of M ALOP Aadds\\nﬁne-grained POS embeddings to the token represen-\\ntation.20Surprisingly, adding ﬁne-grained POS em-\\nbeddings improves the performance even for some\\nlanguages where ﬁne-grained POS tags are not avail-\\nable (e.g., Spanish). This parser outperforms the\\nmonolingual baseline in ﬁve out of seven target lan-\\nguages, and wins on average by 0.3 LAS points. We\\nemphasize that this model is only trained once on\\nall languages, and the same model is used to parse\\nthe test set of each language, which simpliﬁes the\\ndistribution or deployment of multilingual parsing\\nsoftware.\\nQualitative analysis. To gain a better understand-\\ning of the model behavior, we analyze certain\\nclasses of dependency attachments/relations in Ger-\\nman, which has notably ﬂexible word order, in Ta-\\nble 4. We consider the recall of left attachments\\n(where the head word precedes the dependent word\\nin the sentence), right attachments, root attach-\\nments, short-attachments (with distance = 1), long-\\nattachments (with distance >6), as well as the fol-\\nlowing relation groups: nsubj* (nominal subjects:\\n20Fine-grained POS tags were only available for English,\\nItalian, Portuguese and Swedish. Other languages reuse the\\ncoarse POS tags as ﬁne-grained tags instead of padding the ex-\\ntra dimensions in the token representation with zeros.nsubj ,nsubjpass ), dobj (direct object: dobj ),\\nconj (conjunct: conj ), *comp (clausal comple-\\nments: ccomp ,xcomp ), case (clitics and adposi-\\ntions: case ), *mod (modiﬁers of a noun: nmod ,\\nnummod ,amod ,appos ), neg (negation modiﬁer:\\nneg).21\\nFindings. We found that each of the three im-\\nprovements (lexical embeddings, language embed-\\ndings and ﬁne-grained POS embeddings) tends to\\nimprove recall for most classes. M ALOP Aun-\\nderperforms (compared to the monolingual base-\\nline) in some classes: nominal subjects, direct ob-\\njects and modiﬁers of a noun. Nevertheless, M AL-\\nOPAoutperforms the baseline in some important\\nclasses such as: root, long attachments and conjunc-\\ntions.\\nPredicting language IDs and POS tags. In Ta-\\nble 3, we assume that both gold language ID of the\\ninput language and gold POS tags are given at test\\ntime. However, this assumption is not realistic in\\npractical applications. Here, we quantify the degra-\\ndation in parsing accuracy when language ID and\\nPOS tags are only given at training time, but must\\nbe predicted at test time. We do not use ﬁne-grained\\n21For each group, we report recall of both the attach-\\nment and relation weighted by the number of instances in the\\ngold annotation. A detailed description of each relation can\\nbe found at http://universaldependencies.org/\\nu/dep/index.html', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 7}), Document(page_content='POS tags in these experiments because some lan-\\nguages use a very large ﬁne-grained POS tag set\\n(e.g., 866 unique tags in Portuguese).\\nIn order to predict language ID, we use the\\nlangid.py library (Lui and Baldwin, 2012)22and\\nclassify individual sentences in the test sets to one\\nof the seven languages of interest, using the default\\nmodels included in the library. The macro aver-\\nage language ID prediction accuracy on the test set\\nacross sentences is 94.7%. In order to predict POS\\ntags, we use the model described in §3.6 with both\\ninput and hidden LSTM dimensions of 60, and with\\nblock dropout. The macro average accuracy of the\\nPOS tagger is 93.3%. Table 5 summarizes the four\\nconﬁgurations: {gold language ID, predicted lan-\\nguage ID}×{gold POS tags, predicted POS tags}.\\nThe performance of the parser suffers mildly (–0.8\\nLAS points) when using predicted language IDs,\\nbut more (–5.1 LAS points) when using predicted\\nPOS tags. As an alternative approach to predicting\\nPOS tags, we trained the Stanford POS tagger, for\\neach target language, on the coarse POS tag annota-\\ntions in the training section of the universal depen-\\ndency treebanks,23then replaced the gold POS tags\\nin the test set of each language with predictions of\\nthe monolingual tagger. The resulting degradation\\nin parsing performance between gold vs. predicted\\nPOS tags is –6.0 LAS points (on average, compared\\nto a degradation of –5.1 LAS points in Table 5). The\\ndisparity in parsing results with gold vs. predicted\\nPOS tags is an important open problem, and has\\nbeen previously discussed by Tiedemann (2015).\\nThe predicted POS results in Table 5 use block\\ndropout. Without using block dropout, we lose an\\nextra 0.2 LAS points in both conﬁgurations using\\npredicted POS tags.\\nDifferent multilingual embeddings. Several\\nmethods have been proposed for pretraining mul-\\ntilingual word embeddings. We compare three of\\nthem:\\n•multiCCA (Ammar et al., 2016) uses a lin-\\n22https://github.com/saffsd/langid.py\\n23We used version 3.6.0 of the Stanford POS tag-\\nger, with the following pre-packaged conﬁguration ﬁles:\\ngerman-fast-caseless.tagger.props (de), english-caseless-\\nleft3words-distsim.tagger.props (en), spanish.tagger.props (es),\\nfrench.tagger.props (fr). We reused french.tagger.props for (it,\\npt, sv).multilingual embeddings UAS LAS\\nmultiCluster 87.7 84.1\\nmultiCCA 87.8 84.4\\nrobust projection 87.8 84.2\\nTable 6: Effect of multilingual embedding estima-\\ntion method on the multilingual parsing with M AL-\\nOPA. UAS and LAS scores are macro-averaged\\nacross seven target languages.\\near operator to project pretrained monolingual\\nembeddings in each language (except English)\\nto the vector space of pretrained English word\\nembeddings.\\n•multiCluster (Ammar et al., 2016) uses the\\nsame embedding for translationally-equivalent\\nwords in different languages.\\n•robust projection (Guo et al., 2015) ﬁrst pre-\\ntrains monolingual English word embeddings,\\nthen deﬁnes the embedding of a non-English\\nword as the weighted average embedding of\\nEnglish words aligned to the non-English\\nwords (in a parallel corpus). The embedding of\\na non-English word which is not aligned to any\\nEnglish words is deﬁned as the average embed-\\nding of words with a unit edit distance in the\\nsame language (e.g., ‘playz’ is the average of\\n‘plays’ and ‘play’).24\\nAll embeddings are trained on the same data and use\\nthe same number of dimensions (100).25Table 6 il-\\nlustrates that the three methods perform similarly on\\nthis task. Aside from Table 6, in this paper, we ex-\\nclusively use the robust projection multilingual em-\\nbeddings trained in Guo et al. (2016).26The “ro-\\nbust projection” result in Table 6 (which uses 100\\ndimensions) is comparable to the last row in Table 3\\n(which uses 50 dimensions).\\n24Our implementation of this method can be\\nfound at https://github.com/gmulcaire/\\naverage-embeddings .\\n25We share the embedding ﬁles at https://github.\\ncom/clab/language-universal-parser/tree/\\nmaster/pretrained_embeddings .\\n26The embeddings were kindly provided by the authors\\nof Guo et al. (2016) at https://drive.google.com/\\nfile/d/0B1z04ix6jD_DY3lMN2Ntdy02NFU/view', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 8}), Document(page_content='LAS target language average\\nde es fr it pt sv\\nZhang and Barzilay (2015) 54.1 68.3 68.8 69.4 72.5 62.5 65.9\\nGuo et al. (2016) 55.9 73.0 71.0 71.2 78.6 69.5 69.3\\nMALOP A 57.1 74.6 73.9 72.5 77.0 68.1 70.5\\nTable 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the\\nsimulated low-resource scenario where Lt∩Ls=∅.\\nLAS target language\\nde es fr it sv\\nmonolingual 58.0 64.7 63.0 68.7 57.6\\nDuong et al. 61.8 70.5 67.2 71.3 62.5\\nMALOP A 63.4 70.5 69.1 74.1 63.4\\nTable 7: Small (3,000 token) target treebank setting:\\nlanguage-universal dependency parser performance.\\nSmall target treebank. Duong et al. (2015b) con-\\nsidered a setup where the target language has a small\\ntreebank of∼3,000 tokens, and the source language\\n(English) has a large treebank of ∼205,000 tokens.\\nThe parser proposed in Duong et al. (2015b) is a\\nneural network parser based on Chen and Manning\\n(2014), which shares most of the parameters be-\\ntween English and the target language, and uses\\nanℓ2regularizer to tie the lexical embeddings of\\ntranslationally-equivalent words. While not the pri-\\nmary focus of this paper,27we compare our pro-\\nposed method to that of Duong et al. (2015b) on\\nﬁve target languages for which multilingual Brown\\nclusters are available from Guo et al. (2016). For\\neach target language, we train the parser on the En-\\nglish training data in the UD version 1.0 corpus\\n(Nivre et al., 2015b) and a small treebank in the\\ntarget language.28Following Duong et al. (2015b),\\nin this setup, we only use gold coarse POS tags,\\n27The setup cost involved in recruiting linguists, developing\\nand revising annotation guidelines to annotate a new language\\nought to be higher than the cost of annotating 3,000 tokens. Af-\\nter investing much resources in a language, we believe it is un-\\nrealistic to stop the annotation effort after only 3,000 tokens.\\n28We thank Long Duong for sharing the processed,\\nsubsampled training corpora in each target language at\\nhttps://github.com/longdt219/universal_\\ndependency_parser/tree/master/data/\\nuniversal-dep/universal-dependencies-1.0 .we do not use any development data in the target\\nlanguages (we use the English development set in-\\nstead), and we subsample the English training data\\nin each epoch to the same number of sentences in the\\ntarget language. We use the same hyperparameters\\nspeciﬁed before for the single M ALOP Aparser and\\neach of the monolingual baselines. Table 7 shows\\nthat our method outperforms Duong et al. (2015b)\\nby 1.4 LAS points on average. Our method consis-\\ntently outperforms the monolingual baselines in this\\nsetup, with an average improvement of 5.7 absolute\\nLAS points.\\n4.2 Target Languages without a Treebank\\n(Lt∩Ls=∅)\\nMcDonald et al. (2011) established that, when no\\ntreebank annotations are available in the target lan-\\nguage, training on multiple source languages out-\\nperforms training on one (i.e., multi-source model\\ntransfer outperforms single-source model transfer).\\nIn this section, we evaluate the performance of our\\nparser in this setup. We use two strong baseline\\nmulti-source model transfer parsers with no super-\\nvision in the target language:\\n•Zhang and Barzilay (2015) is a graph-based arc-\\nfactored parsing model with a tensor-based scor-\\ning function. It takes typological properties of\\na language as input. We compare to the best\\nreported conﬁguration (i.e., the column titled\\n“OURS” in Table 5 of Zhang and Barzilay, 2015).\\n•Guo et al. (2016) is a transition-based neural-\\nnetwork parsing model based on Chen and Man-\\nning (2014). It uses a multilingual embeddings\\nand Brown clusters as lexical features. We com-\\npare to the best reported conﬁguration (i.e., the\\ncolumn titled “MULTI-PROJ” in Table 1 of Guo\\net al., 2016).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 9}), Document(page_content='Following Guo et al. (2016), for each target lan-\\nguage, we train the parser on six other languages in\\nthe Google universal dependency treebanks version\\n2.029(de, en, es, fr, it, pt, sv, excluding whichever\\nis the target language), and we use gold coarse POS\\ntags. Our parser uses the same word embeddings\\nand word clusters used in Guo et al. (2016), and does\\nnot use any typology information.30\\nThe results in Table 8 show that, on average, our\\nparser outperforms both baselines by more than 1\\npoint in LAS, and gives the best LAS results in four\\n(out of six) languages.\\n5 Related Work\\nOur work builds on the model transfer approach,\\nwhich was pioneered by Zeman and Resnik (2008)\\nwho trained a parser on a source language treebank\\nthen applied it to parse sentences in a target lan-\\nguage. Cohen et al. (2011) and McDonald et al.\\n(2011) trained unlexicalized parsers on treebanks of\\nmultiple source languages and applied the parser to\\ndifferent languages. Naseem et al. (2012), Täck-\\nström et al. (2013), and Zhang and Barzilay (2015)\\nused language typology to improve model trans-\\nfer. To add lexical information, Täckström et al.\\n(2012) used multilingual word clusters, while Xiao\\nand Guo (2014), Guo et al. (2015), Søgaard et al.\\n(2015) and Guo et al. (2016) used multilingual word\\nembeddings. Duong et al. (2015b) used a neural\\nnetwork based model, sharing most of the parame-\\nters between two languages, and used an ℓ2regular-\\nizer to tie the lexical embeddings of translationally-\\nequivalent words. We incorporate these ideas in\\nour framework, while proposing a novel neural ar-\\nchitecture for embedding language typology (see\\n§3.4), and use a variant of word dropout (Iyyer et\\nal., 2015) for consuming noisy structured inputs.\\nWe also show how to replace an array of mono-\\nlingually trained parsers with one multilingually-\\ntrained parser without sacriﬁcing accuracy, which is\\nrelated to Vilares et al. (2016).\\nNeural network parsing models which preceded\\nDyer et al. (2015) include Henderson (2003), Titov\\nand Henderson (2007), Henderson and Titov (2010)\\n29https://github.com/ryanmcd/uni-dep-tb/\\n30In preliminary experiments, we found language embed-\\ndings to hurt the performance of the parser for target languages\\nwithout a treebank.and Chen and Manning (2014). Related to lexi-\\ncal features in cross-lingual parsing is Durrett et al.\\n(2012) who deﬁned lexico-syntactic features based\\non bilingual lexicons. Other related work include\\nÖstling (2015), which may be used to induce more\\nuseful typological properties to inform multilingual\\nparsing.\\nAnother popular approach for cross-lingual su-\\npervision is to project annotations from the source\\nlanguage to the target language via a parallel cor-\\npus (Yarowsky et al., 2001; Hwa et al., 2005) or\\nvia automatically-translated sentences (Tiedemann\\net al., 2014). Ma and Xia (2014) used entropy regu-\\nlarization to learn from both parallel data (with pro-\\njected annotations) and unlabeled data in the target\\nlanguage. Rasooli and Collins (2015) trained an\\narray of target-language parsers on fully annotated\\ntrees, by iteratively decoding sentences in the tar-\\nget language with incomplete annotations. One re-\\nsearch direction worth pursuing is to ﬁnd synergies\\nbetween the model transfer approach and annotation\\nprojection approach.\\n6 Conclusion\\nWe presented M ALOP A, a single parser trained on\\na multilingual set of treebanks. We showed that\\nthis parser, equipped with language embeddings and\\nﬁne-grained POS embeddings, on average outper-\\nforms monolingually-trained parsers for target lan-\\nguages with a treebank. This pattern of results is\\nquite encouraging. Although languages may share\\nunderlying syntactic properties, individual parsing\\nmodels must behave quite differently, and our model\\nallows this while sharing parameters across lan-\\nguages. The value of this sharing is more pro-\\nnounced in scenarios where the target language’s\\ntraining treebank is small or non-existent, where\\nour parser outperforms previous cross-lingual multi-\\nsource model transfer methods.\\nAcknowledgments\\nWaleed Ammar is supported by the Google fellow-\\nship in natural language processing. Miguel Balles-\\nteros is supported by the European Commission un-\\nder the contract numbers FP7-ICT-610411 (project\\nMULTISENSOR) and H2020-RIA-645012 (project\\nKRISTINA). Part of this material is based upon', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 10}), Document(page_content='work supported by a subcontract with Raytheon\\nBBN Technologies Corp. under DARPA Prime Con-\\ntract No. HR0011-15-C-0013, and part of this re-\\nsearch was supported by a Google research award\\nto Noah Smith. We thank Jiang Guo for sharing\\nthe multilingual word embeddings and multilingual\\nword clusters. We thank Lori Levin, Ryan Mc-\\nDonald, Jörg Tiedemann, Yulia Tsvetkov, and Yuan\\nZhang for helpful discussions. Last but not least,\\nwe thank the anonymous TACL reviewers for their\\nvaluable feedback.\\nReferences\\n[Agi ´c et al.2015] Željko Agi ´c, Maria Jesus Aranzabe,\\nAitziber Atutxa, Cristina Bosco, Jinho Choi, Marie-\\nCatherine de Marneffe, Timothy Dozat, Richárd\\nFarkas, Jennifer Foster, Filip Ginter, Iakes Goe-\\nnaga, Koldo Gojenola, Yoav Goldberg, Jan Ha-\\njiˇc, Anders Trærup Johannsen, Jenna Kanerva, Juha\\nKuokkala, Veronika Laippala, Alessandro Lenci, Kris-\\nter Lindén, Nikola Ljubeši ´c, Teresa Lynn, Christopher\\nManning, Héctor Alonso Martínez, Ryan McDonald,\\nAnna Missilä, Simonetta Montemagni, Joakim Nivre,\\nHanna Nurmi, Petya Osenova, Slav Petrov, Jussi Piit-\\nulainen, Barbara Plank, Prokopis Prokopidis, Sampo\\nPyysalo, Wolfgang Seeker, Mojgan Seraji, Natalia Sil-\\nveira, Maria Simi, Kiril Simov, Aaron Smith, Reut\\nTsarfaty, Veronika Vincze, and Daniel Zeman. 2015.\\nUniversal dependencies 1.1. LINDAT/CLARIN digi-\\ntal library at Institute of Formal and Applied Linguis-\\ntics, Charles University in Prague.\\n[Ammar et al.2016] Waleed Ammar, George Mulcaire,\\nYulia Tsvetkov, Guillaume Lample, Chris Dyer, and\\nNoah A. Smith. 2016. Massively multilingual word\\nembeddings. arXiv:1602.01925v2.\\n[Barman et al.2014] Utsab Barman, Amitava Das,\\nJoachim Wagner, and Jennifer Foster. 2014. Code\\nmixing: A challenge for language identiﬁcation in the\\nlanguage of social media. In EMNLP Workshop on\\nComputational Approaches to Code Switching .\\n[Bender2011] Emily M. Bender. 2011. On achieving and\\nevaluating language-independence in NLP. Linguistic\\nIssues in Language Technology , 6(3):1–26.\\n[Buchholz and Marsi2006] Sabine Buchholz and Erwin\\nMarsi. 2006. CoNLL-X shared task on multilingual\\ndependency parsing. In Proc. of CoNLL .\\n[Chen and Manning2014] Danqi Chen and Christopher\\nManning. 2014. A fast and accurate dependency\\nparser using neural networks. In Proc. of EMNLP .\\n[Cohen et al.2011] Shay B. Cohen, Dipanjan Das, and\\nNoah A. Smith. 2011. Unsupervised structure predic-tion with non-parallel multilingual guidance. In Proc.\\nof EMNLP .\\n[Dryer and Haspelmath2013] Matthew S. Dryer and Mar-\\ntin Haspelmath, editors. 2013. WALS Online .\\nMax Planck Institute for Evolutionary Anthropology,\\nLeipzig.\\n[Duong et al.2015a] Long Duong, Trevor Cohn, Steven\\nBird, and Paul Cook. 2015a. Low resource depen-\\ndency parsing: Cross-lingual parameter sharing in a\\nneural network parser. In Proc. of ACL-IJCNLP .\\n[Duong et al.2015b] Long Duong, Trevor Cohn, Steven\\nBird, and Paul Cook. 2015b. A neural network model\\nfor low-resource universal dependency parsing. In\\nProc. of EMNLP .\\n[Durrett et al.2012] Greg Durrett, Adam Pauls, and Dan\\nKlein. 2012. Syntactic transfer using a bilingual lexi-\\ncon. In Proc. of EMNLP .\\n[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang\\nLing, Austin Matthews, and Noah A. Smith. 2015.\\nTransition-based dependency parsing with stack long\\nshort-term memory. In Proc. of ACL .\\n[Gardner et al.2015] Matt Gardner, Kejun Huang, Evan-\\ngelos Papalexakis, Xiao Fu, Partha Talukdar, Christos\\nFaloutsos, Nicholas Sidiropoulos, and Tom Mitchell.\\n2015. Translation invariant word embeddings. In\\nProc. of EMNLP .\\n[Glorot and Bengio2010] Xavier Glorot and Yoshua Ben-\\ngio. 2010. Understanding the difﬁculty of training\\ndeep feedforward neural networks. In Proc. of AIS-\\nTATS .\\n[Graves et al.2013] Alan Graves, Abdel-rahman Mo-\\nhamed, and Geoffrey Hinton. 2013. Speech recog-\\nnition with deep recurrent neural networks. In Proc.\\nof ICASSP .\\n[Guo et al.2015] Jiang Guo, Wanxiang Che, David\\nYarowsky, Haifeng Wang, and Ting Liu. 2015. Cross-\\nlingual dependency parsing based on distributed rep-\\nresentations. In Proc. of ACL .\\n[Guo et al.2016] Jiang Guo, Wanxiang Che, David\\nYarowsky, Haifeng Wang, and Ting Liu. 2016. A rep-\\nresentation learning framework for multi-source trans-\\nfer parsing. In Proc. of AAAI .\\n[Heid and Raab1989] Ulrich Heid and Sybille Raab.\\n1989. Collocations in multilingual generation. In\\nProc. of EACL .\\n[Henderson and Titov2010] James Henderson and Ivan\\nTitov. 2010. Incremental sigmoid belief networks for\\ngrammar learning. Journal of Machine Learning Re-\\nsearch , 11:3541–3570.\\n[Henderson2003] James Henderson. 2003. Inducing his-\\ntory representations for broad coverage statistical pars-\\ning. In Proc. of NAACL-HLT .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 11}), Document(page_content='[Huang et al.2015] Zhiheng Huang, Wei Xu, and Kai Yu.\\n2015. Bidirectional LSTM-CRF models for sequence\\ntagging. arXiv:1508.01991.\\n[Hwa et al.2005] Rebecca Hwa, Philip Resnik, Amy\\nWeinberg, Clara Cabezas, and Okan Kolak. 2005.\\nBootstrapping parsers via syntactic projection across\\nparallel texts. Natural Language Engineering ,\\n11(03):311–325.\\n[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-\\ndan L. Boyd-Graber, and Hal Daumé. 2015. Deep un-\\nordered composition rivals syntactic methods for text\\nclassiﬁcation. In Proc. of ACL .\\n[Lui and Baldwin2012] Marco Lui and Timothy Baldwin.\\n2012. langid.py: An off-the-shelf language identiﬁca-\\ntion tool. In Proc. of ACL .\\n[Ma and Xia2014] Xuezhe Ma and Fei Xia. 2014. Un-\\nsupervised dependency parsing with transferring dis-\\ntribution via parallel guidance and entropy regulariza-\\ntion. In Proc. of ACL .\\n[McDonald et al.2011] Ryan McDonald, Slav Petrov, and\\nKeith Hall. 2011. Multi-source transfer of delexical-\\nized dependency parsers. In Proc. of EMNLP .\\n[McDonald et al.2013] Ryan McDonald, Joakim Nivre,\\nYvonne Quirmbach-Brundage, Yoav Goldberg, Di-\\npanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov,\\nHao Zhang, Oscar Täckström, Claudia Bedini, Núria\\nBertomeu Castelló, and Jungmee Lee. 2013. Univer-\\nsal dependency annotation for multilingual parsing. In\\nProc. of ACL .\\n[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013. Efﬁcient estima-\\ntion of word representations in vector space. In Proc.\\nof ICLR .\\n[Naseem et al.2012] Tahira Naseem, Regina Barzilay, and\\nAmir Globerson. 2012. Selective sharing for multilin-\\ngual dependency parsing. In Proc. of ACL .\\n[Nivre and Nilsson2005] Joakim Nivre and Jens Nilsson.\\n2005. Pseudo-projective dependency parsing. In\\nProc. of ACL .\\n[Nivre et al.2007] Joakim Nivre, Johan Hall, Sandra\\nKubler, Ryan McDonald, Jens Nilsson, Sebastian\\nRiedel, and Deniz Yuret. 2007. The CoNLL 2007\\nshared task on dependency parsing. In Proc. of\\nCoNLL .\\n[Nivre et al.2015a] Joakim Nivre, Željko Agi ´c, Maria Je-\\nsus Aranzabe, Masayuki Asahara, Aitziber Atutxa,\\nMiguel Ballesteros, John Bauer, Kepa Bengoetxea,\\nRiyaz Ahmad Bhat, Cristina Bosco, Sam Bowman,\\nGiuseppe G. A. Celano, Miriam Connor, Marie-\\nCatherine de Marneffe, Arantza Diaz de Ilarraza, Kaja\\nDobrovoljc, Timothy Dozat, Tomaž Erjavec, Richárd\\nFarkas, Jennifer Foster, Daniel Galbraith, Filip Gin-\\nter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-berg, Berta Gonzales, Bruno Guillaume, Jan Ha-\\njiˇc, Dag Haug, Radu Ion, Elena Irimia, Anders Jo-\\nhannsen, Hiroshi Kanayama, Jenna Kanerva, Simon\\nKrek, Veronika Laippala, Alessandro Lenci, Nikola\\nLjubeši ´c, Teresa Lynn, Christopher Manning, C ˘at˘alina\\nM˘ar˘anduc, David Mare ˇcek, Héctor Martínez Alonso,\\nJan Mašek, Yuji Matsumoto, Ryan McDonald, Anna\\nMissilä, Verginica Mititelu, Yusuke Miyao, Simon-\\netta Montemagni, Shunsuke Mori, Hanna Nurmi,\\nPetya Osenova, Lilja Øvrelid, Elena Pascual, Marco\\nPassarotti, Cenel-Augusto Perez, Slav Petrov, Jussi\\nPiitulainen, Barbara Plank, Martin Popel, Prokopis\\nProkopidis, Sampo Pyysalo, Loganathan Ramasamy,\\nRudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolf-\\ngang Seeker, Mojgan Seraji, Natalia Silveira, Maria\\nSimi, Radu Simionescu, Katalin Simkó, Kiril Simov,\\nAaron Smith, Jan Št ˇepánek, Alane Suhr, Zsolt Szántó,\\nTakaaki Tanaka, Reut Tsarfaty, Sumire Uematsu, Lar-\\nraitz Uria, Viktor Varga, Veronika Vincze, Zden ˇek\\nŽabokrtský, Daniel Zeman, and Hanzhi Zhu. 2015a.\\nUniversal dependencies 1.2. LINDAT/CLARIN digi-\\ntal library at Institute of Formal and Applied Linguis-\\ntics, Charles University in Prague.\\n[Nivre et al.2015b] Joakim Nivre, Cristina Bosco, Jinho\\nChoi, Marie-Catherine de Marneffe, Timothy Dozat,\\nRichárd Farkas, Jennifer Foster, Filip Ginter, Yoav\\nGoldberg, Jan Haji ˇc, Jenna Kanerva, Veronika Laip-\\npala, Alessandro Lenci, Teresa Lynn, Christopher\\nManning, Ryan McDonald, Anna Missilä, Simon-\\netta Montemagni, Slav Petrov, Sampo Pyysalo, Na-\\ntalia Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty,\\nVeronika Vincze, and Daniel Zeman. 2015b. Uni-\\nversal dependencies 1.0. LINDAT/CLARIN digital li-\\nbrary at Institute of Formal and Applied Linguistics,\\nCharles University in Prague.\\n[Nivre2004] Joakim Nivre. 2004. Incrementality in de-\\nterministic dependency parsing. In Proceedings of\\nthe Workshop on Incremental Parsing: Bringing En-\\ngineering and Cognition Together .\\n[Östling2015] Robert Östling. 2015. Word order typol-\\nogy through multilingual word alignment. In Proc. of\\nACL-IJCNLP .\\n[Petrov et al.2012] Slav Petrov, Dipanjan Das, and Ryan\\nMcDonald. 2012. A universal part-of-speech tagset.\\nInProc. of LREC .\\n[Rasooli and Collins2015] Mohammad Sadegh Rasooli\\nand Michael Collins. 2015. Density-driven cross-\\nlingual transfer of dependency parsers. In Proc. of\\nEMNLP .\\n[Rösner1988] Deitmar Rösner. 1988. The genera-\\ntion system of the semsyn project: Towards a task-\\nindependent generator for german. Advances in Natu-\\nral Language Generation , 2.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 12}), Document(page_content='[Søgaard et al.2015] Anders Søgaard, Željko Agi ´c, Héc-\\ntor Martínez Alonso, Barbara Plank, Bernd Bohnet,\\nand Anders Johannsen. 2015. Inverted indexing for\\ncross-lingual NLP. In Proc. of ACL-IJCNLP 2015 .\\n[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-\\nton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\\nSalakhutdinov. 2014. Dropout: A simple way to pre-\\nvent neural networks from overﬁtting. Journal of Ma-\\nchine Learning Research , 15(1):1929–1958.\\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\\nQuoc V . Le. 2014. Sequence to sequence learning\\nwith neural networks. In NIPS .\\n[Täckström et al.2012] Oscar Täckström, Ryan McDon-\\nald, and Jakob Uszkoreit. 2012. Cross-lingual word\\nclusters for direct transfer of linguistic structure. In\\nProc. of NAACL-HLT .\\n[Täckström et al.2013] Oscar Täckström, Dipanjan Das,\\nSlav Petrov, Ryan McDonald, and Joakim Nivre.\\n2013. Token and type constraints for cross-lingual\\npart-of-speech tagging. Transactions of the Associa-\\ntion for Computational Linguistics , 1:1–12.\\n[Tiedemann et al.2014] Jörg Tiedemann, Zeljko Agic, and\\nJoakim Nivre. 2014. Treebank translation for cross-\\nlingual parser induction. In Proc. of CoNLL .\\n[Tiedemann2015] Jörg Tiedemann. 2015. Cross-lingual\\ndependency parsing with universal dependencies and\\npredicted POS labels. In Proc. of DepLing .\\n[Titov and Henderson2007] Ivan Titov and James Hen-\\nderson. 2007. Constituent parsing with incremental\\nsigmoid belief networks. In Proc. of ACL .\\n[Vilares et al.2016] David Vilares, Carlos Gómez-\\nRodríguez, and Miguel A. Alonso. 2016. One\\nmodel, two languages: training bilingual parsers with\\nharmonized treebanks. arXiv:1507.08449v2.\\n[Wikipedia2016] Wikipedia. 2016. List of languages\\nby number of native speakers. http://bit.ly/\\n1LUP5kJ . Accessed: 2016-01-26.\\n[Xiao and Guo2014] Min Xiao and Yuhong Guo. 2014.\\nDistributed word representation learning for cross-\\nlingual dependency parsing. In Proc. of CoNLL .\\n[Yarowsky et al.2001] David Yarowsky, Grace Ngai, and\\nRichard Wicentowski. 2001. Inducing multilingual\\ntext analysis tools via robust projection across aligned\\ncorpora. In Proc. of HLT .\\n[Zeman and Resnik2008] Daniel Zeman and Philip\\nResnik. 2008. Cross-language parser adaptation\\nbetween related languages. In Proc. of IJCNLP .\\n[Zhang and Barzilay2015] Yuan Zhang and Regina Barzi-\\nlay. 2015. Hierarchical low-rank tensors for multilin-\\ngual transfer parsing. In Proc. of EMNLP .', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.01595.pdf', 'page': 13}), Document(page_content='Chapter 9\\nThe Grail theorem prover:\\nType theory for syntax and semantics\\nRichard Moot\\nAbstract Type-logical grammars use a foundation of logic and type theory to model\\nnatural language. These grammars have been particularly successful giving an ac-\\ncount of several well-known phenomena on the syntax-semantics interface, such as\\nquantiﬁer scope and its interaction with other phenomena. This chapter gives a high-\\nlevel description of a family of theorem provers designed for grammar development\\nin a variety of modern type-logical grammars. We discuss automated theorem prov-\\ning for type-logical grammars from the perspective of proof nets, a graph-theoretic\\nway to represent (partial) proofs during proof search.\\n9.1 Introduction\\nThis chapter describes a series of tools for developing and testing type-logical gram-\\nmars. The Grail family of theorem provers have been designed to work with a va-\\nriety of modern type-logical frameworks, including multimodal type-logical gram-\\nmars (Moortgat, 2011), NL cl(Barker and Shan, 2014), the Displacement calculus\\n(Morrill et al, 2011) and hybrid type-logical grammars (Kubota and Levine, 2012).\\nThe tools give a transparent way of implementing grammars and testing their\\nconsequences, providing a natural deduction proof in the speciﬁc type-logical gram-\\nmar for each of the readings of a sentence. None of this replaces careful reﬂection\\nby the grammar writer, of course, but in many cases, computational testing of hand-\\nwritten grammars will reveal surprises, showing unintended consequences of our\\ngrammar and such unintended proofs (or unintended absences of proofs) help us\\nimprove the grammar. Computational tools also help us speed up grammar devel-\\nRichard Moot\\nCNRS, LaBRI, Bordeaux University, 351 cours de la Lib ´eration, 33405 Talence, France\\nLIRMM, Montpellier University, 161 rue Ada, 34095 Montpellier cedex 5, France e-mail:\\nRichard.Moot@labri.fr\\n1arXiv:1602.00812v2  [cs.CL]  26 Aug 2016', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 0}), Document(page_content='2 Richard Moot\\nopment, for example by allowing us to compare several alternative solutions to a\\nproblem and investigate where they make different predictions.\\nThis chapter describes the underlying formalism of the theorem provers, as it is\\nvisible during an interactive proof trace, and present the general strategy followed\\nby the theorem provers. The presentation in this chapter is somewhat informal, re-\\nferring the reader elsewhere for full proofs.\\nThe rest of this chapter is structured as follows. Section 9.2 presents a general\\nintroduction to type-logical grammars and illustrates its basic concepts using the\\nLambek calculus, ending the section with some problems at the syntax-semantics\\ninterface for the Lambek calculus. Section 9.3 looks at recent developments in\\ntype-logical grammars and how they solve some of the problems at the syntax-\\nsemantics interface. Section 9.4 looks at two general frameworks for automated\\ntheorem proving for type-logical grammars, describing the internal representation\\nof partial proofs and giving a high-level overview of the proof search mechanism.\\n9.2 Type-logical grammars\\nType-logical grammars are a family of grammar formalisms built on a foundation\\nof logic and type theory. Type-logical grammars originated when Lambek (1958)\\nintroduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).\\nThough Lambek built on the work of Ajdukiewicz (1935), Bar-Hillel (1953) and\\nothers, Lambek’s main innovation was to cast the calculus as a logic, giving a se-\\nquent calculus and showing decidability by means of cut elimination. This combi-\\nnation of linguistic and computational applications has proved very inﬂuential.\\nIn its general form, a type-logical grammar consists of following components:\\n1. a logic , which fulﬁls the role of “Universal Grammar” mainstream linguistics1,\\n2. a homomorphism from this logic to intuitionistic (linear) logic, this mapping is\\nthe syntax-semantics interface, with intuitionistic linear logic — also called the\\nLambek-van Benthem calculus, LP (van Benthem, 1995) — fulﬁlling the role of\\n“deep structure”.\\n3. a lexicon , which is a mapping from words of natural language to sets of formulas\\nin our logic,\\n4. a set of goal formulas , which speciﬁes the formulas corresponding to different\\ntypes of sentences in our grammar.2\\n1This is rather different from Montague’s use of the term “Universal Grammar” (Montague, 1970).\\nIn Montague’s sense, the different components of a type-logical grammar together would be an\\ninstantiation of Universal Grammar.\\n2Many authors use a single designated goal formula, typically s, as is standard in formal language\\ntheory. I prefer this slightly more general setup, since it allows us to distinguish between, for\\nexample, declarative sentences, imperatives, yes/no questions, whquestions, etc., both syntactically\\nand semantically.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 1}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 3\\nA sentence w1,..., wnis grammatical iff the statement A1,..., An⊢Cis provable\\nin our logic, for some Ai∈lex(wi)and for some goal formula C. In other words, we\\nuse the lexicon to map words to formulas and then ask the logic whether the result-\\ning sequence of formulas is a theorem. Parsing in a type-logical grammar is quite\\nliterally a form of theorem proving, a very pure realisation of the slogan “parsing as\\ndeduction”.\\nOne of the attractive aspects of type-logical grammars is their simple and trans-\\nparent syntax-semantics interface. Though there is a variety of logics used for the\\nsyntax of type-logical grammars (I will discuss the Lambek calculus in Section 9.2.1\\nand two generalisations of it in Sections 9.3.1 and 9.3.2), there is a large consensus\\nover the syntax-semantics interface. Figure 9.1 gives a picture of the standard archi-\\ntecture of type-logical grammars.\\nSyntax Semantics\\nPragmatics\\ninput textcategorial\\ngrammar proofmultiplica-\\ntive linear\\nlogic prooflinear\\nlambda term\\nlogical\\nsemantics\\n(formulas)\\nsemantics and\\npragmaticshomomorphismisomorphism\\nlexical sub-\\nstitution,\\nnormalization\\nlexical substi-\\ntution, parsingtheorem\\nproving\\nFig. 9.1 The standard architecture of type-logical grammars\\nThe “bridge” between syntax and semantics in the ﬁgure is the Curry-Howard\\nisomorphism between linear lambda terms and proofs in multiplicative intuitionistic\\nlinear logic.\\nTheorem proving occurs in two places of the picture: ﬁrst when parsing a sen-\\ntence in a given type-logical grammar and also at the end when we use the resulting\\nsemantics for inferences. I will have little to say about this second type of theorem\\nproving (Chatzikyriakidis, 2015; Mineshima et al, 2015, provide some investiga-\\ntions into this question, in a way which seems compatible with the syntax-semantics\\ninterface pursued here, though developing a full integrated system combining these\\nsystems with the current work would be an interesting research project); theorem\\nproving for parsing will be discussed in Section 9.4.\\nThe lexicon plays the role of translating words to syntactic formulas but also\\nspeciﬁes the semantic term which is used to compute the semantics later. The lexi-\\ncon of a categorial grammar is “semantically informed”. The desired semantics of a', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 2}), Document(page_content='4 Richard Moot\\nsentence allows us to reverse-engineer the formula and lexical lambda-term which\\nproduce it.\\nMany current semantic theories do not provide a semantic formula directly, but\\nﬁrst provide a proto-semantics on which further computations are performed to pro-\\nduce the ﬁnal semantics (eg. for anaphora resolution, presuppositions projection\\netc.). In the current context this means at least some inference is necessary to deter-\\nmine semantic and pragmatic wellformedness.\\n9.2.1 The Lambek calculus\\nTo make things more concrete, I will start by presenting the Lambek calculus (Lam-\\nbek, 1958). Lambek introduced his calculus as a way to “obtain an effective rule (or\\nalgorithm) for distinguishing sentences from nonsentences”, which would be appli-\\ncable both to formal and to (at least fragments of) natural languages (Lambek, 1958,\\np. 154). The simplest formulas used in the Lambek calculus are atomic formulas,\\nwhich normally include sfor sentence, nfor common noun, npfor noun phrase. We\\nthen inductively deﬁne the set of formulas of the Lambek calculus by saying that,\\nthey include the atomic formulas, and that, if AandBare formulas (atomic or not),\\nthen A/B,A•BandB\\\\Aare also formulas.\\nThe intended meaning of a formula A/B— called Aover B— is that it is looking\\nfor an expression of syntactic type Bto its right to produce an expression of syntactic\\ntype A. An example would be a word like “the” which is assigned the formula np/n\\nin the lexicon, indicating that it is looking for a common noun (like “student”) to its\\nright to form a noun phrase, meaning “the student” would be assigned syntactic type\\nnp. Similarly, the intended meaning of a formula B\\\\A— called Bunder A— is that\\nit is looking for an expression of syntactic type Bto its left to produce an expression\\nof type A. This means an intransitive verb like “slept”, when assigned the formula\\nnp\\\\sin the lexicon, combines with a noun phrase to its left to form a sentence s. We\\ntherefore predict that “the student slept” is a sentence, given the earlier assignment\\nofnpto “the student”. Finally, a formula A•Bdenotes the concatenation of an\\nexpression of type Ato an expression of type B.\\n∆⊢A•BΓ,A,B,Γ′⊢C\\nΓ,∆,Γ′⊢C[•E]Γ⊢A∆⊢B\\nΓ,∆⊢A•B[•I]\\nΓ⊢A/B∆⊢B\\nΓ,∆⊢A[/E]Γ,B⊢A\\nΓ⊢A/B[/I]\\nΓ⊢B∆⊢B\\\\A\\nΓ,∆⊢A[\\\\E]B,Γ⊢A\\nΓ⊢B\\\\A[\\\\I]\\nTable 9.1 Natural deduction for L', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 3}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 5\\nBasic statements of the Lambek calculus are of the form A1,..., An⊢C(with\\nn≥1), indicating a claim that the sequence of formulas A1,..., Anis of type C; the\\nsequent comma ‘,’ is implicitly associative and non-commutative. Table 9.1 shows\\nthe natural deduction rules for the Lambek calculus. Γ,∆, etc. denote non-empty\\nsequences of formulas.\\nA simple Lambek calculus lexicon is shown in Table 9.2. I have adopted the stan-\\ndard convention in type-logical grammars of not using set notation for the lexicon,\\nbut instead listing multiple lexical entries for a word separately. This corresponds to\\ntreating lexas a non-deterministic function rather than as a set-valued function.\\nlex(Alyssa ) =np lex (ran) =np\\\\s\\nlex(Emory ) =np lex (slept) =np\\\\s\\nlex(logic) =np lex (loves) = ( np\\\\s)/np\\nlex(the) =np/n lex (aced) = ( np\\\\s)/np\\nlex(difﬁcult ) =n/n lex (passionately ) = ( np\\\\s)\\\\(np\\\\s)\\nlex(erratic ) =n/n lex (during ) = (( np\\\\s)\\\\(np\\\\s))/np\\nlex(student ) =n lex (everyone ) =s/(np\\\\s)\\nlex(exam) =n lex (someone ) = ( s/np)\\\\s\\nlex(who) = ( n\\\\n)/(np\\\\s) lex(every) = ( s/(np\\\\s))/n\\nlex(whom ) = ( n\\\\n)/(s/np) lex(some) = (( s/np)\\\\s)/n\\nTable 9.2 Lambek calculus lexicon\\nProper names, such as “Alyssa” and “Emory” are assigned the category np. Com-\\nmon nouns, such as “student” and “exam” are assigned the category n. Adjectives,\\nsuch as “difﬁcult” or “erratic” are not assigned a basic syntactic category but rather\\nthe category n/n, indicating they are looking for a common noun to their right to\\nform a new common noun, so we predict that both “difﬁcult exam” and “exam” can\\nbe assigned category n. For more complex entries, “someone” is looking to its right\\nfor a verb phrase to produce a sentence, where np\\\\sis the Lambek calculus equiva-\\nlent of verb phrase, whereas “whom” is ﬁrst looking to its right for a sentence which\\nis itself missing a noun phrase to its right and then to its left for a noun.\\nGiven the lexicon of Table 9.2, we can already derive some fairly complex sen-\\ntences, such as the following, and, as we will see in the next section, obtain the\\ncorrect semantics.\\n(1) Every student aced some exam.\\n(2) The student who slept during the exam loves Alyssa.\\nOne of the two derivations of Sentence (1) is shown in Figure 9.2. To improve\\nreadability, the ﬁgure uses a “sugared” notation: instead of writing the lexical hy-\\npothesis corresponding to “exam” as n⊢n, we have written it as exam⊢n. The', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 4}), Document(page_content='6 Richard Moot\\nwithdrawn np’s corresponding to the object and the subject are given a labels p0\\nandq0respectively; the introduction rules are coindexed with the withdrawn hy-\\npotheses, even though this information can be inferred from the rule instantiation.\\nWe can always uniquely reconstruct the antecedent from the labels. For example,\\nthe sugared statement “ p0aced q0⊢s” in the proof corresponds to np,(np\\\\s)/np,np⊢\\ns.\\nevery⊢(s/(np\\\\s))/nstudent⊢n\\nevery student⊢s/(np\\\\s)/E[p0⊢np]2aced⊢(np\\\\s)/np[q0⊢np]1\\naced q0⊢np\\\\s/E\\np0aced q0⊢s\\\\E\\np0aced⊢s/np/I1some⊢((s/np)\\\\s)/nexam⊢n\\nsome exam⊢(s/np)\\\\s/E\\np0aced some exam⊢s\\\\E\\naced some exam⊢np\\\\s\\\\I2\\nevery student aced some exam ⊢s/E\\nFig. 9.2 “Every student aced some exam” with the subject wide scope reading.\\nAlthough it is easy to verify that the proof of Figure 9.2 has correctly applied\\nthe rules of the Lambek calculus, ﬁnding such a proof from scratch may look a bit\\ncomplicated (the key steps at the beginning of the proof involve introducing two\\nnphypotheses and then deriving s/npto allow the object quantiﬁer to take narrow\\nscope). We will defer the question “given a statement Γ⊢C, how do we decide\\nwhether or not it is derivable?” to Section 9.4 but will ﬁrst discuss how this proof\\ncorresponds to the following logical formula.\\n∀x.[student (x)⇒∃y.[exam(y)∧ace(x,y)]]\\n9.2.2 The syntax-semantics interface\\nFor the Lambek calculus, specifying the homomorphism to multiplicative intuition-\\nistic linear logic is easy: we replace the two implications ‘ \\\\’ and ‘ /’ by the linear\\nimplication ‘⊸’ and the product ‘ •’ by the tensor ‘⊗’. In a statement Γ⊢C,Γis\\nnow a multiset of formulas instead of a sequence. In other words, the sequent comma\\n‘,’ is now associative, commutative instead of associative, non-commutative. For the\\nproof of Figure 9.2 of the previous section, this mapping gives the proof shown in\\nFigure 9.3.\\nWe have kept the order of the premisses of the rules as they were in Figure 9.2\\nto allow for an easier comparison. This deep structure still uses the same atomic\\nformulas as the Lambek calculus, it just forgets about the order of the formulas and\\ntherefore can no longer distinguish between the leftward looking implication ‘ \\\\’ and\\nthe rightward looking implication ‘ /’.\\nTo obtain a semantics in the tradition of Montague (1974), we use the following\\nmapping from syntactic types to semantic types, using Montague’s atomic types e\\n(forentity ) and t(fortruth value ).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 5}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 7\\nn⊸((np⊸s)⊸s)n\\n(np⊸s)⊸s⊸E[np]2np⊸(np⊸s) [np]1\\nnp⊸s⊸E\\ns⊸E\\nnp⊸s⊸I1n⊸((np⊸s)⊸s)n\\n(np⊸s)⊸s⊸E\\ns⊸E\\nnp⊸s⊸I2\\ns⊸E\\nFig. 9.3 Deep structure of the derivation of Figure 9.2.\\nnp∗=e\\nn∗=e→t\\ns∗=t\\n(A⊸B)∗=A∗→B∗\\nApplying this mapping to the deep structure proof of Figure 9.3 produces the in-\\ntuitionistic proof and the corresponding (linear) lambda term as shown in Figure 9.4\\nz(e→t)→(e→t)→t\\n0ze→t\\n1\\n(z0z1)(e→t)→t→E[xe]2ze→(e→t)\\n2[ye]1\\n(z2y)e→t→E\\n((z2y)x)t→E\\nλy.((z2y)x)e→t→I1z(e→t)→(e→t)→t\\n3ze→t\\n4\\n(z3z4)(e→t)→t→E\\n((z3z4)λy.((z2y)x))t→E\\nλx.((z3z4)λy.((z2y)x))e→t→I2\\n((z0z1)(λx.((z3z4)λy.((z2y)x))))t→E\\nFig. 9.4 Intuitionistic proof and lambda term corresponding to the deep structure of Figure 9.3.\\nThe computed term corresponds to the derivational semantics of the proof. To ob-\\ntain the complete meaning, we need to substitute, for each of z0,..., z4, the meaning\\nassigned in the lexicon.\\nFor example, “every” has syntactic type (s/(np\\\\s))/nand its semantic type is\\n(e→t)→(e→t)→t. The corresponding lexical lambda term of this type is\\nλPe→t.λQe→t.(∀(λxe.((⇒(Px))(Qx)))), with ‘∀’ a constant of type (e→t)→t\\nand ‘⇒’ a constant of type t→(t→t). In the more familiar Montague formulation,\\nthis lexical term corresponds to λPe→t.λQe→t.∀x.[(Px)⇒(Qx)], where we can see\\nthe formula in higher-order logic we are constructing more clearly. Although the\\nderivational semantics is a linear lambda term, the lexical term assigned to “every”\\nis not, since the variable xhas two bound occurrences.\\nThe formula assigned to “some” has the same semantic type but a different term\\nλPe→t.λQe→t.(∃(λxe.((∧(Px))(Qx)))).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 6}), Document(page_content='8 Richard Moot\\nThe other words are simple, “exam” is assigned exame→t, “student” is assigned\\nstudente→t, and “aced” is assigned acee→(e→t).\\nSo to compute the meaning, we start with the derivational semantics, repeated\\nbelow.\\n((z0z1)(λx.((z3z4)λy.((z2y)x))))\\nThen we substitute the lexical meanings, for z0,..., z4.\\nz0:=λPe→t.λQe→t.(∀(λxe.((⇒(Px))(Qx))))\\nz1:=studente→t\\nz2:=acee→(e→t)\\nz3:=λPe→t.λQe→t.(∃(λxe.((∧(Px))(Qx))))\\nz4:=exame→t\\nThis produces the following lambda term.\\n((λPe→t.λQe→t.(∀(λxe.((⇒(Px))(Qx))))studente→t)\\n(λx.((λPe→t.λQe→t.(∃(λxe.((∧(Px))(Qx))))exame→t)\\nλy.((acee→(e→t)y)x))))\\nFinally, when we normalise this lambda term, we obtain the following semantics\\nfor this sentence.\\n(∀(λxe.((⇒(studente→t)x))(∃(λye.((∧(exame→ty))((( acee→(e→t)y)x)))))\\nThis lambda term represents the more readable higher-order logic formula3.\\n∀x.[student (x)⇒∃y.[exam(y)∧ace(x,y)]]\\nProofs in the Lambek calculus, and in type-logical grammars are subsets of the\\nproofs in intuitionistic (linear) logic and these proofs are compatible with formal\\nsemantics in the tradition initiated by Montague (1974).\\nFor the example in this section, we have calculated the semantics of a simple\\nexample in “slow motion”: many authors assign a lambda term directly to a proof\\nin their type-logical grammar, leaving the translation to intuitionistic linear logic\\nimplicit.\\n3We have used the standard convention in Montague grammar of writing (px)asp(x)and((py)x)\\nasp(x,y), for a predicate symbol p.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 7}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 9\\nGiven a semantic analysis without a corresponding syntactic proof, we can try\\nto reverse engineer the syntactic proof. For example, suppose we want to assign the\\nreﬂexive “himself” the lambda term λR(e→e→t)λxe.((Rx)x), that is, a term of type\\n(e→e→t)→e→t. Then, using some syntactic reasoning to eliminate implausible\\ncandidates like (np⊸n)⊸n, the only reasonable deep structure formula is (np⊸\\nnp⊸s)⊸(np⊸s)and, reasoning a bit further about which of the implications is\\nleft and right, we quickly end up with the quite reasonable (though far from perfect)\\nLambek calculus formula ((np\\\\s)/np)\\\\(np\\\\s).\\n9.2.3 Going further\\nThough the Lambek calculus is a beautiful and simple logic and though it gives a\\nreasonable account of many interesting phenomena on the syntax-semantics inter-\\nface, the Lambek calculus has a number of problems, which I will discuss brieﬂy\\nbelow. The driving force of research in type-logical grammars since the eighties has\\nbeen to ﬁnd solutions to these problems and some of these solutions will be the main\\ntheme of the next section.\\nFormal language theory\\nThe Lambek calculus generates only context-free languages (Pentus, 1997). There is\\na rather large consensus that natural languages are best described by a class of lan-\\nguages at least slightly larger than the context-free languages. Classical examples\\nof phenomena better analysed using so-called mildly context-sensitive language in-\\nclude verb clusters in Dutch and in Swiss German (Huijbregts, 1984; Shieber, 1985).\\nThe Syntax-Semantics Interface\\nThough our example grammar correctly predicted two readings for Sentence (1)\\nabove, our treatment of quantiﬁers doesn’t scale well. For example, if we want to\\npredict two readings for the following sentence (which is just Sentence (1) where\\n“some” and “every” have exchanged position)\\n(3) Some student aced every exam.\\nthen we need to add an additional lexical entry both for “some” and for “every”; this\\nis easily done, but we end up with two lexical formulas for both words. However, this\\nwould still not be enough. For example, the following sentence is also grammatical.\\n(4) Alyssa gave every student a difﬁcult exam.\\n(5) Alyssa believes a student committed perjury.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 8}), Document(page_content='10 Richard Moot\\nIn Sentence (4), “every student” does not occur in a peripheral position, and though\\nit is possible to add a more complex formula with the correct behaviour, we would\\nneed yet another formula for Sentence (5). Sentence (5) is generally considered to\\nhave two readings: a de dicto reading, where Alyssa doesn’t have a speciﬁc student\\nin mind (she could conclude this, for example, when two students make contradic-\\ntory statements under oath, this reading can be felicitously followed by “but she\\ndoesn’t know which”), and a de re reading where Alyssa believes a speciﬁc student\\nperjured. The Lambek calculus cannot generate this second reading without adding\\nyet another formula for “a”.\\nIt seems we are on the wrong track when we need to add a new lexical entry for\\neach different context in which a quantiﬁer phrase occurs. Ideally, we would like\\nasingle formula for “every”, “some” and “a” which applied in all these different\\ncases.\\nAnother way to see this is that we want to keep the deep structure formula n⊸\\n((np⊸s)⊸s)and that we need to replace the Lambek calculus by another logic\\nsuch that the correct deep structures for the desired readings of sentences like (4)\\nand (5) are produced.\\nLexical Semantics\\nThe grammar above also overgenerates in several ways.\\n1. “ace” implies a (very positive) form of evaluation with respect to the object.\\n“aced the exam” is good, whereas “aced Emory”, outside of the context of a\\ntennis match is bad. “aced logic” can only mean something like “aced the exam\\nfor the logic course”.\\n2. “during” and similar temporal adverbs imply its argument is a temporal interval:\\n“during the exam” is good, but “during the student” is bad, and “during logic”\\ncan only mean something like “during the contextually understood logic lecture”\\nIn the literature on semantics, there has been an inﬂuential movement towards\\na richer ontology of types (compared to the “ﬂat” Montagovian picture presented\\nabove) but also towards a richer set of operations for combining terms of speciﬁc\\ntypes, notably allowing type coercions (Pustejovsky, 1995; Asher, 2011). So an\\n“exam” can be “difﬁcult” (it subject matter, or informational content) but also “take\\na long time” (the event of taking the exam). The theory of semantics outlined in the\\nprevious section needs to be extended if we want to take these and other observa-\\ntions into account.\\n9.3 Modern type-logical grammars\\nWe ended the last section with some problems with using the Lambek calculus as a\\ntheory of the syntax-semantics interface. The problems are of two different kinds.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 9}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 11\\n1. The problems of the syntax-semantic interface, and, in a sense, also those of for-\\nmal language theory are problems where the deep structure is correct but our\\nsyntactic calculus cannot produce an analysis mapping to the desired deep struc-\\nture. We will present two solutions to these problems in Sections 9.3.1 and 9.3.2.\\n2. The problems of lexical semantics, on the other hand require a more sophisticated\\ntype system than Montague’s simply typed type-logical with basic types eandt\\nand mechanisms like coercions which allow us to conditionally “repair” certain\\ntype mismatches in this system. We will discuss a solution to this problem in\\nSection 9.3.3.\\n9.3.1 Multimodal grammars\\nMultimodal type-logical grammars (Moortgat, 2011) take the non-associative Lam-\\nbek calculus as its base, but allow multiple families of connectives.\\nFor the basic statements Γ⊢Cof the Lambek calculus, we ask the question\\nwhether we can derive formula C, the succedent, from a sequence of formulas Γ,\\nthe antecedent. In the multimodal Lambek calculus, the basic objects are labeled\\nbinary trees4. The labels come from a separate set of indices or modes I. Multimodal\\nformulas are then of the form A/iB,A•iBandA\\\\iB, and antecedent terms are of the\\nformΓ◦i∆, with ian index from I(we have omitted the outer brackets for the rules,\\nbut the operator◦iis non-associative). Sequents are still written as Γ⊢C, butΓis\\nnow a binary branching, labeled tree with formulas as its leaves.\\nGiven a set of words w1,..., wnand a goal formula C, the question is now: is there\\na labeled tree Γwith formulas A1,..., Anas its yield, such that Γ⊢Cis derivable\\nandAi∈lex(wi)for all i(the implementation of Section 9.4.1 will automatically\\ncompute such a Γ).\\nThe rules of multimodal type-logical grammars are shown in Table 9.3. In the\\nrules, Γ[∆]denotes an antecedent tree Γwith distinguished subtree ∆— the subtree\\nnotation is a non-associative version of the Lambek calculus antecedent Γ,∆,Γ′,\\nwhere ∆is a subsequence instead of a subtree as it is in Γ[∆].\\nEach logical connective with mode iuses a structural connective ◦iin its rule. For\\nthe/E,•Iand\\\\Erules, reading from premisses to conclusions, we build structure.\\nFor the /I,•Eand\\\\Irules we remove a structural connective with the same mode\\nas the logical connective. The natural deduction rules use explicit antecedents, al-\\nthough, for convenience, we will again use coindexation between the introduction\\nrules for the implications ‘ /’ and ‘\\\\’ and its withdrawn premiss (and similarly for\\nthe•Erule and its two premisses).\\n4We can also allow unary branches (and, more generally n-ary branches) and the corresponding\\nlogical connectives. The unary connectives ♦and□are widely used, but, since they will only play\\na marginal role in what follows, I will not present them to keep the current presentation simple.\\nHowever, they form an essential part of the analysis of many phenomena and are consequently\\navailable in the implementation.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 10}), Document(page_content='12 Richard Moot\\nLogical Rules\\n∆⊢A•iBΓ[A◦iB]⊢C\\nΓ[∆]⊢C[•iE]Γ⊢A∆⊢B\\nΓ◦i∆⊢A•iB[•iI]\\nΓ⊢A/iB∆⊢B\\nΓ◦i∆⊢A[/iE]Γ◦iB⊢A\\nΓ⊢A/iB[/iI]\\nΓ⊢B∆⊢B\\\\iA\\nΓ◦i∆⊢A[\\\\iE]B◦iΓ⊢A\\nΓ⊢B\\\\iA[\\\\iI]\\nStructural Rules\\nΓ[Ξ′[∆1,...,∆n]]⊢C\\nΓ[Ξ[∆π1,...,∆πn]]⊢C[SR]\\nTable 9.3 Natural deduction for NLR\\nThe main advantage of adding modes to the logic is that modes allow us to control\\nthe application of structural rules lexically. This gives us ﬁne-grained control over\\nthe structural rules in our logic.\\nFor example, the base logic is non-associative. Without structural rules, the se-\\nquent a/b,b/c⊢a/c, which is derivable in the Lambek calculus is notderivable\\nin its multimodal incarnation a/ab,b/ac⊢a/ac. The proof attempt below, with the\\nfailed rule application marked by the ‘ E’ symbol, shows us that the elimination rules\\nand the introduction rule for this sequent do not match up correctly.\\na/ab⊢a/abb/ac⊢b/ac c⊢c\\nb/ac◦ac⊢b[/E]\\na/ab◦a(b/ac◦ac)⊢a[/E]\\n(a/ab◦ab/ac)◦ac⊢aE\\na/ab◦ab/ac⊢a/ac[/I]\\nThis is where the structural rules, shown at the bottom of Table 9.3 come in. The\\ngeneral form, read from top to bottom, states that we take a structure Γcontaining\\na distinguished subtree Ξwhich itself has nsubtrees ∆1,...,∆n, and we replace this\\nsubtree Ξwith a subtree Ξ′which has the same number of subtrees, though not\\nnecessarily in the same order ( πis a permutation on the leaves). In brief, we replace\\na subtree Ξby another subtree Ξ′and possibly rearrange the leaves (subtrees) of\\nΞ, without deleting or copying any subtrees. Examples of structural rules are the\\nfollowing.\\nΓ[∆1◦a(∆2◦a∆3)]⊢C\\nΓ[(∆1◦a∆2)◦a∆3]⊢CAssΓ[(∆1◦1∆3)◦0∆2]⊢C\\nΓ[(∆1◦0∆2)◦1∆3]⊢CMC\\nThe ﬁrst structural rule is one of the structural rules for associativity. It is the\\nsimplest rule which will make the proof attempt above valid (with Γ[]the empty', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 11}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 13\\ncontext, ∆1=a/ab,∆2=b/acand∆3=c). This structural rule keeps the order of\\nthe∆ithe same.\\nThe rule above on the right is slightly more complicated. There, the positions of\\n∆2and∆3are swapped as are the relative positions of modes 0 and 1. Rules like this\\nare called “mixed commutativity”, they permit controlled access to permutation.\\nOne way to see this rule, seen from top to bottom, is that is “moves out” a ∆3\\nconstituent which is on the right branch of mode 1. Rules of this kind are part of the\\nsolution to phenomena like Dutch verb clusters (Moortgat and Oehrle, 1994).\\nMany modern type-logical grammars, such as the Displacement calculus and\\nNLclcan be seen as multimodal grammars (Valent ´ın, 2014; Barker and Shan, 2014).\\n9.3.2 First-order linear logic\\nWe have seen that multimodal type-logical grammars generalise the Lambek calcu-\\nlus by offering the possibility of ﬁne-tuned controlled over the application of struc-\\ntural rules. In this section, I will introduce a second way of extending the Lambek\\ncalculus.\\nMany parsing algorithms use pairs of integers to represent the start and end po-\\nsition of substrings of the input string. For example, we can represent the sentence\\n(6) Alyssa believes someone committed perjury.\\nas follows (this is a slightly simpliﬁed version of Sentence (5) from Section 9.2.3);\\nwe have treated “committed perjury” as a single word.\\n0 1 2 3 4Alyssa believes someone committed perjury\\nThe basic idea of ﬁrst-order linear logic as a type-logical grammar is that we\\ncan code strings as pairs (or, more generally, tuples) of integers representing string\\npositions. So for deciding the grammaticality of a sequence of words w1,..., wn⊢C,\\nwith a goal formula C, we now give a parametric translation from ∥Ai∥i−1,ifor each\\nlexical entry wiand∥C∥0,nfor the conclusion formula.\\nGiven these string positions, we can assign the noun phrase “Alyssa” the formula\\nnp(0,1), that is a noun phrase from position 0 to position 1. The verb “believes”,\\nwhich occurs above between position 1 and 2, can then be assigned the complex\\nformula∀x2.[s(2,x2)⊸∀x1.[np(x1,1)⊸s(x1,x2)]], meaning that it ﬁrst selects a\\nsentence to its right (that is, starting at its right edge, position 2 and ending any-\\nwhere) and then a noun phrase to its left (that is, starting anywhere and ending at its\\nleft edge, position 1) to produce a sentence from the left position of the noun phrase\\nargument to the right position of the sentence argument.\\nWe can systematise this translation, following Moot and Piazza (2001), and ob-\\ntain the following translation from Lambek calculus formulas to ﬁrst-order linear\\nlogic formulas.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 12}), Document(page_content='14 Richard Moot\\n∥p∥x,y=p(x,y)\\n∥A/B∥x,y=∀z.∥B∥y,z⊸∥A∥x,z\\n∥B\\\\A∥y,z=∀x.∥B∥x,y⊸∥A∥x,z\\n∥A•B∥x,z=∃y.∥A∥x,y⊗∥B∥y,z\\nGiven this translation, the lexical entry for “believes” discussed above is simply\\nthe translation of the Lambek calculus formula (np\\\\s)/s, with position pair 1 ,2, to\\nﬁrst-order linear logic. Doing the same for “committed perjury” with formula np\\\\s\\nand positions 3 ,4 gives∀z.[np(z,3)⊸s(z,4)]. For “someone” we would simply\\ntranslate the Lambek calculus formula s/(np\\\\s), but we can do better than that:\\nwhen we translate “someone” as ∀y1.∀y2.[(np(2,3)⊸s(y1,y2))⊸s(y1,y2)], we\\nimprove upon the Lambek calculus analysis.\\nAs we noted in Section 9.2.3, the Lambek calculus cannot generate the “de re”\\nreading, where the existential quantiﬁer has wide scope. Figure 9.5 shows how the\\nsimple ﬁrst-order linear logic analysis does derive this reading.\\nnp(0,1)[np(2,3)]1∀A.[np(A,3)⊸s(A,4)]\\nnp(2,3)⊸s(2,4)∀E\\ns(2,4)⊸E∀B.[s(2,B)⊸∀C.[np(C,1)⊸s(C,B)]]\\ns(2,4)⊸∀C.[np(C,1)⊸s(C,4)]∀E\\n∀C.[np(C,1)⊸s(C,4)]⊸E\\nnp(0,1)⊸s(0,4)∀E\\ns(0,4)⊸E\\nnp(2,3)⊸s(0,4)⊸I1∀D.∀E.[(np(2,3)⊸s(D,E))⊸s(D,E)]\\n∀E.[(np(2,3)⊸s(0,E))⊸s(0,E)]∀E\\n(np(2,3)⊸s(0,4))⊸s(0,4)∀E\\ns(0,4)⊸E\\nFig. 9.5 “De re” reading for the sentence “Alyssa believes someone committed perjury”.\\nBesides the Lambek calculus, ﬁrst-order linear logic has many other modern\\ntype-logical grammars as fragments. Examples include lambda grammars (Oehrle,\\n1994), the Displacement calculus (Morrill et al, 2011) and hybrid type-logical gram-\\nmars (Kubota and Levine, 2012). We can see ﬁrst-order linear logic as a sort of\\n“machine language” underlying these different formalisms, with each formalism in-\\ntroducing its own set of abbreviations convenient for the grammar writer. Seeing\\nﬁrst-order linear logic as an underlying language allows us to compare the analyses\\nproposed for different formalisms and ﬁnd, in spite of different starting points, a\\nlot of convergence. In addition, as discussed in Section 9.4.2, we can use ﬁrst-order\\nlinear logic as a uniform proof strategy for these formalisms.\\nSyntax-Semantics Interface\\nAs usual, we obtain the deep structure of a syntactic derivation by deﬁning a homo-\\nmorphism from the syntactic proof to a proof in multiplicative intuitionistic linear\\nlogic. For ﬁrst-order linear logic, the natural mapping simply forgets all ﬁrst-order', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 13}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 15\\nA⊗B[A]i[B]i\\n....\\nC\\nC⊗EiA B\\nA⊗B⊗I\\nA A⊸B\\nB⊸E[A]i\\n....\\nB\\nA⊸B⊸I\\n∃x.A[A]i\\n....\\nC\\nC∃E∗\\niA[x:=t]\\n∃x.A∃I\\n∀x.A\\nA[x:=t]∀EA\\n∀x.A∀I∗\\n∗no free occurrences of xin any of the free hypotheses\\nTable 9.4 Natural deduction rules for MILL1\\nquantiﬁers and replaces all atomic predicates p(x1,..., xn)by propositions p. Since\\nthe ﬁrst-order variables have, so far, only been used to encode string positions, such\\na forgetful mapping makes sense.\\nHowever, other solutions are possible. When we add semantically meaningful\\nterms to ﬁrst-order linear logic, the Curry-Howard isomorphism for the ﬁrst-order\\nquantiﬁers will give us dependent types and this provides a natural connection to\\nthe work using dependent types for formal semantics (Ranta, 1991; Pogodalla and\\nPompigne, 2012; Luo, 2012b, 2015).\\n9.3.3 The Montagovian Generative Lexicon\\nIn the previous sections, we have discussed two general solutions to the problems\\nof the syntax-semantics interface of the Lambek calculus. Both solutions proposed\\na more ﬂexible syntactic logic. In this section, we will discuss a different type of\\nadded ﬂexibility, namely in the syntax-semantics interface itself.\\nThe basic motivating examples for a more ﬂexible composition have been amply\\ndebated in the literature (Pustejovsky, 1995; Asher, 2011). Our solution is essen-\\ntially the one proposed by Bassac et al (2010), called the Montagovian Generative\\nLexicon. I will only give a brief presentation of this framework. More details can be\\nfound in Chapter 6.\\nLike many other solutions, the ﬁrst step consists of splitting Montague’s type\\nefor entities into several types: physical objects, locations, informational objects,', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 14}), Document(page_content='16 Richard Moot\\neventualities, etc. Although there are different opinions with respect to the correct\\ngranularity of types (Pustejovsky, 1995; Asher, 2011; Luo, 2012a), nothing much\\nhinges on this for the present discussion.\\nThe second key element is the move to the second-order lambda calculus, sys-\\ntem F (Girard et al, 1995), which allows abstraction over types as well as over terms.\\nIn our Lambek calculus, the determiner “the” was assigned the formula np/nand\\nthe type of its lexical semantics was therefore (e→t)→e, which we implement\\nusing the ιoperators of type (e→t)→e, which, roughly speaking, selects a con-\\ntextually salient entity from (a characteristic function of) a set. When we replace\\nthe single type eby several different types, we want to avoid listing several separate\\nsyntactically identical by semantically different entries for “the” in the lexicon, and\\ntherefore assign it a polymorphic term Λα.ι(α→t)→αof type Πα.((α→t)→α),\\nquantifying over alltypes α. Though this looks problematic, the problem is re-\\nsolved once we realise that only certain function words (quantiﬁers, conjunctions\\nlike “and”) are assigned polymorphic terms and that we simply use universal in-\\nstantiation to obtain the value of the quantiﬁer variable. So if “student” is a noun of\\ntype human, that is of type h→t, then “the student” will be of type h, instantiating\\nαtoh. Formally, we use βreduction as follows (this is substitution of types instead\\nof terms, substituting type hforα).\\n((Λα.ι(α→t)→α){h}studenth→t) =β(ιstudent )h\\nThe ﬁnal component of the Montagovian Generative Lexicon is a set of lexi-\\ncally speciﬁed, optional transformations. In case of a type mismatch, an optional\\ntransformation can “repair” the term.\\nAs an example from Moot and Retor ´e (2011) and Mery et al (2013), one of the\\nclassic puzzles in semantics are plurals and collective and distributive readings. For\\nexample, verbs like “meet” have collective readings, they apply to groups of individ-\\nuals collectively, so we have the following contrast, where collectives like commit-\\ntees and plurals like students can meet, but not singular or distributively quantiﬁed\\nnoun phrases. The contrast with verbs like “sneeze”, which force a distributive read-\\ning is clear.\\n(7) The committee met.\\n(8) All/the students met\\n(9) *A/each/the student met.\\n(10) All/the students sneezed.\\n(11) A/each/the student sneezed.\\nIn the Montagovian Generative lexicon, we can models these fact as follows.\\nFirst, we assign the plural morphology “-s” the semantics Λαλ Pα→tλQα→t.|Q|>\\n1∧∀xα.Q(x)⇒P(x), then “students” is assigned the following term λQh→t.|Q|>\\n1∧∀xh.Q(x)⇒student (x), that is the sets of cardinality greater than one such that\\nall its members are students. Unlike “student” which was assigned a term of type\\nh→t, roughly a property of humans, the plural “students” is assigned a term of', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 15}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 17\\ntype(h→t)→t, roughly a property of sets of humans. Consequently, the contrast\\nbetween “the student” and “the students” is that the ﬁrst is of type h(a human) and\\nthe second of type h→t(a set of humans) as indicated below.\\nphrase syntactic type lambda-term\\nthe student np (ιstudent )h\\nthe students np (ι(λQh→t.|Q|>1∧∀xhQ(x)⇒student (x)))h→t\\nTherefore, the meaning of “the students” is the contextually determined set of\\nhumans, from the sets of more than one human such that all of them are students.\\nThen we distinguish the verbs “meet” and “sneeze” as follows, with the simpler\\nverb “sneeze” simply selecting for a human subject and the collective verb “meet”\\nselecting for a set of humans (of cardinality greater than one) as its subject.\\nword syntactic type lambda-term\\nmetnp\\\\s λPh→t.|P|>1∧meet(P)\\n# Λαλ R(α→t)→tλS(α→t)→t∀Pα→t.S(P)⇒R(P)\\nmet#np\\\\s λR(h→t)→t∀Ph→t.R(P)⇒|P|>1∧meet(P)\\nsneezed np\\\\s λxh.sneeze (x)\\n* Λαλ Pα→tλQα→t∀xα.Q(x)⇒P(x)\\nsneezed∗np\\\\s λPh→t.∀xh.P(x)⇒sneeze (x)\\nGiven these basic lexical entries, we already correctly predict that “the student\\nmet” is ill-formed semantically (there is an unresolvable type mismatch) but “the\\nstudents met” and “the student sneezed” are given the correct semantics.\\nThe interesting case is “the students sneezed” which has as its only reading that\\neach student sneezed individually. Given that “the students” is of type h→tand\\nthat “sneezed” requires an argument of type h, there is a type mismatch when we\\napply the two terms. However, “sneeze” has the optional distributivity operator ‘*’,\\nwhich when we apply it to the lexical semantics for “sneeze” produces the term\\nλPh→t.∀xh.P(x)⇒sneeze (x), which combines with “the students” to produce the\\nreading.\\n∀xh.(ι(λQh→t.|Q|>1∧∀yhQ(y)⇒student (y))x)⇒sneeze (x)\\nIn other words, all of the members of the contextually determined set of more\\nthan human which are all students, sneeze.\\nThe basic idea for the Montagovian Generative Lexicon is that lexical entries\\nspecify optional transformations which can repair certain sorts of type mismatches\\nin the syntax-semantics interface. This adaptability allows the framework to solve\\nmany semantic puzzles.\\nThough a proof-of-concept application of these ideas exists, more robust and\\nscalable applications, as well as efforts incorporate these ideas into wide-coverage\\nsemantics, are ongoing research.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 16}), Document(page_content='18 Richard Moot\\n9.4 Theorem proving\\nWhen looking at the rules and examples for the different logics, the reader may have\\nwondered: how do we actually ﬁnd proofs for type-logical grammars? This question\\nbecomes especially urgent once our grammars become more complex and the con-\\nsequences of our lexical entries, given our logic, become hard to oversee. Though\\npen and paper generally sufﬁce to show that a given sentence is derivable for the\\ndesired reading, it is generally much more laborious to show that a given sentence\\nis underivable or that it has only the desired readings. This is where automated the-\\norem provers are useful: they allow more extensive and intensive testing of your\\ngrammars, producing results more quickly and with less errors (though we should\\nbe careful about too naively assuming the implementation we are using is correct:\\nwhen a proof is found it is generally easy to verify its correctness by hand, but when\\na proof isn’t found because of a programming error this can be hard to detect).\\nThough the natural deduction calculi we have seen so far can be used for au-\\ntomated theorem proving (Carpenter, 1994; Moot and Retor ´e, 2012), and though\\nLambek (1958) already gave a sequent calculus decision procedure, both logics have\\nimportant drawbacks for proof search.\\nNatural deduction proofs have a 1-1 correspondence between proofs and read-\\nings, though this is somewhat complicated to enforce for a logic with the •Erule\\n(and the related♦Erule). For the sequent calculus, the product rule is just like the\\nother rules, but sequent calculus suffers from the so-called “spurious ambiguity”\\nproblem, which means that it generates many more proofs than readings.\\nFortunately, there are proof systems which combine the good aspects of natural\\ndeduction and sequent calculus, and which eliminate their respective drawbacks.\\nProof nets are a graphical representation of proofs ﬁrst introduced for linear logic\\n(Girard, 1987). Proof nets suffer neither from spurious ambiguity nor from compli-\\ncations for the product rules.\\nProof nets are usually deﬁned as a subset of a larger class, called proof struc-\\ntures . Proof structures are “candidate proofs”: part of the search space of a naive\\nproof search procedure which need not correspond to actual proofs. Proof nets are\\nthose proof structures which correspond to sequent proofs. Perhaps surprisingly, we\\ncan distinguish proof nets from other proof structures by looking only at graph-\\ntheoretical properties of these structures.\\nProof search for type-logical grammars using proof nets uses the following gen-\\neral procedure.\\n1. For each of the words in the input sentence, ﬁnd one of the formulas assigned to\\nit in the lexicon.\\n2. Unfold the formulas to produce a partial proof structure.\\n3. Enumerate all proof structures for the given formulas by identifying nodes in the\\npartial proof structure.\\n4. Check if the resulting proof structure is a proof net according to the correctness\\ncondition.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 17}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 19\\nIn Sections 9.4.1 and 9.4.2 we will instantiate this general procedure for multi-\\nmodal type-logical grammar and for ﬁrst-order linear logic respectively.\\n9.4.1 Multimodal proof nets\\n[/E]CC/iB B\\ni\\n[/I]C/iBi\\nBC[\\\\E]CA\\\\iC A\\ni\\n[\\\\I]A\\\\iCi\\nAC[•E]Ai\\nBA•iB\\n[•I]A•iBB A\\ni\\nTable 9.5 Links for multimodal proof nets\\nTable 9.5 presents the links for multimodal proof nets. The top row list the links\\ncorresponding to the elimination rules of natural deduction, the bottom row those\\ncorresponding to the introduction rules. There are two types of links: tensor links,\\nwith an open center, and par links, with a ﬁlled center. Par links have a single arrow\\npointing to the main formula of the link (the complex formula containing the prin-\\ncipal connective). The top and bottom row are up-down symmetric with tensor and\\npar reversed. The tensor links correspond to the logical rules which build structure\\nwhen we read them from top to bottom, the par links to those rules which remove\\nstructure.\\nThe formulas written above the central node of a link are its premisses, whereas\\nthe formulas written below it are its conclusions. Left-to-right order of the premisses\\nas well as the conclusions is important.\\nAproof structure is a set of formula occurrences and a set of links such that:\\n1. each formula is at most once the premiss of a link,\\n2. each formula is at most once the conclusion of a link.\\nA formula which is not the premiss of any link is a conclusion of the proof\\nstructure. A formula which is not the conclusion of any link is a hypothesis of the\\nproof structure. We say a proof structure with hypotheses Γand conclusions ∆is a\\nproof structure of Γ⊢∆(we are overloading of the ‘ ⊢’ symbol here, though this use\\nshould always be clear from the context; note that ∆can contain multiple formulas).\\nAfter the ﬁrst step of lexical lookup we have a sequent Γ⊢C, and we can enu-\\nmerate its proof structures as follows: unfold the formulas in Γ,C, unfolding them', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 18}), Document(page_content='20 Richard Moot\\nso that the formulas in Γare hypotheses and the formula Cis a conclusion of the\\nresulting structure, until we reach the atomic subformulas (this is step 2 of the gen-\\neral procedure), then identify atomic subformulas (step 3 of the general procedure,\\nwe turn to the last step, checking correctness, below). This identiﬁcation step can,\\nby the conditions on proof structures only identify hypotheses with conclusions and\\nmust leave all formulas of Γ, including atomic formulas, as hypotheses and Cas a\\nconclusion.\\naa/ab b\\na\\nbb/ac c\\na\\na/aca\\nca\\nFig. 9.6 Lexical unfolding of a/ab,b/ac⊢a/ac\\nFigure 9.6 shows the lexical unfolding of the sequent a/ab,b/ac⊢a/ac. It is\\nalready a proof structure, though a proof structure of a,a/ab,b,b/ac,c⊢a,a/ac,b,c\\n(to the reader familiar with the proof nets of linear logic: some other presentations of\\nproof nets use more restricted deﬁnitions of proof structures where a “partial proof\\nstructure” such as shown in the ﬁgure is called a module ).\\naa/ab b\\nab/ac c\\na\\na/aca\\ncaa/ab b\\nab/ac c\\na\\na/aca\\nFig. 9.7 The proof structure of Figure 9.6 after identiﬁcation of the aandbatoms (left) and after\\nidentiﬁcation of all atoms\\nTo turn this proof structure into a proof structure of a/ab,b/ac⊢a/ac, we identify\\nthe atomic formulas. In this case, there is only a single way to do this, since a,band\\ncall occur once as a hypothesis and once as a conclusion, though in general there\\nmay be many possible matchings. Figure 9.7 shows, on the left, the proof structure\\nafter identifying the aandbformulas. Since left and right (linear order), up and\\ndown (premiss, conclusion) have meaning in the graph, connecting the cformulas\\nis less obvious: cis a conclusion of the /Ilink and must therefore be below it, but', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 19}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 21\\na premiss of the /Elink and must therefore be above it. This is hard to achieve in\\nthe ﬁgure shown on the left. Though a possible solution would be to draw the ﬁgure\\non a cylinder, where “going up” from the topmost cwe arrive at the bottom one, for\\nease of type-setting and reading the ﬁgure, I have chosen the representation shown\\nin Figure 9.7 on the right. The curved line goes up from the cpremiss of the /Elink\\nand arrives from below at the /Ilink, as desired. One way so see this strange curved\\nconnection is as a graphical representation of the coindexation of a premiss with a\\nrule in the natural deduction rule for the implication.\\nFigure 9.7 therefore shows, on the right, a proof structure for a/ab,b/ac⊢a/ac.\\nHowever, is it also a proof net , that is, does it correspond to a proof? In a multi-\\nmodal logic, the answer depends on the available structural rules. For example, if\\nno structural rules are applicable to mode athen a/ab,b/ac⊢a/acis underivable,\\nbut if mode ais associative, then it is derivable.\\naa/ab b\\nab/ac c\\na\\na/aca\\x05a/ab\\x05\\nab/ac\\x05\\na\\na/aca\\nFig. 9.8 The proof structure of Figure 9.7 (left) and its abstract proof structure (right)\\nWe decide whether a proof structure is a proof net based only on properties of\\nthe graph. As a ﬁrst step, we erase all formula information from the internal nodes\\nof the graph; for administrative reasons, we still need to be able to identify which of\\nthe hypotheses and conclusion of the structure correspond to which formula occur-\\nrence5. All relevant information for correctness is present in this graph, which we\\ncall an abstract proof structure .\\nWe talked about how the curved line in proof structures (and abstract proof struc-\\nture) corresponds to the coindexation of discharged hypotheses with rule names for\\n5We make a slight simpliﬁcation here. A single vertex abstract proof structure can have both\\na hypothesis and a conclusion without these two formulas necessarily being identical, e.g. for\\nsequents like (a/b)•b⊢a. Such a sequent would correspond to the abstract proof structure(a/b)•b·\\na.\\nSo, formally, both the hypotheses and the conclusions of an abstract proof structure are assigned\\na formula and when a node is both a hypothesis and a conclusion it can be assigned two different\\nformulas. In order not to make the notation of abstract proof structure more complex, we will stay\\nwith the simpler notation. Moot and Puite (2002) present the full details.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 20}), Document(page_content='22 Richard Moot\\nthe implication introduction rules. However, the introduction rules for multimodal\\ntype-logical grammars actually do more than just discharge a hypothesis, they also\\ncheck whether the discharged hypothesis is the immediate left (for \\\\I) or right (for\\n/I) daughter of the root node, that is, that the withdrawn hypothesis Aoccurs as\\nA◦iΓ(for\\\\Iand mode i) orΓ◦iA(for/Iand mode i). The par links in the (ab-\\nstract) proof structure represent a sort of “promise” that will produce the required\\nstructure. We check whether it is satisﬁed by means of contractions on the abstract\\nproof structure.\\n[/I]\\x05\\x05\\x05\\ni\\n\\x05i\\n[•E]\\x05\\x05\\x05\\ni\\x05\\ni\\n[\\\\I]\\x05\\x05\\x05\\ni\\n\\x05i\\nTable 9.6 Contractions — multimodal binary connectives\\nThe multimodal contractions are shown in Table 9.6. All portrayed conﬁgurations\\ncontract to a single vertex: we erase the two internal vertices and the paired links\\nand we identify the two external vertices, keeping all connections of the external\\nvertices to the rest of the abstract proof structure as they were: the vertex which is\\nthe result of the contraction will be a conclusion of the same link as the top external\\nvertex (or a hypothesis of the abstract proof structure in case it wasn’t) and it will\\nbe a premiss of the same link as the bottom external vertex (or a conclusion of the\\nabstract proof structure in case it wasn’t).\\nThe contraction for /Ichecks if the withdrawn hypothesis is the right daughter\\nof a tensor link with the same mode information i, and symmetrically for the \\\\I\\ncontraction. The•Econtraction contracts two hypotheses occurring as sister nodes.\\nAll contractions are instantiations of the same pattern: a tensor link and a par\\nlink are connected, respecting left-right and up-down the two vertices of the par link\\nwithout the arrow.\\nTo get a better feel for the contractions, we will start with its simplest instances.\\nWhen we do pattern matching on the contraction for /I, we see that it corresponds\\nto the following patterns, depending on our choice for the tensor link (the par link is\\nalways /I).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 21}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 23\\nC/iB⊢C/iB\\nA⊢(A•iB)/iB\\nA⊢C/i(A\\\\iC)\\nA proof structure is a proof net iff it contracts to a tree containing only tensor\\nlinks using the contractions of Table 9.6 and any structural rewrites, discussed below\\n— Moot and Puite (2002) present full proofs. In other words, we need to contract all\\npar links in the proof structure according to their contraction, each contraction en-\\nsuring the correct application of the rule after which it is named. The abstract proof\\nstructure on the right of Figure 9.8 does not contract, since there is no substructure\\ncorresponding to the /Icontraction: for a valid contraction, a par link is connected\\nto both “tentacles” of a single tensor link, and in the ﬁgure the two tentacles without\\narrow are connected to different tensor links. This is correct, since a/ab,b/ac⊢a/ac\\nis underivable in a logic without structural rules for a.\\nHowever, we have seen that this statement becomes derivable once we add asso-\\nciativity of aand it is easily veriﬁed to be a theorem of the Lambek calculus. How\\ncan we add a modally controlled version of associativity to the proof net calculus?\\nWe can add such a rule by adding a rewrite from a tensor tree to another tensor tree\\nwith the same set of leaves. The rewrite for associativity is shown in Figure 9.9. To\\napply a structural rewrite, we replace the tree on the left hand side of the arrow by\\nthe one on the right hand side, reattaching the leaves and the root to the rest of the\\nproof net.\\nJust like the structural rules, a structural rewrite always has the same leaves on\\nboth sides of the arrow — neither copying nor deletion is allowed6, though we can\\nreorder the leaves in any way (the associativity rule doesn’t reorder the leaves).\\nvx\\x05\\nay z\\na\\nv\\x05 z\\nax y\\na\\nFig. 9.9 Structural rewrites for associativity of mode a.\\nFigure 9.10 shows how the contractions and the structural rewrites work together\\nto derive a/ab,b/ac⊢a/ac.\\nWe start with a structural rewrite, which rebrackets the pair of tensor links. The\\ntwo hypotheses are now the premisses of the same link, and this also produces a\\ncontractible structure for the /Ilink. Hence, we have shown the proof structure to\\nbe a proof net.\\n6From the point of view of linear logic, we stay within the purely multiplicative fragment, which\\nis simplest proof-theoretically.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 22}), Document(page_content='24 Richard Moot\\n\\x05a/ab\\x05\\nab/ac\\x05\\na\\na/aca\\x05\\x05\\x05\\naa/ab b/ac\\na\\na/acaa/ab b/ac\\na\\na/ac\\nFig. 9.10 Structural rewrite and contraction for the abstract proof structure of Figure 9.8, showing\\nthis is a proof net for a/ab◦ab/ac⊢a/ac\\nthe1n1exam42was5n1difﬁcult12n1erratic15n1\\n8Goal23\\na192632313n316231023\\n12\\nFig. 9.11 Interactive Grail outputIn the Grail theorem prover, the rep-\\nresentation of abstract proof structures\\nlooks as shown in Figure 9.11 (this is an\\nautomatically produced subgraph close\\nto the graph on the left of Figure 9.10,\\nthough with a non-associative mode n\\nand therefore not derivable). This graph\\nis used during user interaction. The\\ngraphs are drawn using GraphViz, an\\nexternal graph drawing program which\\ndoes not guarantee respecting our de-\\nsires for left, right and top/bottom, so\\ntentacles are labeled 1, 2 and 3 (for\\nleft, right and top/bottom respectively)\\nto allow us to make these distinctions\\nregardless of the visual representation.\\nVertices are given unique identiﬁers for\\nuser interaction, for example to allow\\nspecifying which pair of atoms should\\nbe identiﬁed or which par link should\\nbe contracted.\\nAlthough the structural rules give\\nthe grammar writer a great deal of ﬂexibility, such ﬂexibility complicates proof\\nsearch. As discussed at the beginning of Section 9.4, theorem proving using proof\\nnets is a four step process, which in the current situation looks as follows: 1) lexical\\nlookup, 2) unfolding, 3) identiﬁcation of atoms, 4) graph rewriting. In the current', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 23}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 25\\ncase, both the graph rewriting and the identiﬁcation of atoms are complicated7and\\nsince we can interleave the atom connections and the graph rewriting it is not a\\npriori clear which strategy is optimal for which set of structural rules. The current\\nimplementation does graph rewriting only once all atoms have been connected.\\nThe Grail theorem prover implements some strategies for early failure. Since all\\nproofs in multimodal type-logical grammars are a subset of the proofs in multi-\\nplicative linear logic, we can reject (partial) proof structures which are invalid in\\nmultiplicative linear logic, a condition which is both powerful and easy to check.\\nAs a compromise between efﬁciency and ﬂexibility, Grail allows the grammar\\nwriter to specify a ﬁrst-order approximation of her structural rules. Unlike the test\\nfor validity in multiplicative linear logic which is valid for any set of structural rules,\\nspecifying such a ﬁrst-order approximation is valid only when there is a guarantee\\nthat all derivable sequents in the multimodal grammar are a subset of their approx-\\nimations derivable in ﬁrst-order linear logic. Errors made here can be rather subtle\\nand hard to detect. It is recommended to use such methods to improve parsing speed\\nonly when a grammar has been sufﬁciently tested and where it is possible to verify\\nwhether no valid readings are excluded, or, ideally, to prove that the subset relation\\nholds between the multimodal logic and its ﬁrst-order approximation.\\nThe next section will discuss ﬁrst-order proof nets in their own right. Though\\nthese proof nets have been used as an underlying mechanism in Grail for a long\\ntime, we have seen in Section 9.3.2 that many modern type-logical grammars are\\nformulated in a way which permits a direct implementation without an explicit set\\nof structural rules.\\nAs to the proof search strategy used by Grail, it is an instance of the “dancing\\nlinks” algorithm (Knuth, 2000): when connecting atomic formulas, we always link a\\nformula which has the least possibilities and we rewrite the abstract proof structures\\nonly once a fully linked proof structure has been produced. Though the parser is\\nnot extremely fast, evaluation both on randomly generated statements and on multi-\\nmodal statements extracted from corpora show that the resulting algorithm performs\\nmore than well enough (Moot, 2008).\\n9.4.2 First-order proof nets\\nProof nets for ﬁrst-order linear logic (Girard, 1991) are a simple extension of the\\nproof nets for standard, multiplicative linear logic (Danos and Regnier, 1989). Com-\\npared to the multimodal proof nets of the previous section, all logical links have the\\nmain formula of the link as their conclusion but there is now a notion of polarity ,\\ncorresponding to whether or not the formula occurs on the left hand side of the\\nturnstile (negative polarity) or on the right hand side (positive polarity).\\n7Lexical ambiguity is a major problem for automatically extracted wide-coverage grammars as\\nwell, though standard statistical methods can help alleviate this problem (Moot, 2010).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 24}), Document(page_content='26 Richard Moot\\nWe unfold a sequent A1,..., An⊢Cby using the negative unfolding for each of\\ntheAiand the positive unfolding for C. The links for ﬁrst-order proof nets are shown\\nin Table 9.7.\\n−\\nA+\\nA−\\nA+\\nA\\n−\\n∀x.A−\\nA[x:=t]\\n+\\n∀x.A+\\nA\\nx\\n−\\n∃x.A−\\nA\\nx−\\nA⊗B−\\nA−\\nB\\n+\\nA⊗B+\\nA+\\nB\\n+\\n∃x.A+\\nA[x:=t]−\\nA⊸B+\\nA−\\nB\\n+\\nA⊸B−\\nA+\\nB\\nTable 9.7 Logical links for MILL1 proof structures\\nContrary to multimodal proof nets, where a tensor link was drawn with an open\\ncentral node and a par link with a ﬁlled central node, here par links are drawn as a\\nconnected pair of dotted lines and tensor links as a pair of solid lines.\\nAs before, premisses are drawn above the link and conclusions are drawn below\\nit. With the exception of the cut and axiom links, the order of the premisses and the\\nconclusions is important. We assume without loss of generality that every quantiﬁer\\nlink uses a distinct eigenvariable.\\nA set of formula occurrences connected by links is a proof structure if every\\nformula is at most once the premiss of a link and if every formula is exactly once\\nthe conclusion of a link. Those formulas which are not the premiss of any link are\\nthe conclusions of the proof structure — note the difference with multimodal proof\\nnets: a proof structure has conclusions but no hypotheses and, as a consequence,\\neach formula in the proof net must be the conclusion of exactly one (instead of at\\nmost one) link.\\nFor polarised proof nets, unfolding the formulas according to the links of Ta-\\nble 9.7 no longer produces a proof structure, since the atomic formulas after unfold-\\ning are not the conclusions of any link. Such “partial proof structures” are called a\\nmodules. To turn a module into a proof structure, we connect atomic formulas of\\nopposite polarity by axiom links until we obtain a complete matching of the atomic\\nformulas, that is until every atomic formula is the conclusion of an axiom link.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 25}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 27\\nThe negative∀and the positive∃link, are deﬁned using substitution of an arbi-\\ntrary term tfor the eigenvariable of the link. In actual proof search, we use uniﬁca-\\ntion of these variables when the axiom links are performed.\\nAs usual, not all proof structures are proof nets. However, since the logical rules\\nfor the quantiﬁers make essential use of the notion of “free occurrence of a variable”,\\nthis should be reﬂected in out correctness condition. Girard (1991) uses a notion of\\nswitching for proof structures which extends the switchings of Danos and Regnier\\n(1989).\\nAswitching is, for each of the binary par links a choice of its left or right premiss\\nand for each of the unary par links with eigenvariable xa choice of one of the\\nformulas in the structure with a free occurrence of x orof the premiss of the rule.\\nGiven a switching, a correction graph replaces a binary par link by a connection\\nfrom the conclusion of the link to the premiss chosen by the switching, and it re-\\nplace a unary par link by a link from the conclusion to the formula chosen by the\\nswitching.\\nFinally, a proof structure is a proof net when all its correction graphs are both\\nacyclic and connected (Girard, 1991).\\nAs an example, look at the proof structure of a⊸∃x.b(x)⊢∃y.[a⊸b(y)]shown\\nin Figure 9.12. This statement is notderivable in ﬁrst-order linear logic (nor in intu-\\nitionistic logic). Consider therefore the switching connecting the binary par link to\\nits left premiss aand the link for xto the formula a⊸b(x)(it has a free occurrence\\nofx, so this like is a valid switching).\\n−\\na⊸∃x.b(x)+a−\\n∃x.b(x)−\\nb(x)\\nx\\n+\\n∃y.[a⊸b(y)]+\\na⊸b(x)−a+\\nb(x)\\nFig. 9.12 Proof structure for a⊸∃x.b(x)⊢∃y.[a⊸b(y)].\\nThis switching produces the correction graph shown in Figure 9.13. It contains a\\ncycle, drawn with bold edges, and is therefore not a proof structure (in addition, the\\nbaxiom is disconnected from the rest of the structure, giving a second reason for\\nrejecting the proof structure).', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 26}), Document(page_content='28 Richard Moot\\n−\\na⊸∃x.b(x)+a−\\n∃x.b(x)−\\nb(x)\\n+\\n∃y.[a⊸b(y)]+\\na⊸b(x)−a+\\nb(x)\\nFig. 9.13 Correction graph for the proof structure of Figure 9.12 with the cycle indicated, showing\\na⊸∃x.b(x)⊢∃y.[a⊸b(y)]is underivable\\nContractions\\nThough switching conditions for proof nets are simple and elegant, they don’t lend\\nthemselves to naive application: already for the example proof structure of Fig-\\nure 9.12 there are six possible switchings to consider and, as the reader can verify,\\nonly the switching shown in Figure 9.13 is cyclic (and disconnected). In general, it\\nis often the case that all switchings but one are acyclic and connected, as it is here.\\nThough there are efﬁcient ways of testing acyclicity and connectedness for mul-\\ntiplicative proof nets (Guerrini, 1999; Murawski and Ong, 2000) and it seems these\\ncan be adapted to the ﬁrst-order case (though some care needs to be taken when we\\nallow complex terms), the theorem prover for ﬁrst-order linear logic uses a extension\\nof the contraction criterion of Danos (1990).\\nGiven a proof structure we erase all formulas from the vertices and keep only\\na set of the free variables at this vertex. We then use the contractions of Table 9.8\\nto contract the edges of the graph. The resulting vertex of each contraction has\\nthe union of the free variables of the two vertices of the redex (we remove the\\neigenvariable xof a∀contraction, “⇒u”). A proof structure is a proof net iff it\\ncontracts to a single vertex using the contractions of Table 9.8.\\nvivj\\nvi⇒p\\nvivj\\nx vi⇒u\\nvivj\\nvi⇒c\\nTable 9.8 Contractions for ﬁrst-order linear logic. Conditions: vi̸=vjand, for the ucontraction,\\nall free occurrences of xare at vj.\\nTo give an example of the contractions, Figure 9.14 shows the contractions for the\\nunderivable proof structure of Figure 9.12. The initial structure, which simply takes\\nthe proof structure of Figure 9.12 and replaces the formulas by the corresponding', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 27}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 29\\nset of free variables, is shown on the left. Contracting the ﬁve solid edges using the\\nccontraction produces the structure shown in the ﬁgure on the right.\\n⇒∗{x} / 0\\n{x}x\\n/ 0/ 0 / 0{x}\\nx\\n/ 0{x}/ 0{x}\\nFig. 9.14 Contractions for the underivable a⊸∃x.b(x)⊢∃y.[a⊸b(y)].\\nNo further contractions apply: the two connected dotted links from the binary par\\nlink do not end in the same vertex, so the par contraction pcannot apply. In addition,\\nthe universal contraction ucannot apply either, since it requires all vertices with\\nits eigenvariable xto occur at the node from which the arrow is leaving and there\\nis another occurrence of xat the bottom node of the structure. We have therefore\\nshown that this is not a proof net.\\nSince there are no structural rewrites, the contractions for ﬁrst-order linear logic\\nare easier to apply than those for multimodal type-logical grammars: it is rather easy\\nto show conﬂuence for the contractions (the presence of structural rules, but also the\\nunary versions of the multimodal contractions, means conﬂuence is not guaranteed\\nfor multimodal proof nets). We already implicitly used conﬂuence when we argued\\nthat the proof structure in Figure 9.14 was not a proof net. The theorem prover uses\\na maximally contracted representation of the proof structure to represent the current\\nstate of proof search and this means less overhead and more opportunities for early\\nfailure during proof search.\\nLike before, the theorem proving uses four steps, which look as follows in the\\nﬁrst-order case: 1) lexical lookup, 2) unfolding, 3) axiom links with uniﬁcation, 4)\\ngraph contraction. Unlike the multimodal proof nets of the previous section, the\\ngraph contractions are now conﬂuent and can be performed efﬁciently (the linear\\ntime solutions for the multiplicative case may be adaptable, but a naive implemen-\\ntation already has an O(n2)worst-case performance). After lexical lookup, theorem\\nproving for ﬁrst-order linear logic unfolds the formulas as before, but uses a greedy\\ncontraction strategy. This maximally contracted partial proof net constrains further\\naxiom links: for example, a vertex containing a free variable xcannot be linked to\\nthe conclusion of the edge of its eigenvariable (the vertex to which the arrow of\\nthe edge with variable xpoints) or to one of its descendants, since such a structure\\nwould fail to satisfy the condition that the two vertices of a ∀link for the ucontrac-\\ntion of Figure 9.8 are distinct. Another easily veriﬁed constraint is that two atomic', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 28}), Document(page_content='30 Richard Moot\\nformulas can only be connected by an axiom link if these formulas unify8. Like for\\nmultimodal proof nets, the ﬁrst-order linear logic theorem prover chooses an axiom\\nlink for one of the atoms with the fewest possibilities.\\n9.4.3 Tools\\nTable 9.9 lists the different theorem provers which are available. Grail 0 (Moot\\net al, 2015) and Grail 3 (Moot, 2015a) use the multimodal proof net calculus of\\nSection 9.4.1, whereas LinearOne (Moot, 2015c) uses the ﬁrst-order proof nets of\\nSection 9.4.2. GrailLight (Moot, 2015b) is a special-purpose chart parser, intended\\nfor use with an automatically extracted French grammar for wide-coverage parsing\\nand semantics (Moot, 2010, 2012). All provers are provided under the GNU Lesser\\nGeneral Public License — this means, notably, there is no warranty, though I am\\ncommitted to making all software as useful as possible; so contact me for any com-\\nments, feature requests or bug reports. All theorem provers can be downloaded from\\nthe author’s GitHub site.\\nhttps://github.com/RichardMoot/\\nThe columns of table Table 9.9 indicate whether the theorem provers provide\\nnatural deduction output, graph output (of the partial proof nets), whether there is\\nan interactive mode for proof search, whether the implementation is complete and\\nwhether the grammar can specify its own set of structural rules; “NA” means the\\nquestion doesn’t apply to the given system (GrailLight doesn’t use a graphs to rep-\\nresent proofs and ﬁrst-order linear logic does not have a grammar-speciﬁc set of\\nstructural rules). The table should help you select the most adequate tool for your\\npurposes.\\nLinearOne provides natural deduction output not only for ﬁrst-order linear logic,\\nbut also for the Displacement calculus, hybrid type-logical grammars and lambda\\ngrammars. That is, the grammar writer can write a grammar in any of these for-\\nmalisms, LinearOne will do proof search of the translation of this grammar in ﬁrst-\\norder linear logic and then translate any resulting proofs back to the source language.\\nProver ND Graph Interactive Complete User-deﬁned SR\\nGrail 0 + – – + +\\nGrail 3 – + + + +\\nGrailLight + NA + – –\\nLinearOne + + – + NA\\nTable 9.9 The different theorem provers\\n8As discussed in Section 9.4.1, the multimodal theorem prover allows the grammar writer to\\nspecify ﬁrst-order approximations of speciﬁc formulas. So underneath the surface of Grail there is\\nsome ﬁrst-order reasoning going on as well.', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 29}), Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 31\\nThe syntactic example proofs in this chapter have been automatically generated\\nusing these tools and the corresponding grammars ﬁles, as well as many other ex-\\nample grammars, are included in the repository.\\nReferences\\nAjdukiewicz K (1935) Die syntaktische Konnexit ¨at. Studies in Philosophy 1:1–27\\nAsher N (2011) Lexical Meaning in Context: A Web of Words. Cambridge University Press\\nBar-Hillel Y (1953) A quasi-arithmetical notation for syntactic description. Language 29(1):47–58\\nBarker C, Shan C (2014) Continuations and Natural Language. Oxford Studies in Theoretical\\nLinguistics, Oxford University Press\\nBassac C, Mery B, Retor ´e C (2010) Towards a type-theoretical account of lexical semantics. Jour-\\nnal of Logic, Language and Information 19(2):229–245, DOI 10.1007/s10849-009-9113-x,\\nURLhttp://dx.doi.org/10.1007/s10849-009-9113-x\\nvan Benthem J (1995) Language in Action: Categories, Lambdas and Dynamic Logic. MIT Press,\\nCambridge, Massachusetts\\nCarpenter B (1994) A natural deduction theorem prover for type-theoretic categorial grammars.\\nTech. rep., Carnegie Mellon Laboratory for Computational Linguistics, Pittsburgh, Pennsylva-\\nnia\\nChatzikyriakidis S (2015) Natural language reasoning using Coq: Interaction and automation. In:\\nProceedings of Traitement Automatique des Langues Naturelles (TALN 2015)\\nDanos V (1990) La logique lin ´eaire appliqu ´ee`a l’´etude de divers processus de normalisation (prin-\\ncipalement du λ-calcul). PhD thesis, University of Paris VII\\nDanos V , Regnier L (1989) The structure of multiplicatives. Archive for Mathematical Logic\\n28:181–203\\nGirard JY (1987) Linear logic. Theoretical Computer Science 50:1–102\\nGirard JY (1991) Quantiﬁers in linear logic II. In: Corsi G, Sambin G (eds) Nuovi problemi della\\nlogica e della ﬁlosoﬁa della scienza, CLUEB, Bologna, Italy, vol II, proceedings of the confer-\\nence with the same name, Viareggio, Italy, January 1990\\nGirard JY , Lafont Y , Regnier L (eds) (1995) Advances in Linear Logic. London Mathematical\\nSociety Lecture Notes, Cambridge University Press\\nGuerrini S (1999) Correctness of multiplicative proof nets is linear. In: Fourteenth Annual IEEE\\nSymposium on Logic in Computer Science, IEEE Computer Science Society, pp 454–263\\nHuijbregts R (1984) The weak inadequacy of context-free phrase structure grammars. In: de Haan\\nG, Trommelen M, Zonneveld W (eds) Van Periferie naar Kern, Foris, Dordrecht\\nKnuth DE (2000) Dancing links. arXiv preprint cs/0011047\\nKubota Y , Levine R (2012) Gapping as like-category coordination. In: B ´echet D, Dikovsky A (eds)\\nLogical Aspects of Computational Linguistics, Springer, Nantes, Lecture Notes in Computer\\nScience, vol 7351, pp 135–150\\nLambek J (1958) The mathematics of sentence structure. American Mathematical Monthly\\n65:154–170\\nLuo Z (2012a) Common nouns as types. In: Logical aspects of computational linguistics\\n(LACL2012), Springer, Lecture Notes in Artiﬁcial Intelligence, vol 7351\\nLuo Z (2012b) Formal semantics in modern type theories with coercive subtyping. Linguistics and\\nPhilosophy 35(6):491–513\\nLuo Z (2015) A Lambek calculus with dependent types. In: Types for Proofs and Programs\\n(TYPES 2015), Tallinn\\nMery B, Moot R, Retor ´e C (2013) Plurals: individuals and sets in a richly typed semantics. In:\\nThe Tenth International Workshop of Logic and Engineering of Natural Language Semantics\\n10 (LENLS10)', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 30}), Document(page_content='32 Richard Moot\\nMineshima K, Mart ´ınez-G ´omez P, Miyao Y , Bekki D (2015) Higher-order logical inference with\\ncompositional semantics. In: Proceedings of Empirical Method for Natural Language Process-\\ning (EMNLP 2015)\\nMontague R (1970) Universal grammar. Theoria 36(3):373–398\\nMontague R (1974) The proper treatment of quantiﬁcation in ordinary English. In: Thomason R\\n(ed) Formal Philosophy. Selected Papers of Richard Montague, Yale University Press, New\\nHaven\\nMoortgat M (2011) Categorial type logics. In: van Benthem J, ter Meulen A (eds) Handbook of\\nLogic and Language, North-Holland Elsevier, Amsterdam, chap 2, pp 95–179\\nMoortgat M, Oehrle RT (1994) Adjacency, dependency and order. In: Proceedings 9th Amsterdam\\nColloquium, pp 447–466\\nMoot R (2008) Filtering axiom links for proof nets. Tech. rep., CNRS and Bordeaux University\\nMoot R (2010) Wide-coverage French syntax and semantics using Grail. In: Proceedings of Traite-\\nment Automatique des Langues Naturelles (TALN), Montreal, system Demo\\nMoot R (2012) Wide-coverage semantics for spatio-temporal reasoning. Traitement Automatique\\ndes Languages 53(2):115–142\\nMoot R (2015a) Grail. http://www.labri.fr/perso/moot/grail3.html , mature and\\nﬂexible parser for multimodal grammars\\nMoot R (2015b) Grail light. https://github.com/RichardMoot/GrailLight , fast,\\nlightweight version of the Grail parser\\nMoot R (2015c) Linear one: A theorem prover for ﬁrst-order linear logic.\\nhttps://github.com/RichardMoot/LinearOne\\nMoot R, Piazza M (2001) Linguistic applications of ﬁrst order multiplicative linear logic. Journal\\nof Logic, Language and Information 10(2):211–232\\nMoot R, Puite Q (2002) Proof nets for the multimodal Lambek calculus. Studia Logica 71(3):415–\\n442\\nMoot R, Retor ´e C (2011) Second order lambda calculus for meaning assembly: on the logical\\nsyntax of plurals. In: Computing Natural Reasoning (COCONAT), Tilburg\\nMoot R, Retor ´e C (2012) The Logic of Categorial Grammars: A Deductive Account of Natural\\nLanguage Syntax and Semantics. No. 6850 in Lecture Notes in Artiﬁcial Intelligence, Springer\\nMoot R, Schrijen X, Verhoog GJ, Moortgat M (2015) Grail0: A theorem prover for multimodal\\ncategorial grammars. https://github.com/RichardMoot/Grail0\\nMorrill G, Valent ´ın O, Fadda M (2011) The displacement calculus. Journal of Logic, Language\\nand Information 20(1):1–48\\nMurawski AS, Ong CHL (2000) Dominator trees and fast veriﬁcation of proof nets. In: Logic in\\nComputer Science, pp 181–191\\nOehrle RT (1994) Term-labeled categorial type systems. Linguistics & Philosophy 17(6):633–678\\nPentus M (1997) Product-free Lambek calculus and context-free grammars. Journal of Symbolic\\nLogic 62:648–660\\nPogodalla S, Pompigne F (2012) Controlling extraction in abstract categorial grammars. In:\\nde Groote P, Nederhof MJ (eds) Proceedings of Formal Grammar 2010–2011, Springer, LNCS,\\nvol 7395, pp 162–177\\nPustejovsky J (1995) The generative lexicon. M.I.T. Press\\nRanta A (1991) Intuitionistic categorial grammar. Linguistics and Philosophy 14(2):203–239\\nShieber S (1985) Evidence against the context-freeness of natural language. Linguistics & Philos-\\nophy 8:333–343\\nValent ´ın O (2014) The hidden structural rules of the discontinuous Lambek calculus. In: Casadio C,\\nCoecke B, Moortgat M, Scott P (eds) Categories and Types in Logic, Language, and Physics:\\nEssays dedicated to Jim Lambek on the Occasion of this 90th Birthday, no. 8222 in Lecture\\nNotes in Artiﬁcial Intelligence, Springer, pp 402–420', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.00812.pdf', 'page': 31}), Document(page_content='arXiv:1602.07618v1  [cs.CL]  22 Feb 2016From quantum foundations via natural\\nlanguage meaning to a theory of everything\\nBob Coecke\\nUniversity of Oxford\\ncoecke@cs.ox.ac.uk\\nAbstract\\nIn this paper we argue for a paradigmatic shift from ‘reduction-\\nism’ to ‘togetherness’. In particular, we show how interaction betw een\\nsystems in quantum theory naturally carries over to modelling how\\nword meanings interact in natural language. Since meaning in natura l\\nlanguage, depending on the subject domain, encompasses discuss ions\\nwithin any scientiﬁc discipline, we obtain a template for theories such\\nas social interaction, animal behaviour, and many others.\\n1 ...in the beginning was b\\nNo physicists! ...the symbol babove does not stand for the operation that\\nturns two Hilbert spaces into the smallest Hilbert space in w hich the two\\ngiven ones bilinearly embed. No category-theoreticians! . ..neither does it\\nstand for the composition operation that turns any pair of ob jects (and\\nmorphisms) in a monoidal category into another object, and t hat is sub-\\nject to a horrendous bunch of conditions that guaranty coher ence with the\\nremainder of the structure. Instead, this is what it means:\\nb ”“togetherness”\\nMore speciﬁcally, it represents the togetherness of foo 1and foo 2without\\ngiving any speciﬁcation of who/what foo 1and foo 2actually are. Diﬀerently\\nput, it’s the new stuﬀ that emerges when foo 1and foo 2get together. If they\\ndon’t like each other at all, this may be a ﬁght. If they do like each other a\\nlot, this may be a marriage, and a bit later, babies. Note that togetherness\\nis vital for the emergence to actually take place, given that it is quite hard\\nto either have a ﬁght, a wedding, or a baby, if there is nobody e lse around.\\nIt is of course true that in von Neumann’s formalisation of qu antum\\ntheory the tensor product of Hilbert spaces (also denoted by b) plays this\\nrole [39], giving rise to the emergent phenomenon of entanglement [20, 36].\\nAnd more generally, in category theory one can axiomatise composition of\\n1', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 0}), Document(page_content='objects(again denoted by b) within a symmetric monoidal category [5],\\ngiving rise to elements that don’t simply arise by pairing, j ust like in the\\ncase of the Hilbert space tensor product.\\nHowever, in the case of von Neumann’s formalisation of quant um theory\\nwe are talking about a formalisation which, despite being wi dely used, its\\ncreator von Neumann himself didn’t even like [33]. Moreover , in this formal-\\nismbonly arises as a secondary construct, requiring a detailed d escription\\nof foo1and foo 2, whose togetherness it describes. What we are after is\\na ‘foo-less’ conception of b. The composition operation bin symmetric\\nmonoidal categories heads in that direction. However, by ma king an unnec-\\nessary commitment to set-theory, it makes things unnecessa rily complicated\\n[16]. Moreover, while this operation is general enough to ac commodate to-\\ngetherness, it doesn’t really tell us anything about it.\\nThe title of this section is a metaphor aimed at confronting t he complete\\ndisregard that the concept of togetherness has suﬀered in the sciences, and\\nespecially, in physics, where all of the eﬀort has been on desc ribing the in-\\ndividual, typically by breaking its description down to tha t of even smaller\\nindividuals. While, without any doubt, this has been a usefu l endeavour,\\nit unfortunately has evolved in a rigid doctrine, leaving no space for any-\\nthing else. The most extreme manifestation of this dogma is t he use of the\\nterm ‘theory of everything’ in particle physics. We will pro vide an alterna-\\ntive conceptual template for a theory of everything, suppor ted not only by\\nscientiﬁc examples, but also by everyday ones.\\nBiology evolved from chopping up individual animals in labo ratories, to\\nconsidering them in the context of other other animals and va rying envi-\\nronments. the result is the theory of evolution of species. S imilarly, our\\ncurrent (still very poor) understanding of the human brain m akes it clear\\nthat the human brain should not be studied as something in iso lation, but\\nas something that fundamentally requires interaction with other brains [30].\\nIn contemporary audio equipment, music consists of nothing but a strings\\nof zeros and ones. Instead, the entities that truly make up mu sic are pitch,\\nsound, rhythm, chord progression, crescendo, and so on. And in particu-\\nlar, music is not just a bag of these, since their intricate in teraction is even\\nmore important than these constituents themselves. The sam e is true for\\nﬁlm, where it isn’t even that clear what it is made up from, but it does\\ninclude such things as (easily replaceable) actors, decors , cameras, which all\\nare part of a soup stirred by a director. But again, in contemp orary video\\nequipment, it is nothing but a string of zeros and ones.\\nIn fact, everything that goes on in pretty much all modern dev ices is\\nnothing but zeros and ones. While it was Turing’s brilliance to realise that\\nthis could in fact be done, and provided a foundation for the t heory of\\ncomputability [38], this is in fact the only place where the z eros and ones are\\ntruly meaningful, in the form of a Turing machine. Elsewhere , it is nothing\\nbut a (universal) representation, with no conceptual quali ties regarding the\\n2', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 1}), Document(page_content='subject matter.\\n2 Formalising togetherness 1: not there yet\\nSo, how does one go about formalising the concept of together ness? While\\nwe don’t want an explicit description of the foo involved, we do need some\\nkindof means for identifying foo. Therefore, we simplegive each foo aname,\\nsayA,B,C,... . Then,AbBrepresents the togetherness of AandB. We\\nalso don’t want an explicit description of AbB, so how can we say anything\\nabout bwithout explicitly describing A,BandAbB?\\nWell, rather than describing these systems themselves, we c ould describe\\ntheir relationships. For example, in a certain theory toget herness could obey\\nthe following equation:\\nAbA“A\\nThat is, togetherness of two copies of something is exactly t he same as a\\nsingle copy, or in simpler terms, one is as good as two. For exa mple, if one\\nis in need of a plumber to ﬁx a pipe, one only needs one. The only thing\\na second plumber would contribute is a bill for the time he was ted coming\\nto your house. Obviously, this is not the kind of togethernes s that we are\\nreally interested in, given that this kind adds nothing at al l.\\nA tiny bit more interesting is the case that two is as good as th ree:\\nAbAbA“AbA\\ne.g. when something needs to be carried on a staircase, but th ere really is\\nonly space for two people to be involved. Or, when Ais female and ¯Ais\\nmale, and the goal is reproduction, we have:\\nAb¯Ab¯A“Ab¯A\\n(ignoring testosterone induced scuﬄes and the beneﬁts of na tural selection.)\\nWe really won’t get very far this manner. One way in which thin gs can\\nbe improved is by replacing equations by inequalities. For e xample, while:\\nA“B\\nsimply means that one of the two is redundant, instead:\\nAďB\\ncan mean that from Awe can produce B, and:\\nAbBďC\\ncan mean that from AandBtogether we can produce C, and:\\nAbCďBbC\\n3', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 2}), Document(page_content='can mean that in the presence of CfromAwe can produce B, i.e. that C\\nis a catalyst.\\nWhat we have now is a so-called resource theory , that is, a theory which\\ncaptures how stuﬀ we care about can be interconverted [13]. R esource the-\\nories allow for quantitative analysis, for example, in term s of aconversion\\nrate:\\nrpAÑBq:“sup#\\nm\\nnˇˇˇˇˇAb...bA loooooomoooooon\\nnďBb...bB loooooomoooooon\\nm+\\nSo evidently we have some genuine substance now.1\\n3 Formalising togetherness 2: that’s better\\nBut we can still do a lot better. What a resource theory fails t o capture\\n(on purpose in fact) is the actual process that converts one r esource into\\nanother one. So let’s ﬁx that problem, and explicitly accoun t for processes.\\nIn terms of togetherness, this means that we bring the fun foo 1and foo 2\\ncan have together explicitly in the picture. Let:\\nf:AÑB\\ndenote some process that transforms AintoB.Then, given two such pro-\\ncessesf1andf2we can also consider their togetherness:\\nf1bf2:A1bA2ÑB1bB2\\nMoreover, some processes can be sequentially chained:\\ng˝f:AÑBÑC\\nWe say ‘some’, since fhas to produce B, in order for:\\ng:BÑC\\nto take place.\\nNow, here one may end up in a bit of a mess if one isn’t clever. In\\nparticular, with a bit of thinking one quickly realises that one wants some\\nequations to be obeyed, for example:\\npf1bf2q bf3“f1b pf2bf3q (1)\\nh˝ pg˝fq “ ph˝gq ˝f (2)\\n1In fact, resource theories are currently a very active area o f research in the quantum\\ninformation and quantum foundations communities, e.g. the resource theories of entangle-\\nment [23], symmetry [21], and athermality [7].\\n4', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 3}), Document(page_content='and a bit more sophisticated, also:\\npg1bg2q ˝ pf1bf2q “ pg1˝f1q b pg2˝f2q (3)\\nThere may even be some more equations that one wants to have, b ut which\\nones? This turns out to be a very diﬃcult problem. Too diﬃcult in the light\\nof our limited existence in this world. The origin of this pro blem is that we\\ntreat b, and also ˝, as algebraic connectives, and that algebra has its roots\\nin set-theory. The larger-than-life problem can be avoided in a manner that\\nis equally elegant as it is simple.\\nTo state that things are together, we just write them down tog ether:\\nA B\\nThere really is no reason to adjoin the symbol bbetween them. Now, this A\\nandBwill play the role of an input or an output of processes transf orming\\nthem. Therefore, it will be useful to represent them by a wire :\\nB A\\nThen, a process transforming AintoBcan be represented by a box:\\nf\\nAB\\nTogetherness of processes now becomes:\\nf1f2\\nand chaining processes becomes:\\nfg\\nIn particular, equations (1), (2) and (3) become:\\nfgh\\ngh\\nf“ f1 “\\ng1\\nf1g2\\nf2 f1g1\\nf2g2“f2f3 f1f2f3\\n5', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 4}), Document(page_content='That is, all equations have become tautologies!2\\n4 Anti-cartesian togetherness\\nOne important kind of processes are states:\\ns\\nThese are depicted without any inputs, where ‘no wire’ can be read as\\n‘nothing’ (or ‘no-foo’).3The opposite notion is that of an eﬀect, that is,\\na process without an output:\\ne\\nborrowing terminology from quantum theory.4\\nWe can now identify those theories in which togetherness doe sn’tyield\\nanything new. Life in such a world is pretty lonely...\\nDeﬁnition 4.1. A theory of togetherness is cartesian if each state:\\nBA\\ns\\ndecomposes as follows:\\nBA\\ns“s1s2A B\\nSo cartesianness means that all possible realisations of tw o foo-s can be\\nachieved by pairing realisations of the individual foo-s in volved. In short, a\\nwhole can be described in term of its parts, rendering togeth erness a void\\nconcept. So very lonely and indeed... But, wait a minute. Why is it then the\\ncase that somuchof traditional mathematics follows thisca rtesian template,\\nand that even category theory for a long time has followed a st rict cartesian\\nstance? Beats me. Seriously...beats me!\\nAnyway, an obvious consequence of this is that for those area s where\\ntogetherness is a genuinely non-trivial concept, traditio nal mathematical\\nstructures aren’t always that useful. That is maybe why soci al sciences\\ndon’t make much use of any kind of modern pure mathematics.\\nAnd now for something completely diﬀerent:\\n2A more extensive discussion of this bit of magic can be found i n [16, 12, 15].\\n3That we use triangles for these rather than boxes is inspired by the Dirac notation\\nwhich is used in quantum theory. Please consult [10, 16, 15] f or a discussion.\\n4Examples of these include ‘tests’ [15].\\n6', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 5}), Document(page_content='Deﬁnition 4.2. A theory of togetherness is anti-cartesian if for each A\\nthere exists A˚, a special state Yand a special eﬀect X:\\nAA˚\\nY and\\nA˚AX\\nwhich are such that the following equation holds:\\nX\\nA˚\\nY“AA\\nA(4)\\nThe reason for ‘anti’ in the name of this kind of togetherness is the fact\\nthat when a theory of togetherness is both cartesian and anti -cartesian, then\\nit is nothing but a theory of absolute death, i.e. it describe s a world in which\\nnothing ever happens. Indeed, we have:\\n“Y1 Y2X\\nY“X\\n“Y2\\nY1Xcartesian anti-cartesian\\n“Y2\\nt\\nThat is, the identityis a constant process, always outputting the state Y2,\\nindependent of what the input is. And if that isn’t weird enou gh, any\\narbitrary process fdoes the same:\\n“\\nf“\\nfY2\\ntY2\\nt1f“\\nTherefore, any anti-cartesian theory of togetherness that involves some as-\\npect of change cannot be cartesian, and hence will have inter esting stuﬀ\\nemerging from togetherness.5\\n5 Example 1: quantum theory\\nAnti-cartesian togetherness is a very particular alternat ive to cartesian to-\\ngetherness (contra any theory that fails to becartesian). S oone may wonder\\n5Many more properties of anti-cartesian togetherness can be found in [15].\\n7', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 6}), Document(page_content='whether there are any interesting examples. And yes, there a re! One ex-\\nample is quantum entanglement in quantum theory. That is in fact where\\nthe author’s interest in anti-cartesian togetherness star ted [9, 1, 10].6As\\nshown in these papers, equation (4) pretty much embodies the phenomenon\\nof quantum teleportation [6]. The full-blown description o f quantum tele-\\nportation goes as follows [17, 18, 15]:\\nρpUAlice Bob\\npUAlice’s measurement\\nBob’s unitary correctionclassical communication\\nstate to be teleported\\nshared Bell-state\\nIt is not important to fully understand the details here. Wha t is important\\nis to note that the bit of this diagram corresponding to equat ion (4) is the\\nbold wire which zig-zags through it:\\nYX\\nThethin wires and theboxes labelled pUare related to the fact that quantum\\ntheory is non-deterministic. By conditioning on particula r measurement\\noutcomes, teleportation simpliﬁes to [15]:\\n6Independently, similar insights appeared in [3, 26].\\n8', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 7}), Document(page_content='ρAlice Bob\\n“Alice Bob\\nρ\\nstate to be teleported\\nshared Bell-stateAlice’s (conditioned) measurement state now with Bob\\nEquality oftheleft-hand-sideandoftheright-hand-sidef ollows directlyfrom\\nequation (4). While in this picture we eliminated quantum no n-determinism\\nby conditioning on a measurement outcome, there still is som ething very\\n‘quantum’goingonhere: Alice’s(conditioned)measuremen tisnothinglikea\\npassive observation, but a highly non-trivial interventio n that makes Alice’s\\nstateρappear at Bob’s side:\\nρAlice Bob\\nρAlice Bob\\nÞÑ\\nLet’s analyse more carefully what’s going on here by explici tly distin-\\nguishing the top layer and the bottom layer of this diagram:\\nρtop\\nbottom\\nThe bottom part:\\nρ\\nconsists of the state ρtogether with a special Y-state, while the top part:\\nincludes the corresponding X-eﬀect, as well as an output. By making the\\nbottom part and the top part interact, and, in particular, th eYand the X,\\nthe state ρends up at the output of the top part.\\nA more sophisticated variation on the same theme makes it muc h clearer\\nwhich mechanism is going on here. Using equation (4), the dia gram:\\n9', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 8}), Document(page_content='π ρ ρ2ρ1\\nreduces to:\\nπ\\nρρ2ρ1 where ρ1:“ρ1\\nThe grey dot labeled πis some (at this point not important) unitary quan-\\ntum operation [15]. Let us again consider the bottom and top p arts:\\nπ ρ ρ2ρ1 bottomtop\\nThe top part is a far more sophisticated measurement consist ing mainly of\\nX-s. Also the bottom part is a lot more sophisticated, involvi ng many Y-s.\\nThese now cause a highly non-trivial interaction of the thre e states ρ,ρ1\\nandρ2. Why we have chosen this particular example will become clea r in\\nthe next section. What is important to note is that the overal l state and\\noverall eﬀect have to be chosen in a very particular way to crea te the desired\\ninteraction, similarly to an old-fashion telephone switch board that has to be\\nconnected in a very precise manner in order to realise the rig ht connection.\\n6 Example 2: natural language meaning\\nAnother example of anti-cartesian togetherness is the mann er in which word\\nmeaningsinteract innaturallanguage! Given thatlogicori ginated innatural\\nlanguage, whenAristotle analysed arguments involving ‘an d’, ‘if...then’, ‘or’,\\netc., anti-cartesianness can beconceived as somenew kindo f logic!7Sowhat\\nareYand Xin this context?\\nIn order to understand what Xis, we need to understand the mathe-\\nmatics of grammar. The study of the mathematical structure o f grammar\\nhas indicated that the fundamental things making up sentenc es are not the\\n7A more detailed discussion is in [11].\\n10', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 9}), Document(page_content='words, but some atomic grammatical types, such as the noun-t ype and the\\nsentence-type [2, 4, 27]. The transitive verb-type is not an atomic gram-\\nmatical type, but a composite made up of two noun-types and on e sentence-\\ntype. Hence, particularly interesting here is that atomic d oesn’t really mean\\nsmallest...\\nOn the other hand, just like in particle physics where we have particles\\nand anti-particles, the atomic types include types as well a s anti-types. But\\nunlike in particle physics, there are two kinds of anti-type s, namely left\\nones and right ones. This makes language even more non-commu tative than\\nquantum theory!\\nAll of this becomes much clearer when considering an example . Letn\\ndenote the atomic noun-type and let´1nandn´1be the corresponding\\nanti-types. Let sdenote the atomic sentence-type. Then the non-atomic\\ntransitive verb -type is´1n¨s¨n´1. Intuitively, it is easy to understand why.\\nConsider a transitive verb, like ‘hate’. Then, simply sayin g ‘hate’ doesn’t\\nconvey any useful information, until, we also specify ‘whom ’ hates ‘whom’.\\nThat’s exactly the role of the anti-types: they specify that in order to form\\na meaningful sentence, a noun is needed on the left, and a noun is needed\\non the right:\\nAlicelo omo on\\nnhateslo omo on\\n´1n¨s¨n´1Boblo omo on\\nn\\nThen,nand´1ncancel out, and so do n´1andn. What remains is s,\\nconﬁrming that ‘Alice hates Bob’ is a grammatically well-ty ped sentence.\\nWe can now depict the cancelations as follows:\\nn ns nn-1 -1nand´1ncancel out n´1andncancel outsis sole survivor\\nand bingo, we found X!\\nWhile the mathematics of sentence structure has been explor ed now for\\nsome 80 years, the fact that X-s can account for grammatical structure is\\nmerely a 15 years old idea [28]. So what are the Y-s? That is an even\\nmore recent story in which we were involved, and in fact, for w hich we took\\ninspiration from the story of the previous section [8]. Whil eX-s are about\\ngrammar, Y-s are about meaning.\\nThedistributional paradigm for natural language meaning states that\\nmeaning can be represented by vectors in a vector space [37]. Until recently,\\ngrammatical structure was essentially ignored in doing so, and in particular,\\nthere was no theory for how to compute the meaning of a sentenc e, given\\nthe meanings of its words. Our new compositional distributional model of\\n11', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 10}), Document(page_content='meaning of [19] does exactly that.8\\nIn order to explain how this compositional distributional m odel of mean-\\ning works, let’s get back to our example. Since we have gramma tical types\\naround, the meaning vectors should respect grammatical str ucture, that\\nis, the vectors representing compound types should themsel ves live in com-\\npoundvectorspaces. Sothestringofvectorsrepresentingt hewordmeanings\\nof our example would look as follows:\\nA B hates\\nNow we want to put forward a new hypothesis:\\nGrammar is all about how word meanings interact.\\nInspired by the previous section, this can be realised as fol lows:\\nA B hates\\nwhere the X-s are now interpreted in exactly the same manner as in the\\nprevious section. And here is a more sophisticated example:\\nπ A B likemeaningsgrammar\\ndoes not\\nwherethe π-labeled grey circle shouldnowbeconceived as negatingmea ning\\n[19]. The grammatical structure is here:\\nnn-1n-1n σ-1σ-1σσ s j j-1j j-1\\nIt is simply taken from a textbook such as [29], the meanings o fAlice,\\nlikesandBobcan be automatically generated from some corpus, while the\\nmeanings of doesandnotare just cleverly chosen to be [32, 19]:\\nπ\\n8...and has meanwhile outperformed other attempts in severa l benchmark natural lan-\\nguage processing (NLP) tasks [22, 25].\\n12', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 11}), Document(page_content='In the previous section we already saw that in this way we obta in:\\nπ\\nABlikenot\\nThis indeed captures the intended meaning:\\nnotplikepAlice,Bob qq\\nwhere we can think of likeas being a predicate and notas being itself.\\nSo an interesting new aspect of the last example is that some o f the\\nmeaning vectors of words are simply cleverly chosen, and in p articular, in-\\nvolve Y-s. Hence, we genuinely exploit full-blown anti-cartesian ess. What\\nanti-cartesianess does here is making sure that the transit ive verb likes‘re-\\nceives’Aliceas its object. Note also how notdoes pretty much the same\\nasdoes, guiding word meanings through the sentence, with, of cours e, one\\nvery important additional task: negating the sentence mean ing.\\nThe cautious reader must of course have noticed that in the pr evious\\nsection we used thick wires, while here we used thin ones. Als o, the dots in\\nthe full-blown description of quantum teleportation, whic h represent classi-\\ncal data operations, have vanished in this section. Meanwhi le, thick wires as\\nwell as the dots all of these have acquired a vary natural role in a more re-\\nﬁned model of natural language meaning. The dots allow to cle verly choose\\nthe meanings of relative pronouns [34, 35]:\\nShe hates\\nwhoB\\nThick wires (representing density matrices, rather than ve ctors [15]) allow\\nto encode word ambiguity as mixedness [31, 24]. For example, the diﬀerent\\nmeanings of the word queen(a rock band, a person, a bee, a chess piece, or\\na drag —). Mixedness vanishes when providing a suﬃcient stri ng of words\\nthat disambiguates that meaning, e.g.:\\nqueen stings rocks queen to C4 queen\\nwhile in the case of:\\n13', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 12}), Document(page_content='rules queen\\nwe need more disambiguating words, since queencan still refer to a person,\\na rock band, as well as a drag queen.\\n7 Meaning is everything\\nThe distributional model of meaning [37] is very useful in th at it allows for\\nautomation, given a substantially large corpus of text. How ever, from a\\nconceptual point of view it is far from ideal. So one may ask th e question:\\nWhat is meaning?\\nOne may try to play around with a variety of mathematical stru ctures.\\nThe method introduced in [19] doesn’t really depend on how on e models\\nmeaning, as long as we stick to anti-cartesian togetherness , or something\\nsuﬃciently closely related [14]. It is an entertaining exer cise to play around\\nwith the idea of what possibly could be the ultimate mathemat ical structure\\nthat captures meaning in natural language, until one realis es that meaning\\nin natural language truly encompasses everything . Indeed, we use language\\nto talk about everything, e.g. logic, life, biology, physic s, social behaviours,\\npolitics, so the ultimate model of meaning should encompass all of these\\nﬁelds. So, a theory of meaning in natural language is actuall y a theory of\\neverything! Can we make sense of the template introduced in t he previous\\nsection for meaning in natural language, as one for ... every thing?\\nLet us ﬁrst investigate, whether the general distributiona l paradigm can\\nbe specialised to the variety of subject domains mentioned a bove. The man-\\nner in which the distributional model works, is that meaning s are assigned\\nrelative to a ﬁxed chosen set of context words. The meaning ve ctor of any\\nword then arises by counting the number of occurrences of tha t word in the\\nclose neighbourhood of each of the context words, within a la rge corpus of\\ntext. One can think of the context words as attributes, and th e relative fre-\\nquencies as therelevance of an attributefor the word. Simpl y by specialising\\nthe context words and the corpus, one can specialise to a cert ain subject do-\\nmain. For example, if one is interested in social behaviours then the corpus\\ncould consist of social networking sites, and the context wo rds could be cho-\\nsen accordingly. This pragmatic approach allows for quanti tative analysis,\\njust like the compositional distributional model of [19].\\nHere’s another example:\\nlion pray hunts\\n14', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 13}), Document(page_content='Here the meaning of pray could include speciﬁcation of the av ailable pray,\\nand then the meaning of the sentence would capture the surviv al success of\\nthe lion, given the nature of the available pray. All togethe r, the resulting\\nmeaning is the result of the interaction between a particula r hunter, a par-\\nticular pray, and the intricacies of the hunting process, wh ich may depend\\non the particular environment in which it is taking place. It should be clear\\nthat again this situation is radically non-cartesian.\\nOf course, if we now consider the example of quantum theory fr om two\\nsections ago, the analogues to grammatical types are system types i.e. a\\nspeciﬁcation of the kinds (incl. quantity) of systems that a re involved. So it\\nmakessensetoreﬁnethegrammaticaltypesaccordingtothes ubjectdomain.\\nJust like nouns in physics would involve speciﬁcation of the kinds of systems\\ninvolved, in biology, for example, this could involve speci ﬁcation of species,\\npopulation size, environment, availability of food etc. Co rrespondingly, the\\ntop part would not just be restricted to grammatical interac tion, but also\\ndomain speciﬁc interaction, just like in the case of quantum theory. All\\ntogether, what we obtain is the following picture:\\nπ x z yinteraction structure\\ndata/descriptions of relevant entitiestypes\\nas a (very rough) template for a theory of everything.\\nAcknowledgements\\nThe extrapolation of meaning beyond natural language was pr ompted by\\nhaving to give a course in a workshop on Logics for Social Beha viour, organ-\\nised by Alexander Kurz and Alessandra Palmigiano at the Lore ntz centre in\\nLeiden. The referee provided useful feedback—I learned a ne w word: ‘foo’.\\nReferences\\n[1] S. Abramsky and B. Coecke. A categorical semantics of qua ntum\\nprotocols. In Proceedings of the 19th Annual IEEE Symposium on\\nLogic in Computer Science (LICS) , pages 415–425, 2004. arXiv:quant-\\nph/0402130.\\n15', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 14}), Document(page_content='[2] K. Ajdukiewicz. Die syntaktische konnexit¨ at. Studia Philosophica , 1:1–\\n27, 1937.\\n[3] J. C. Baez. Quantum quandaries: a category-theoretic pe rspective. In\\nD. Rickles, S. French, and J.T. Saatsi, editors, The Structural Foun-\\ndations of Quantum Gravity , pages 240–266. Oxford University Press,\\n2006. arXiv:quant-ph/0404040.\\n[4] Y. Bar-Hillel. A quasiarithmetical notation for syntac tic description.\\nLanguage , 29:47–58, 1953.\\n[5] J. Benabou. Categories avec multiplication. Comptes Rendus des\\nS´ eances de l’Acad´ emie des Sciences. Paris , 256:1887–1890, 1963.\\n[6] C. H. Bennett, G. Brassard, C. Crepeau, R. Jozsa, A. Peres , and W. K.\\nWootters. Teleporting an unknown quantum state via dual cla ssi-\\ncal and Einstein-Podolsky-Rosen channels. Physical Review Letters ,\\n70(13):1895–1899, 1993.\\n[7] F. G. S. L. Brand˜ ao, M. Horodecki, J. Oppenheim, J. M. Ren es, and\\nR. W Spekkens. The resource theory of quantum states out of th ermal\\nequilibrium. Physical Review Letters , 111:250404, 2013.\\n[8] S. Clark, B. Coecke, E. Grefenstette, S. Pulman, and M. Sa drzadeh. A\\nquantum teleportation inspired algorithm produces senten ce meaning\\nfrom word meaning and grammatical structure. arXiv:1305.0 556, 2013.\\n[9] B. Coecke. The logic of entanglement. An invitation. Tec hnical Report\\nRR-03-12, Department of Computer Science, Oxford Universi ty, 2003.\\n[10] B.Coecke. Kindergartenquantummechanics. InA.Khren nikov, editor,\\nQuantum Theory: Reconsiderations of the Foundations III , pages 81–\\n98. AIP Press, 2005. arXiv:quant-ph/0510032.\\n[11] B. Coecke. The logic of quantum mechanics – take II. arXi v:1204.3458,\\n2012.\\n[12] B. Coecke. An alternative Gospel of structure: order, c omposition, pro-\\ncesses. InC.Heunen,M.Sadrzadeh,andE.Grefenstette, edi tors,Quan-\\ntum Physics and Linguistics. A Compositional, Diagrammatic D is-\\ncourse, pages 1 – 22. Oxford University Press, 2013. arXiv:1307.40 38.\\n[13] B. Coecke, T. Fritz, and R. W. Spekkens. A mathematical t he-\\nory of resources. Information and Computation, to appear , 2014.\\narXiv:1409.5531.\\n[14] B. Coecke, E. Grefenstette, and M. Sadrzadeh. Lambek vs . Lambek:\\nFunctorial vector space semantics and string diagrams for L ambek cal-\\nculus.Annals of Pure and Applied Logic , 164:1079–1100, 2013.\\n16', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 15}), Document(page_content='[15] B. Coecke and A. Kissinger. Picturing Quantum Processes. A First\\nCourse in Quantum Theory and Diagrammatic Reasoning . Cambridge\\nUniversity Press, 2016.\\n[16] B. Coecke and ´E. O. Paquette. Categories for the practicing physicist.\\nIn B. Coecke, editor, New Structures for Physics , Lecture Notes in\\nPhysics, pages 167–271. Springer, 2011. arXiv:0905.3010.\\n[17] B. Coecke, ´E. O. Paquette, and D. Pavlovi´ c. Classical and quantum\\nstructuralism. In S. Gay and I. Mackie, editors, Semantic Techniques\\nin Quantum Computation , pages 29–69. Cambridge University Press,\\n2010. arXiv:0904.1997.\\n[18] B. Coecke and S. Perdrix. Environment and classical cha nnels in cate-\\ngorical quantum mechanics. In Proceedings of the 19th EACSL Annual\\nConference on Computer Science Logic (CSL) , volume 6247 of Lecture\\nNotes in Computer Science , pages 230–244, 2010. Extended version:\\narXiv:1004.1598.\\n[19] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical fou ndations\\nfor a compositional distributional model of meaning. In J. v an Ben-\\nthem, M. Moortgat, and W. Buszkowski, editors, A Festschrift for\\nJim Lambek , volume 36 of Linguistic Analysis , pages 345–384. 2010.\\narxiv:1003.4394.\\n[20] A. Einstein, B. Podolsky, and N. Rosen. Can quantum-mec hanical\\ndescription of physical reality be considered complete? Physical review ,\\n47(10):777, 1935.\\n[21] G. Gour and R. W. Spekkens. The resource theory of quantu m refer-\\nence frames: manipulations and monotones. New Journal of Physics ,\\n10:033023, 2008.\\n[22] E. Grefenstette and M. Sadrzadeh. Experimental suppor t for a categor-\\nical compositional distributional model of meaning. In The 2014 Con-\\nference on Empirical Methods on Natural Language Processin g., pages\\n1394–1404, 2011. arXiv:1106.4058.\\n[23] R. Horodecki, P. Horodecki, M. Horodecki, and K. Horode cki. Quan-\\ntum entanglement. Reviews of Modern Physics , 81:865–942, 2009.\\narXiv:quant-ph/0702225.\\n[24] D. Kartsaklis. Compositional Distributional Semantics with Compact\\nClosed Categories and Frobenius Algebras . PhD thesis, University of\\nOxford, 2014.\\n17', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 16}), Document(page_content='[25] D. Kartsaklis and M. Sadrzadeh. Prior disambiguation o f word tensors\\nfor constructing sentence vectors. In The 2013 Conference on Empiri-\\ncal Methods on Natural Language Processing. , pages 1590–1601. ACL,\\n2013.\\n[26] L. H. Kauﬀman. Teleportation topology. Optics and Spectroscopy ,\\n99:227–232, 2005.\\n[27] J. Lambek. The mathematics of sentence structure. American Mathe-\\nmatics Monthly , 65, 1958.\\n[28] J. Lambek. Type grammar revisited. Logical Aspects of Computational\\nLinguistics , 1582, 1999.\\n[29] J. Lambek. From word to sentence. Polimetrica, Milan , 2008.\\n[30] M. D. Lieberman. Social: Why our Brains are Wired to Connect . Ox-\\nford University Press, 2013.\\n[31] R. Piedeleu. Ambiguity in categorical models of meanin g. Master’s\\nthesis, University of Oxford, 2014.\\n[32] A. Preller and M. Sadrzadeh. Bell states and negative se ntences in the\\ndistributed model of meaning. Electronic Notes in Theoretical Com-\\nputer Science , 270(2):141–153, 2011.\\n[33] M. Redei. Why John von Neumann did not like the Hilbert sp ace\\nformalism of quantum mechanics (and what he liked instead). Studies\\nin History and Philosophy of Modern Physics , 27(4):493–510, 1996.\\n[34] M.Sadrzadeh,S.Clark,andB.Coecke. TheFrobeniusana tomyofword\\nmeanings I: subject and object relative pronouns. Journal of Logic and\\nComputation , 23:1293–1317, 2013. arXiv:1404.5278.\\n[35] M. Sadrzadeh, S. Clark, and B. Coecke. The Frobenius ana tomy of\\nword meanings II: possessive relative pronouns. Journal of Logic and\\nComputation , page exu027, 2014.\\n[36] E. Schr¨ odinger. Discussion of probability relations between separated\\nsystems. Cambridge Philosophical Society , 31:555–563, 1935.\\n[37] H. Sch¨ utze. Automatic word sense discrimination. Computational lin-\\nguistics, 24(1):97–123, 1998.\\n[38] A. M. Turing. On computable numbers, with an applicatio n to the\\nEntscheidungsproblem. Proceedings of the London Mathematical Soci-\\nety, 42:230–265, 1937.\\n18', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 17}), Document(page_content='[39] J. von Neumann. Mathematische grundlagen der quantenmechanik .\\nSpringer-Verlag, 1932. Translation, Mathematical foundations of quan-\\ntum mechanics , Princeton University Press, 1955.\\n19', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1602.07618.pdf', 'page': 18})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 04: Extract the text from the PDF's**"
      ],
      "metadata": {
        "id": "mnfFT3FzUjnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "5B9QhJ6uNHDX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 05: Split the Extracted Data into Text Chunks**"
      ],
      "metadata": {
        "id": "ElztuJ_jUsGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "9kr7eZr4NI4B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMr3q6TyNLfO",
        "outputId": "6d9348bd-f65a-4d8e-ed19-8be5e6191531"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[34]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDRO2CDZNRDv",
        "outputId": "3b9f64db-7fce-4653-e2ec-ed1fd5933706"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='(a) A neural find module for computing an attention over\\npixels. (b) The same operation applied to a knowledge base.\\n(c) Using an attention produced by a lower module to identify', metadata={'source': '/content/drive/MyDrive/Set1_100_Papers/EVAL_question_paper/Copy of 1601.01705.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 06: Downlaod the Embeddings From the Hugging Face**"
      ],
      "metadata": {
        "id": "QWtr3ZP8UzC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "D5PIwt6QNR9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "acc164f856c14a3fb2de6c024d79c718",
            "240c56296c91453cb47559990719a50a",
            "411f6aa03f0f4964a7b70a8394b4ce76",
            "d454e2b981ae4ef38a93d6c118245631",
            "1ea6754d8b14433e9200f3bf0ed0a4ca",
            "b39eb357def142408441faf35aa31321",
            "d4c80790ab2543d7a99d95c6b8ffad3d",
            "dbd9dece30b94dce9dfe57239a6b046f",
            "23514a968836402abad23d63d889ad0c",
            "6d77f4b2d3f44160b402c50beb88672b",
            "3d910f6db09d4374bed44b2906684f85",
            "ab31bac58ac441a09e09c1c5bfffb69b",
            "98ffda006b5d48199c7e79d4aedf3ee6",
            "4728b1dad370472796c890c1b7055281",
            "02549f35f74a454a9bc58092da667cc9",
            "e52402be8ff641d098f990fcb0019d65",
            "df2807bc936140d8ad162c3de8ce53f7",
            "59c78bb6b95f4a55af87195bf6c2a5fa",
            "045189bedbe84110a19f48d9eef4d60d",
            "bcf0852b53524a25825c6f2f360efc72",
            "4017f9a1d5744ba3ad3da37136fcfe4c",
            "8b5b134086eb40efbf46d4400d7635d7",
            "b1ff83ffe0294fc4bc399efdcf6bbe4f",
            "0ec2a11b048e439ebb5ff76b0077c56d",
            "45e9f5c7bb0c4d05a7b9ec76c356d192",
            "6f12b7b66f304de7a05da6c720d8b81f",
            "5238401aed724cc398de72163daf361f",
            "f2cb1fffe06c42df82024efd37075a2a",
            "93db9e8c227b41ceacb4dc46a49750c4",
            "a92ed17fdd8a4cedb4b1d93d16675553",
            "3a1fe82a0cb748cb99e39d02f73cc17d",
            "251b27fa83444977aadd836f50807d83",
            "36b223cbaa9d460dba44c2195b755789",
            "80b4b8c723b24d64a9a986413c344c46",
            "b3f7ba5282db441fbb8761771f19fa64",
            "66d67c9abd404f74a6c36417b6eca7d0",
            "33ebed83905942c58bf470e5bddf3efd",
            "6ed93fad5515466bb1008329a2d0e817",
            "10140c51849e4862bb7922dbe67dd43f",
            "d2cd503578e748fb8531a0b2bae42982",
            "89b37a17a1c4412fb932e0df1a4db965",
            "6b3deb1a7b074205933f8f38997471fb",
            "65cdaebc4b054b77bc63e9755c194e8e",
            "1d321a89ab314c31977ab3adf09c3107",
            "491b62f0bfce42c78e5c3ed8c791f140",
            "2d0799fd4e16458bb22ecc838631c90f",
            "9c6a198836054def8e34b280d843087d",
            "bea933973ce7470e9333cfe45ce64a9e",
            "d16c5708d6744ca19d0d9cf677383a7f",
            "8736bc930975483f8d82f0ddf9caebdd",
            "8998617c493048d29c185f9aa5c026a7",
            "49f14c8a01204ef2bec9aafbc9df27d9",
            "2d6002004ae04e4b8b10916de1593c23",
            "c77bf9094af84c8f9b1702c3402e9237",
            "706e876a3cac430b9f77df0d53b7e496",
            "64426035659b436780080e780a646ffb",
            "93737b5623dd445e982d951874fd89dd",
            "34a9e3fcd9764a6ca11485b45c010979",
            "77ebe2d1b4304c26a150174ab101389e",
            "1ce6770615d3462fa1fe2849d03af33c",
            "50a3b6f6712442619f1e575ecbbd641c",
            "91f8eb16cf7347d6b0658b66d39a50e2",
            "ec3e65b4baaf405a93543c0a91beb1f7",
            "5b0d6d31b8a1418f9f7bb3703385705c",
            "f4a2f314a25c43338aaf3120d3a8d6f4",
            "f58b82bf17984241a9de92bbf54268ad",
            "0056f070dfd34827a5435ddb2d9e6320",
            "8f7daac1a00e4a7f919f824c6388cbcf",
            "c70b1d407c774aed838fe23ee23098c3",
            "f0a3e88fff2d4843be2966a1528dde0e",
            "b036ef8590924547b0f02064d22c1bda",
            "dc38988ab72c49e68d30ca9903550326",
            "216cec2210e34883a1d98d2716e8c802",
            "7b913ff733e043d8957eeb3775a4873c",
            "7a0006874bd74c5fac7f143316d39fe4",
            "8af55f91a8624fe8afd6d4f1679f1d73",
            "d0d2b7cf639c4430bd02a860ca7b13a9",
            "41c919f4499f43bc8acbfbf45b44fa9e",
            "cef3371198454c0dada18c0ac0243a83",
            "9a4069bedf844305832c68a5c332542d",
            "90851ac8c3b446049857d648e464d08d",
            "f64b6819e1294df1962a9c1a9d39a4b7",
            "921b80a213b148cca704dee0665782a8",
            "585f811ad55e48b5a6bb384954bd48c5",
            "f12aa43e940741bc9e7dd4cc8cfe5e09",
            "23a6ed5be3a0428f9b7961df3720563b",
            "9268221d6aa34c808504fcbf118d64d6",
            "16fb94f40d4c47dca26fc21c0d8590b6",
            "7965b3093236408f9bcb59056d877883",
            "c91d09ed59db4f298adf9b97d6b29baf",
            "6d65cdedef8940628bdb296392fb9c1b",
            "c4ba6ecd50e04f9ba2c8a1e366a76b51",
            "cb673b6db5ea4638a8cb11bc93a99aab",
            "799ef4fee61f4c2ba58288b8943eeee7",
            "87b5567ebaac429586367a3007374db1",
            "3564ea7ba9ab446d891bb6edf087be3f",
            "8628f7947140467ea65da89a69723946",
            "c3689042fdc3499da5877d181c518b9a",
            "2f419b4852f44627b4a33cb31f202a53",
            "6445a00ed9434fb6b06625977f823775",
            "2e22cf1785c74cecabdcc534256b5fab",
            "b47879b7a2a74f918c186af044d6af99",
            "bb85f6601a4548c3a637998f3cb67188",
            "d28eddf96b524ba29fc769f50502748d",
            "2c1b3d9db5734f95bbf0145ac33cba33",
            "24af5a50035241bca67ded3c10c5bca0",
            "81c4083ae5d94404a441bb211b8d8b4a",
            "9890ddfc5c0d4a51bce343f9ebb3e214",
            "47f7d42eb93d4fb6bccc59bc518dc7d9",
            "d95a5f5642134f3bb68ca098715a587d",
            "af9c6bbe18934eff86680cdade04c38a",
            "2a7f01fb4ec947e8897100e84d7d995b",
            "d26ebe5166d7442d80896a82841efb70",
            "2df1b7ae27e044d4b4b744e3cae13a8e",
            "3579b68d704b4c0b84b2744335845de3",
            "20f94e26dacd4638a2a42e32d80953b1",
            "73fa1b681312421cbe4ab7ea92e372de",
            "629ee9c9e89b40eeaad25384c9964bf8",
            "f11fd149c44a47ba8267098532238ac0",
            "270519d0da064fa7810d69e18ab99771",
            "3a3591c554a74502aebc36ec1aad6bdd",
            "cd906bd9b9094cefb39b81a0cc16fda2",
            "369285630d6a485f95a458bd6b0c4b47",
            "dc66ac4b4b0c4590bfdb2f6388e739f7",
            "aea3bf9ea73a424398307cf9334688ab",
            "6de6aa9aed66461fbeb088327c6ade44",
            "612896638e024c7d8e9310aa3c8879c0",
            "2e5a77bb548e4808aa1b195c72941f89",
            "6918ff3a306a4149a834c60930ad75be",
            "3f9e316b1bc44b3a944954c1f09a322e",
            "15e2f0fd376640ef83ba7368ddeea3d2",
            "335debbaad864d1bbb9d6acce60079ca",
            "0fa04ee6946941778c2906aacfefcc25",
            "615d728354a8428da3f852d9d7bdbd52",
            "53513ca471ae4f0285248f3e7ce5bc75",
            "29c80bb779524aeea836cdd3c1c23995",
            "cc3cc69d6902400b85d6923d0b8df88a",
            "38f3218b517443729fef733ad5c53ce3",
            "b41f725971b7435e838a46b05353de3d",
            "2ab2470691d84e16a8a477c5330b3c16",
            "e3ac5c4943ce440ab9636b04e88682cd",
            "9d52fb65898141b2a29f6a66147d9aff",
            "396aa44de2f34ae2ba47d299b60c4a26",
            "09f42c04be204babbe49c3911ff57f5a",
            "61e46d06943744069483b399ff1f355a",
            "172193310beb48878e1dbc48dedb613b",
            "b495ef48425a4149bd735f78ab5fd938",
            "f66ef04eb564421eb29a05222272ab62",
            "e6dedcb833ad4240becd9a6a59d4c99d",
            "d3470a00170d4fb197e3464339b2b49a",
            "d94ec03e837348038948405b66fb10f4",
            "4c112568cf6343b2b1b59a1f8d2130e1",
            "646f4e4096b24acbb9075906274fd01b",
            "3c63e1d7af97417ab450e11fcefb765a"
          ]
        },
        "outputId": "4d62f9b5-039a-492b-83a8-75f275ce7900"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acc164f856c14a3fb2de6c024d79c718"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab31bac58ac441a09e09c1c5bfffb69b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1ff83ffe0294fc4bc399efdcf6bbe4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80b4b8c723b24d64a9a986413c344c46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "491b62f0bfce42c78e5c3ed8c791f140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64426035659b436780080e780a646ffb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0056f070dfd34827a5435ddb2d9e6320"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41c919f4499f43bc8acbfbf45b44fa9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7965b3093236408f9bcb59056d877883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6445a00ed9434fb6b06625977f823775"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af9c6bbe18934eff86680cdade04c38a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd906bd9b9094cefb39b81a0cc16fda2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fa04ee6946941778c2906aacfefcc25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09f42c04be204babbe49c3911ff57f5a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(\"This is a test query\")"
      ],
      "metadata": {
        "id": "bFPLjzpSNZDM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length\", len(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT-KE5fHNbmL",
        "outputId": "0ec50bf3-0ac4-42d2-b096-0fd8d4f5ec06"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 07: Initializing the Pinecone**"
      ],
      "metadata": {
        "id": "z35sKuSAU4z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'c5742382-d6c6-435c-a821-d5d834f172a9')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
      ],
      "metadata": {
        "id": "o2261SdRNeQQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"ncu\" # put in the name of your pinecone index her"
      ],
      "metadata": {
        "id": "WN_YpNcDNN7d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 08: Create Embeddings for each of the Text Chunk**"
      ],
      "metadata": {
        "id": "ctff_2NfVASe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "-u1Qcm0HD0d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#If you already have an index, you can load it like this"
      ],
      "metadata": {
        "id": "5FJ46U9VD11n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "SDqOKjGCNm0v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 09: Similarity Search**"
      ],
      "metadata": {
        "id": "yiDPS-1CVdVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"In the paper The Grail theorem prover: Type theory for syntax and semantics, What formalism does Grail use?\""
      ],
      "metadata": {
        "id": "SmurKUaMOjMz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docsearch.similarity_search(query, k=4)"
      ],
      "metadata": {
        "id": "qpd4Bs_JOkJe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkEiK8cpOmmQ",
        "outputId": "52b85d04-3a5c-4f79-8ea8-fbf56ea11e9c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 9\\nGiven a semantic analysis without a corresponding syntactic proof, we can try'),\n",
              " Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 9\\nGiven a semantic analysis without a corresponding syntactic proof, we can try'),\n",
              " Document(page_content='9 The Grail theorem prover: Type theory for syntax and semantics 9\\nGiven a semantic analysis without a corresponding syntactic proof, we can try'),\n",
              " Document(page_content='Chapter 9\\nThe Grail theorem prover:\\nType theory for syntax and semantics\\nRichard Moot\\nAbstract Type-logical grammars use a foundation of logic and type theory to model')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 10: Creating a Llama2 Model Wrapper**"
      ],
      "metadata": {
        "id": "kdRw9ATkVmle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import torch"
      ],
      "metadata": {
        "id": "3EIH77PY0Ckg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "1a5f25e1a81146f8a9bdb8e3bf62a527",
            "c90aa9bdd27d41d2b06c8e1213177317",
            "214a640f9a7c4c358527d9dde2329551",
            "95e22dda833b4f939fe02be52d6d1080",
            "fe9d58d4cc1945679c423ff15c86f87f",
            "a90b1fa40ceb4ab1940e192d6c1c3d15",
            "d821e5f448c44f4d8c7d164ee1142c35",
            "8065f590ff5b471393c419b442daab19",
            "184f012beb704cc3a1bbc113c2e9032a",
            "6747d1ba5f7e4638bdfc19244a5af9a3",
            "c70afc62257744948a9abeef8e0ed95a",
            "1400d37ae3c041a2a3f56fdb461f1f96",
            "1a0ff08431144766b92e715d2989c79e",
            "e8ecd30f355d4b32ac313155374c37e6",
            "049d43d58b134ee885c21212e4ce85c2",
            "f6411d343be149c1b240aeee351fce0f",
            "6a195ef1fd8f4627aae70b5ef7137fee",
            "7f18b08e3ed04fdf9b25bec96f53973f",
            "946d7ccf4ae7405ba294a1b007c4884d",
            "fcdcc1714c5c4d4ca0a0b673132cccea",
            "fc90add27c2d4de4ac3b89e10b344897",
            "eceed43a804c4467a6cc5258d98e8482",
            "3dca81550b57417aa9fbaf9e371b61f2",
            "82a937baf104441fbc04f2b986281f89",
            "cfdf40549f04413b833b670066baff25",
            "1743bb1c1fdd46c99ce0afd525bf71a3",
            "48392ceba8af4ab4a8c7ee2943add5b3",
            "c6bb0fc7670845ceac7c3e8275569d6b",
            "cdd7cb5bbfec46a0a725d415054e0914",
            "df248a71770044efb8a24f8a5e9d8f9d",
            "221f7823354c4058bea465510b2d6fdf",
            "99f47dc35c784f369808d56a566c6cd1"
          ]
        },
        "id": "mF42MkHD0Gor",
        "outputId": "3beee50e-6edd-497e-a71d-620dc3ba6673"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a5f25e1a81146f8a9bdb8e3bf62a527"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "bd936267170e4e949972461da449ba98",
            "52dd459e01454a3eb8a0acd097de7325",
            "b038c3bb4e10488a9269c58680e4f121",
            "8a1fcb08c5db40178ad31de55dd2d400",
            "d6d88c72539b47438aa91ca03b801322",
            "52709ac8b59b4c4ea2f4686ddc6cac18",
            "f34fc7249cf845788e3a83ebf943f916",
            "63b1c3956fa746dd8af79feaf0f91f83",
            "164f3730277447b4af92c267bdb4a87f",
            "d5eb6449daf34bfe9848c3345bb962f6",
            "4c1eb3395ae34d27b1a4e5754e8472df",
            "b5a84a0e977246f0a0059601f4efa88c",
            "4cd6411f0baa47a3bed321f4c70cea60",
            "58ae471918534655bda14a3f6bdfdb8c",
            "46a8a74a51a144f392159d0cb08f7ab4",
            "de4cac5a69d644148669b468fb80bb61",
            "22ecb9cb514248ae92736fbc44d2ff0f",
            "7dc701af4a5842aba8eb370d8ae5882d",
            "58f05b4653474883a6aec9d5efdeecca",
            "a3f080da9869400fa6d972f69ed875f6",
            "c6597bd7fb5a43df83e7cbb5d2e59450",
            "589ee30147c2424e92f457b9746899b3",
            "804bb6009e1546dca36f66794d27f1e8",
            "8cbc9b2932634a9ab240dc6b644af903",
            "e9dfc86c9b3e48e2ae27fec50d9c1b99",
            "0470f3d00a8f42ec80fa705fdba5ed80",
            "321b320d455e491a993c6911b1890b06",
            "1eeb1f9eded6453a938a078a838383b3",
            "14dcef79caa54b96a8998f92c8ef6610",
            "1ef463439e8a45419f86be22068424a3",
            "7c94bbb6cf9049fca2c8553f650160e1",
            "0d6ea9a78fb64dd0a8fc92686528ce99",
            "4a6b70d82d3d446e8ba6abd95c88bbc6",
            "eb284a3f27244d1bb41e20e5e74f6d84",
            "9629b394cdfa4f93990ca45482a749a4",
            "07a7b25c310d4a2086b0fd804118b782",
            "de622d525eca4779a2e06437254c45ca",
            "a8ccd5de72674da4a3c35f350b2bd1df",
            "2cc51593450a4ca0952dbc573e1ada77",
            "a603972ada6246ce8c1ff21ed729279e",
            "5cab29ce43b244aba97355d331d9f4a9",
            "b9494e650e424f58a0082de782cc962b",
            "941dcb28ff9e475c85e426fc4323b841",
            "5b55dcb7841e4489ad9b059308c71a48"
          ]
        },
        "id": "apZu6ZqF0JdZ",
        "outputId": "46643a0a-f90a-4642-efba-17c4450850fe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd936267170e4e949972461da449ba98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5a84a0e977246f0a0059601f4efa88c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "804bb6009e1546dca36f66794d27f1e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb284a3f27244d1bb41e20e5e74f6d84"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install accelerate\n",
        "#!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "3PARu3_mDgPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0655ca-c053-4b55-e095-dcab081e6813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.2.post2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float32,\n",
        "                                             use_auth_token=True\n",
        "                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "54825b751974458b8fb12bf071cace8e",
            "dbfb40ffef92488ba3c6075e646c272c",
            "ef92216f6f2a408dada9a6dab9ff644b",
            "ed89e182b5f64cd191190fadde63261f",
            "328c658f1af24008b952a0f72aba0cef",
            "bc572ffd1af54f46bf220efe536092cb",
            "0ca37488639840edb42c5737740d76eb",
            "5ccd936d37bf47b3997ff3d184b45022",
            "694229b06ae74c25804a57c09cd3167b",
            "135798794a3c45d7bbcf99714b72fc54",
            "1900235009b34c24ba830bed7572afea",
            "1bc06635bc4f433e80902ae1231b9831",
            "820b86c96bd94a78b4f7c395ef65c07e",
            "631a7877173346348f7ebf16aa7ea902",
            "41004ff5f0f14383b37649ec182c0fa2",
            "7ab7433e2cfd44fab9e05780da861005",
            "e2427fb048a54cb694fe559cb21133f4",
            "a2d4cb553a7244b58b78365d1f1917e7",
            "2b3292623eef426cb466f2426c190205",
            "f13d775ac88142cb81e2f4fb88f139f7",
            "06a6b90169474220b02b1c83346d17ba",
            "5ab576a2790f47daad04a7febcc16350",
            "1d24d8fb049c4f88ab9a267996289918",
            "73b123f5b5b64c50bf18b6aa789c88ad",
            "185cc09767d249268fb3cb7f637b7031",
            "c6141babf6de48cda13fa53085ec5f7c",
            "ac6564231ac54097a9953c760016908d",
            "90f312b935f1461ba001184351b8ced3",
            "7d6e3043e02847c7813a312cc3de54e6",
            "d8f0ba671d61456bbd9f641d836acfc4",
            "3ef8e73904464c0fb1033413c2f07d7c",
            "93da92d7bfc54d9c91641f922240b7b1",
            "27a84e7bfb634b2c8f6e66a5df1a8d3f",
            "2f977a8bc30b45bf91b61a0e48d1a52e",
            "a15cdea078a1453393358ee343a74a4a",
            "8019cb7d6f2d4e1396b01a04a1e82ac7",
            "97d0c1d5626449fa9135a7a3393c68c6",
            "00dc1df77ed141eeb04286b8fcb0bf12",
            "5fb15a50c606476fbc6cae2f1d0450be",
            "d6480e397752474da49e929f73edfb3b",
            "e7a7b33d625e42d9bb6693b95ea60d62",
            "76192ef0e1d4460a88aa0859f2cfc4d6",
            "9800bd892ef940ce9a9491d7a9c0ebb5",
            "5d5b4ae748be403cb8c07e50584d9c8a",
            "b48919d61d4a45629e6823c61c51bd65",
            "1901f283cb6b4dc5914106290b15b8c9",
            "14a07e3ffd704ebd9e5d123971cbebc9",
            "3d4d1bc814b34c32a4f4f0b36d698c5a",
            "b0088d682c6a49149c3e5f7962e62408",
            "681e4490d9414fb7bdf15e764e62dffe",
            "688dfca43d5d4aa781ffe1bd2ded270f",
            "a4ceaef9f037414e9ca680b1db745c6b",
            "d1ddf0b391de421e8e865fb415860576",
            "5d7bdf335cc446f686f55a0211db5941",
            "6ed0ff3d20304e9ea5fa76496eb06689",
            "272945957c6843bfbdb7e138d952eb57",
            "e76eae6654e84d6f9450a4788b8b0be7",
            "a12ffd24728c4b5da2f3c84ece8b0586",
            "6eeb2211ed8043d7a12d84fa07b5bf7e",
            "cafc604c7a914f7799f1507a3bfd4a75",
            "884872ed68274b2587ef01fb03be49e9",
            "9410c6ae56e94814a2710970faeeacbc",
            "5ad248a058d04faaacffa8378a44513a",
            "5776a4cb1c45415ab238520df7c5c2d4",
            "4dffa2996d5346bcae3eac7dc71b535f",
            "8898bfce9c0148e9b28dba20af8003fd",
            "2a1ff053401041fd9c008ca6f6618159",
            "361365d014c246f1b4d79b6f6362c0f8",
            "662e1a95b9364de1838b1a9a8e6d4d9b",
            "2d174f65ea2d49ec8660c301f2650ccc",
            "f5237c5359f24fc29a95f48683af434b",
            "9485d282c90d47ff97c935cf53f9ba78",
            "16a028fe96554b4194b0f7e29d0c5b21",
            "b3c713107afd4e8381c60b2229604e30",
            "aad8d43438294bd6a3f2dff24c817409",
            "13d1ab7b274044d3b69dd0ca35699ae5",
            "56af4a1308314b8aa394d972f79ede36"
          ]
        },
        "id": "3NmEB9kB0MDR",
        "outputId": "2fcd5cb0-5fcb-4bf0-c358-63e4cf21e3c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54825b751974458b8fb12bf071cace8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bc06635bc4f433e80902ae1231b9831"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d24d8fb049c4f88ab9a267996289918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f977a8bc30b45bf91b61a0e48d1a52e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b48919d61d4a45629e6823c61c51bd65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "272945957c6843bfbdb7e138d952eb57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a1ff053401041fd9c008ca6f6618159"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a text generation pipeline\n",
        "from transformers import pipeline, set_seed\n",
        "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e95471c7df2e40a0b459b6612582f565",
            "7680a53b73044835a397a9395172bb99",
            "48211845c1554f4da8a749c7d6105d41",
            "e90e668748474ec5b993996627a35287",
            "5538009baa014fe08f1bd2690854646d",
            "4208bdbd1af74b8a9605124acbff7918",
            "f721665ba21d4757972b7f8189762fb8",
            "137170840ddd438ebe44bbe788576c14",
            "3a50106cbf0f4039870fd1c5e238c009",
            "da2fed2ec393463ebc940054cdf6d0db",
            "db8d69ae1f574df4a9fea3da57736702"
          ]
        },
        "id": "vUFZYJN4Euxy",
        "outputId": "3d5c4ecb-b95b-4a09-8282-e737ccd6126c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e95471c7df2e40a0b459b6612582f565"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.1})"
      ],
      "metadata": {
        "id": "HtGhKZppFgDW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 11: Create a Prompt Template**"
      ],
      "metadata": {
        "id": "fMM6VjsLVs9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SYSTEM_PROMPT = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "#If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\"\"\n",
        "#SYSTEM_PROMPT = \"\"\"You will be provided with the names of research papers, and your task is to generate coherent and detailed paragraphs summarizing key aspects of each paper. For questions about the central argument, focus on extracting the primary point and overarching theme of the paper. When summarizing, concisely capture the main findings, conclusions, and relevant details. If you're unfamiliar with the paper, respond with 'I don't know' and consider adding a contextually appropriate remark. Aim for a fluid and informative paragraph style, avoiding a strict 'question and answer' format.\"\"\"\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Given the diverse range of academic papers, your task is to provide insightful and accurate responses to questions related to various research articles. Your answers should demonstrate a comprehensive understanding of the paper's content and key findings. Questions will start with the paper's name, where your initial response should include the author names. Subsequent questions will delve into specific details of that particular paper.If you're unfamiliar with the paper, respond with 'I don't know' and consider adding a contextually appropriate remark.\n",
        "\n",
        "**Instructions for Model:**\n",
        "1. Read the prompts carefully and ensure your responses directly address the questions asked.\n",
        "2. Pay attention to details such as experimental methodologies, key arguments, and specific examples mentioned in the papers.\n",
        "3. Provide concise and informative answers, avoiding vague or generic responses.\n",
        "4. If a question refers to a specific paper, generate your response based on the content of that paper, making sure to incorporate relevant details.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ZDIBDrOxFjNr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "#B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "#SYSTEM_PROMPT = [B_SYS + prompt + E_SYS for prompt in SYSTEM_PROMPT]\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
      ],
      "metadata": {
        "id": "db8ke2AkFp4g"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT + E_SYS"
      ],
      "metadata": {
        "id": "Z0FSBYXxsxsc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instruction = \"\"\"\n",
        "#{context}\n",
        "\n",
        "#Question: {question}\n",
        "#\"\"\"\n",
        "\n",
        "instruction = \"\"\"\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M8NzXpw-Fvwz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#template = B_INST + '\\n'.join(SYSTEM_PROMPT) + instruction + E_INST\n",
        "template = B_INST + SYSTEM_PROMPT + instruction + E_INST"
      ],
      "metadata": {
        "id": "gperUvZyFyMS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "q6F2aMglF2K-",
        "outputId": "397498c6-26f8-481e-bff9-88317e009836"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[INST]<<SYS>>\\n\\nGiven the diverse range of academic papers, your task is to provide insightful and accurate responses to questions related to various research articles. Your answers should demonstrate a comprehensive understanding of the paper's content and key findings. Questions will start with the paper's name, where your initial response should include the author names. Subsequent questions will delve into specific details of that particular paper.If you're unfamiliar with the paper, respond with 'I don't know' and consider adding a contextually appropriate remark.\\n\\n**Instructions for Model:**\\n1. Read the prompts carefully and ensure your responses directly address the questions asked.\\n2. Pay attention to details such as experimental methodologies, key arguments, and specific examples mentioned in the papers.\\n3. Provide concise and informative answers, avoiding vague or generic responses.\\n4. If a question refers to a specific paper, generate your response based on the content of that paper, making sure to incorporate relevant details.\\n\\n<</SYS>>\\n\\n\\n{context}\\n\\nQuestion: {question}\\n[/INST]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "JmcX6UGHF3bZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "id": "Ws1_gnAWF6Yx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"In the paper The Grail theorem prover: Type theory for syntax and semantics, What formalism does Grail use?\")"
      ],
      "metadata": {
        "id": "VfyZn9kHOtFW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results for a single query**"
      ],
      "metadata": {
        "id": "g-_SZ-c2V4H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qD0yx2mhJaLY",
        "outputId": "deb6b811-3739-4b9e-b20a-1811fd1dc393"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  In the paper \"The Grail theorem prover: Type theory for syntax and semantics,\" the authors use a formalism called \"type theory\" to provide a foundation for the design and implementation of a theorem prover. Specifically, Grail is based on the Calculus of Inductive Constructions (CIC), which is a type theory that allows for the expression of complex mathematical structures through a series of inductive definitions. (Source: Page 2 of the paper)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results for Multiple queries**"
      ],
      "metadata": {
        "id": "4GD2T6_VWBAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input=input(f\"prompt:\")\n",
        "    if user_input=='exit':\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "    if user_input=='':\n",
        "        continue\n",
        "    result=qa_chain({'query':user_input})\n",
        "    print(f\"Answer:{result['result']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Ywla1SoQO6X",
        "outputId": "0b70aaba-3960-4b86-98f0-9da5902907c0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt:In the paper Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing, How many paraphrases are generated per question?\n",
            "Answer:  According to the paper \"Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing\" by Shashi Narayan, Siva Reddy, and Shay B. Cohen, the authors generate a diverse set of paraphrases for each question in the semantic parsing task. They do not provide a specific number of paraphrases generated per question, as the number of paraphrases can vary depending on the complexity of the question and the model's performance. However, they report that their model is able to generate a large number of paraphrases for each question, with an average of 10-15 paraphrases per question in their experimental evaluation.\n",
            "prompt:In the paper Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing, What latent variables are modeled in the PCFG?\n",
            "Answer:  According to the paper \"Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing\" by Shashi Narayan, Siva Reddy, and Shay B. Cohen, the latent variables modeled in the PCFG are:\n",
            "\n",
            "* Lexical semantics: The paper models the meaning of words and phrases in the context of a sentence.\n",
            "* Syntactic structure: The paper models the syntactic structure of sentences, including the relationships between words and the structure of phrases.\n",
            "* Discourse context: The paper models the discourse context in which a sentence appears, including the relationships between sentences and the overall structure of a document.\n",
            "\n",
            "These latent variables are represented as a set of latent variables in the PCFG, which are learned from a large corpus of text data using a variety of techniques, such as maximum likelihood estimation and Bayesian inference. The learned PCFG can then be used for paraphrase generation, which involves generating new sentences that preserve the meaning of a given sentence while differing in terms of its surface form.\n",
            "prompt:In the paper Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing, What are the baselines?\n",
            "Answer:  According to the paper \"Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing\" by Shashi Narayan, Siva Reddy, and Shay B. Cohen, the baselines used in their experiments are:\n",
            "\n",
            "1. Random Paraphrasing: This baseline generates paraphrases by randomly replacing words in the original sentence with their synonyms.\n",
            "2. Word-Level Paraphrasing: This baseline generates paraphrases by replacing individual words in the original sentence with their synonyms, while keeping the sentence structure unchanged.\n",
            "3. Sentence-Level Paraphrasing: This baseline generates paraphrases by rephrasing the entire sentence while preserving the original meaning.\n",
            "\n",
            "These baselines are used to evaluate the performance of the proposed paraphrasing method, which is based on latent-variable probabilistic context-free grammars (PCFGs). The authors compare the performance of their method with these baselines and demonstrate its superiority in generating high-quality paraphrases.\n",
            "prompt:In the paper Learning to Compose Neural Networks for Question Answering, What benchmark datasets they use?\n",
            "Answer:  In the paper \"Learning to Compose Neural Networks for Question Answering\" by Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein, the authors use several benchmark datasets for evaluating the performance of their proposed method. These datasets include:\n",
            "\n",
            "1. SQuAD (Stanford Question Answering Dataset): This dataset contains 100,000+ questions and answers from various domains, including science, history, and entertainment.\n",
            "2. MS MARCO (Microsoft Research Paraphrase Corpus): This dataset contains 2.8 million questions and answers from various domains, including news articles, books, and web pages.\n",
            "3. CNN/Daily Mail: This dataset contains 300,000+ questions and answers from various domains, including news articles and web pages.\n",
            "4. NQ (Natural Questions): This dataset contains 100,000+ questions and answers from various domains, including science, history, and entertainment.\n",
            "\n",
            "The authors use these datasets to evaluate the performance of their proposed method, which involves training a neural network to compose answers from a set of pre-trained language models. They compare their approach with other state-of-the-art methods and show that it outperforms them on several benchmark datasets.\n",
            "prompt:who is the author of this paper?\n",
            "Answer:  The author of this paper is Hoeksema (1983).\n",
            "prompt:What are the two components of the model?\n",
            "Answer:  Based on the paper \"A Survey on Deep Learning for Natural Language Processing\" by Yue Zhang and Jun Zhu, the two components of the model are:\n",
            "\n",
            "1. Word embeddings: This component represents words as dense vectors in a high-dimensional space, capturing their semantic meaning. Word embeddings are typically learned using unsupervised methods, such as Word2Vec or GloVe.\n",
            "2. Recurrent Neural Networks (RNNs): This component models the sequential structure of language, capturing the dependencies between words in a sentence. RNNs can be used for various NLP tasks, including language modeling, machine translation, and text generation.\n",
            "prompt:In the paper Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences, How do they show that acquiring names of places helps self-localization?\n",
            "Answer:  In the paper \"Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences,\" the authors demonstrate the importance of acquiring names of places in helping self-localization through several experiments. Here are some key findings from the paper:\n",
            "\n",
            "1. In the first experiment, the authors trained a robot to recognize places in a simulated environment using a supervised learning approach. They found that the robot was able to accurately localize itself in the environment when the places were named, but struggled when the places were not named.\n",
            "2. In a second experiment, the authors tested the robot's ability to localize itself in a real-world environment. They found that when the robot was able to recognize named places, it was able to accurately localize itself more often than when it was not able to recognize named places.\n",
            "3. The authors also found that the robot was able to learn new places and adapt its map of the environment more quickly when the places were named. This suggests that having a robust vocabulary of place names can help the robot to better understand its environment and improve its self-localization capabilities.\n",
            "\n",
            "Overall, the authors demonstrate that acquiring names of places is an important step in helping a mobile robot to accurately localize itself in a variety of environments. By integrating unsupervised word discovery and self-localization, the robot is able to learn and adapt its understanding of its environment more quickly and accurately, which can be particularly useful in applications such as search and rescue or environmental monitoring.\n",
            "prompt:In the paper Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences, How do they evaluate how their model acquired words?\n",
            "Answer:  In the paper \"Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences,\" the authors evaluate their model's ability to acquire words through a series of experiments. Specifically, they use a combination of behavioral tests and language understanding tasks to assess the robot's language learning abilities.\n",
            "\n",
            "The behavioral tests include:\n",
            "\n",
            "1. Object recognition: The authors evaluate the robot's ability to recognize objects in different spatial locations using a variety of objects.\n",
            "2. Spatial memory: The authors test the robot's ability to remember the location of objects in a previously explored environment.\n",
            "3. Language grounding: The authors assess the robot's ability to associate words with objects in the environment.\n",
            "\n",
            "The language understanding tasks include:\n",
            "\n",
            "1. Object-based sentence completion: The authors present the robot with incomplete sentences that describe objects in the environment and ask it to complete the sentences.\n",
            "2. Object-based question answering: The authors ask the robot questions about objects in the environment and evaluate its ability to provide accurate answers.\n",
            "\n",
            "The authors use a variety of metrics to evaluate the robot's performance in these tasks, including accuracy, precision, and recall. They also compare the performance of their model to a baseline model that does not use unsupervised word discovery to demonstrate the effectiveness of their approach.\n",
            "prompt:In the paper Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences, Which method do they use for word segmentation?\n",
            "Answer:  In the paper \"Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences,\" the authors use a method called \"hidden Markov model (HMM) based word segmentation\" to segment spoken sentences into individual words. Specifically, they use an HMM to model the probability distribution over the possible word sequences in a spoken sentence, given the acoustic features of the speech signals. This allows the robot to identify and separate individual words within the sentence.\n",
            "prompt:In the paper Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences, Does their model start with any prior knowledge of words?\n",
            "Answer:  Yes, the model in the paper \"Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences\" starts with some prior knowledge of words. Specifically, the authors use a pre-trained language model to generate a set of candidate words that are likely to be relevant to the robot's environment. These candidate words are then used as inputs to the robot's perception and understanding of its environment.\n",
            "prompt:In the paper Evaluating the Performance of a Speech Recognition based System, what bottlenecks were identified?\n",
            "Answer:  In the paper \"Evaluating the Performance of a Speech Recognition based System,\" the authors identified several performance bottlenecks of the speech solution. These bottlenecks include:\n",
            "\n",
            "1. Limited vocabulary size: The authors found that the system's vocabulary size was limited, which resulted in reduced accuracy for words outside of the trained vocabulary.\n",
            "2. Noise and interference: The system was found to be sensitive to background noise and interference, which negatively impacted its performance.\n",
            "3. Limited domain adaptation: The system was not able to adapt well to different domains, resulting in reduced accuracy in unfamiliar environments.\n",
            "4. Lack of contextual information: The system relied solely on acoustic features, which limited its ability to understand the context of the speech.\n",
            "5. Limited ability to handle multi-speaker dialogues: The system struggled to distinguish between different speakers in a conversation, leading to reduced accuracy.\n",
            "\n",
            "These bottlenecks highlight the areas where the system can be improved to enhance its overall performance.\n",
            "prompt:Which words in the speech recognition system are phonetically similar?\n",
            "Answer:  Based on the paper \"A Survey on Speech Recognition Systems and Their Applications\" by S. S. Sridhar and B. Vasundhara, the phoneme recognition accuracy is typically low, which means that the system may struggle to accurately identify the phonemes in a spoken utterance. However, there are certain phonemes that are more similar to each other than others, and these phonemes are referred to as \"phonetically similar\" phonemes.\n",
            "\n",
            "According to the paper, some examples of phonetically similar phonemes include:\n",
            "\n",
            "* /p/ and /b/\n",
            "* /t/ and /d/\n",
            "* /k/ and /g/\n",
            "* /s/ and /z/\n",
            "* /m/ and /n/\n",
            "\n",
            "These phonemes are similar in terms of their acoustic properties, such as their frequency and duration, and the system may have difficulty distinguishing between them. Therefore, it is important to consider the phonetic similarity of phonemes when designing a speech recognition system to improve its accuracy.\n",
            "prompt:How can the Levenshtein distance be used to identify confusing words in the speech recognition system?\n",
            "Answer:  Great, let's dive into the details of the paper you mentioned!\n",
            "\n",
            "The Levenshtein distance can be used to identify confusing words in the speech recognition system by measuring the number of single-character edits (insertions, deletions, or substitutions) needed to transform one word into another. This distance metric can help identify words that are similar but not identical, which can cause confusion for the speech recognition system.\n",
            "\n",
            "For example, the words \"bat\" and \"hat\" have a Levenshtein distance of 1, as the only difference between them is a single letter. Similarly, the words \"car\" and \"bar\" have a Levenshtein distance of 2, as they differ by two letters. By analyzing the Levenshtein distance between words, the speech recognition system can better distinguish between similar words and improve its accuracy.\n",
            "\n",
            "In the paper you mentioned, the authors used Levenshtein distance to analyze and identify confusing words in the speech recognition system. They found that the distance metric was effective in distinguishing between words that were similar but not identical, and that it could be used to improve the accuracy of the speech recognition system.\n",
            "\n",
            "I hope that helps clarify things! Let me know if you have any further questions.\n",
            "prompt:In the paper Detecting and Extracting Events from Text Documents, Which datasets are used in this work?\n",
            "Answer:  In the paper \"Detecting and Extracting Events from Text Documents,\" the authors use several datasets to evaluate the performance of their proposed event extraction method. These datasets include:\n",
            "\n",
            "1. The Cornell Movie Dialog Corpus: This dataset contains transcriptions of movie dialogues, which are used to train and test the event extraction model.\n",
            "2. The MIT-BIH Arrhythmia Database: This dataset contains electrocardiogram (ECG) recordings of patients with various heart conditions, which are used to evaluate the model's ability to extract events related to cardiovascular health.\n",
            "3. The Stanford Sentiment Treebank: This dataset contains movie reviews with sentiment labels, which are used to evaluate the model's ability to extract sentiment-bearing phrases and events.\n",
            "4. The 20 Newsgroups dataset: This dataset contains approximately 20,000 news articles from various newsgroups, which are used to evaluate the model's ability to extract events from different domains.\n",
            "\n",
            "The authors also mention that they use a combination of these datasets to create a more comprehensive dataset for event extraction.\n",
            "prompt:What are the three classes of events according to Aristotle?\n",
            "Answer:  Based on the paper [Kenny 2003], Aristotle's three classes of events are:\n",
            "\n",
            "1. Substantial events: These are events that involve changes in the intrinsic nature of the thing affected, such as the growth of a plant or the development of a child.\n",
            "2. Quantitative events: These are events that involve changes in the quantity or amount of something, such as the addition of a certain number of objects or the increase in the weight of an object.\n",
            "3. Qualitative events: These are events that involve changes in the quality or kind of something, such as the change in color of an object or the shift in the taste of a substance.\n",
            "prompt:What is the difference between an activity and a performance according to Kenny?\n",
            "Answer:  Based on the paper \"The Differences Between an Activity and a Performance\" by Kenny (2017), the main difference between an activity and a performance is the concept of delimitation. According to Kenny, an activity is a continuous and ongoing process with no clear beginning or end, while a performance is a delimited event with a natural end. In other words, an activity is an open-ended process, while a performance is a closed event with a specific start and end time.\n",
            "prompt:In the paper The Grail theorem prover: Type theory for syntax and semantics, What formalism does Grail use?\n",
            "Answer:  In the paper \"The Grail theorem prover: Type theory for syntax and semantics,\" the authors use a formalism called \"type theory\" to provide a foundation for the design and implementation of a theorem prover. Specifically, they use a variant of type theory called \"homotopy type theory\" (HOTT), which combines the rigorous mathematical foundations of type theory with the expressive power of homotopy theory. The authors argue that this approach allows for a more flexible and robust way of reasoning about the semantics of programming languages, and enables the development of more powerful and efficient theorem provers.\n",
            "prompt:What are the three main components of a type-logical grammar?\n",
            "Answer:  The three main components of a type-logical grammar, as described in the passage, are:\n",
            "\n",
            "1. Terms: These are the basic elements of the grammar, which can be combined to form phrases and sentences.\n",
            "2. Types: These are the categories or classes of terms, which determine their meaning and usage.\n",
            "3. Rules: These are the principles that govern the combination of terms and types to form phrases and sentences, and which specify the relationships between them.\n",
            "prompt:Who is the author of the paper?\n",
            "Answer:  The authors of the papers you mentioned are:\n",
            "\n",
            "* Hoeksema (1983) - John Hoeksema\n",
            "* Mourelatos (1978) - Paul Mourelatos\n",
            "* Ter Meulen (1983) - Ton van Ter Meulen\n",
            "* (1997) - Unknown author (please provide the name of the author for this paper)\n",
            "prompt:Exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-2355e12cd0b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer:{result['result']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             outputs = (\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         answer = self.combine_documents_chain.run(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    513\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             outputs = (\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             outputs = (\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    505\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                 )\n\u001b[1;32m    655\u001b[0m             ]\n\u001b[0;32m--> 656\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             output = (\n\u001b[0;32m--> 530\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    531\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# Process batch of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# Process each response in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 )\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2802\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    920\u001b[0m                 )\n\u001b[1;32m    921\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}